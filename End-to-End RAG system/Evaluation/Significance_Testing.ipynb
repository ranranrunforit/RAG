{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoJO-2CLihs3"
      },
      "source": [
        "# Significance Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-80Jet0WRPFD"
      },
      "source": [
        "Evaluate the performance of the generated answers in the RAG-based QA system.\n",
        "\n",
        "For evaluation metrics, we use 3 metrics: answer recall, exact match, and F1 score frollowing the setting in\n",
        "the SQuAD paper (https://arxiv.org/pdf/1606.05250)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ActBx_a9OpKQ",
        "outputId": "c487ce33-faf8-490d-bf58-98731b59ac8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['Kansas'], ['3 years'], ['1897'], ['Several'], ['Appalachia']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 9.446419502285424, 'F1 Score': 16.01480979249139, 'Answer Recall': 16.77409508429453}\n",
            "Results saved to ./results/llama3_baseline.json\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import json\n",
        "import string\n",
        "from collections import Counter\n",
        "import logging\n",
        "import os\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "# Configure logging to display information about the script's execution\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "def clean_answer(s):\n",
        "    \"\"\"\n",
        "    normalize an answer.\n",
        "    - Converts the text to lowercase.\n",
        "    - Removes punctuation, articles ('a', 'an', 'the'), and extra whitespace.\n",
        "    - Returns the cleaned text.\n",
        "    \"\"\"\n",
        "    def remove_articles(text):\n",
        "        # Remove articles using a regex pattern\n",
        "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "        return re.sub(regex, ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        # Fix extra whitespace by joining split words\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        # Remove punctuation characters\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        # Convert text to lowercase\n",
        "        return str(text).lower()\n",
        "\n",
        "    # Apply all cleaning steps\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def get_tokens(s):\n",
        "    \"\"\"\n",
        "    Tokenize a string by cleaning it and splitting it into words.\n",
        "    - Returns a list of tokens.\n",
        "    \"\"\"\n",
        "    if not s:\n",
        "        return []\n",
        "    return clean_answer(s).split()\n",
        "\n",
        "def compute_exact_match_single(gold_answer_list, generated_answer):\n",
        "    \"\"\"\n",
        "    Check if the generated answer exactly matches any of the gold answers.\n",
        "    - Cleans both the generated answer and the gold answers.\n",
        "    - Returns True if there is an exact match, otherwise False.\n",
        "    \"\"\"\n",
        "    cleaned_generated = clean_answer(generated_answer)\n",
        "    return any(clean_answer(gold) == cleaned_generated for gold in gold_answer_list)\n",
        "\n",
        "def compute_exact_match(gold_answers, generated_answers):\n",
        "    \"\"\"\n",
        "    Compute the exact match score.\n",
        "    - Compares each generated answer with its corresponding gold answers.\n",
        "    - Returns the percentage of exact matches.\n",
        "    \"\"\"\n",
        "    exact_match = sum(compute_exact_match_single(gold, gen) for gold, gen in zip(gold_answers, generated_answers))\n",
        "    return 100 * exact_match / len(gold_answers)\n",
        "\n",
        "def compute_recall_f1_single(args):\n",
        "    \"\"\"\n",
        "    Compute recall and F1 score for a single pair of gold and generated answers.\n",
        "    - Tokenizes the gold and generated answers.\n",
        "    - Calculates precision, recall, and F1 score based on token overlap.\n",
        "    - Returns the maximum recall and F1 score across all gold answers.\n",
        "    \"\"\"\n",
        "    gold_answer_list, generated_answer = args\n",
        "\n",
        "    # Tokenize the generated answer and count the occurrences of each token\n",
        "    predicted_tokens = Counter(get_tokens(generated_answer))\n",
        "    num_predicted = sum(predicted_tokens.values())  # Total number of tokens in the generated answer\n",
        "\n",
        "    max_recall, max_f1 = 0, 0  # Initialize maximum recall and F1 score\n",
        "    for gold_answer in gold_answer_list:\n",
        "        gold_tokens = Counter(get_tokens(gold_answer))  # Tokenize and count tokens in the gold answer\n",
        "        num_gold = sum(gold_tokens.values())  # Total number of tokens in the gold answer\n",
        "        num_same = sum((predicted_tokens & gold_tokens).values())  # Count overlapping tokens\n",
        "\n",
        "        if num_same == 0:  # Skip if there are no overlapping tokens\n",
        "            continue\n",
        "\n",
        "        # Calculate precision and recall\n",
        "        precision = 1.0 * num_same / num_predicted\n",
        "        recall = 1.0 * num_same / num_gold\n",
        "\n",
        "        # Update maximum recall and F1 score\n",
        "        max_recall = max(recall, max_recall)\n",
        "        max_f1 = max(((2 * precision * recall) / (precision + recall)), max_f1)\n",
        "\n",
        "    return max_recall, max_f1\n",
        "\n",
        "def compute_recall_f1(gold_answers, generated_answers):\n",
        "    \"\"\"\n",
        "    Compute average recall and F1 score using parallel processing.\n",
        "    - Processes each pair of gold and generated answers in parallel.\n",
        "    - Returns the average recall and F1 score as percentages.\n",
        "    \"\"\"\n",
        "    with Pool(cpu_count()) as pool:\n",
        "        # Use multiprocessing to compute recall and F1 scores for all pairs\n",
        "        results = pool.map(compute_recall_f1_single, zip(gold_answers, generated_answers))\n",
        "\n",
        "    # Sum up recall and F1 scores\n",
        "    total_recall, total_f1 = map(sum, zip(*results))\n",
        "    avg_recall = 100 * total_recall / len(gold_answers)  # Calculate average recall\n",
        "    avg_f1 = 100 * total_f1 / len(gold_answers)  # Calculate average F1 score\n",
        "\n",
        "    return avg_recall, avg_f1\n",
        "\n",
        "def evaluate(gold_answers, generated_answers):\n",
        "    \"\"\"\n",
        "    Evaluate generated answers against gold answers.\n",
        "    - Computes exact match, recall, and F1 score.\n",
        "    - Returns a dictionary with the evaluation metrics.\n",
        "    \"\"\"\n",
        "    exact_match = compute_exact_match(gold_answers, generated_answers)\n",
        "    answer_recall, f1_score_avg = compute_recall_f1(gold_answers, generated_answers)\n",
        "\n",
        "    return {\n",
        "        \"Exact Match\": exact_match,\n",
        "        \"F1 Score\": f1_score_avg,\n",
        "        \"Answer Recall\": answer_recall\n",
        "    }\n",
        "\n",
        "def run_evaluate(combined_dir=None, gold_answer_dir=None, generated_answer_dir=None, output_dir=None):\n",
        "    \"\"\"\n",
        "    Evaluate the performance of the generated answers.\n",
        "\n",
        "    Args:\n",
        "        combined_dir (str): Path to the CSV file containing combined gold and generated answers.\n",
        "        gold_answer_dir (str): Path to the file containing the gold answers.\n",
        "        generated_answer_dir (str): Path to the file containing the generated answers.\n",
        "        output_dir (str): Path to save the evaluation results.\n",
        "    \"\"\"\n",
        "    if not output_dir:\n",
        "        raise ValueError(\"The 'output_dir' argument is required.\")\n",
        "\n",
        "    if combined_dir:\n",
        "        # Read combined data from a CSV file\n",
        "        generation_df = pd.read_csv(combined_dir)\n",
        "        generated_answers = generation_df[\"Generated_Answer\"].tolist()  # Extract generated answers\n",
        "        gold_answers = generation_df[\"Reference_Answers\"].apply(lambda x: str(x).split(\"[SEP]\")).tolist()  # Extract gold answers\n",
        "        print(gold_answers[:5])  # Print the first 5 gold answers for verification\n",
        "        print(\"Loaded combined gold and generated answers from CSV files.\")\n",
        "    else:\n",
        "        # Read gold and generated answers from separate files\n",
        "        if not gold_answer_dir or not generated_answer_dir:\n",
        "            raise ValueError(\"Both 'gold_answer_dir' and 'generated_answer_dir' arguments are required if 'combined_dir' is not provided.\")\n",
        "\n",
        "        # Read gold answers from a file\n",
        "        with open(gold_answer_dir, \"r\") as f:\n",
        "            gold_answers = [line.strip().split(\";\") for line in f]\n",
        "\n",
        "        # Read generated answers from a file\n",
        "        with open(generated_answer_dir, \"r\") as f:\n",
        "            generated_answers = [line.strip() for line in f]\n",
        "\n",
        "    # Evaluate the generated answers\n",
        "    results = evaluate(gold_answers, generated_answers)\n",
        "    print(f\"Evaluation results: {results}\")\n",
        "\n",
        "    # Save the evaluation results to a JSON file\n",
        "    with open(output_dir, \"w\") as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "    print(f\"Results saved to {output_dir}\")\n",
        "\n",
        "# Run the evaluation with specified input and output paths\n",
        "run_evaluate(combined_dir=\"./output/llama3_baseline.csv\", output_dir=\"./results/llama3_baseline.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7KO4MIIq6kW",
        "outputId": "01d3b96a-44f9-4ec1-ec18-8724ce2a57a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 7.0, 'F1 Score': 12.878471528471529, 'Answer Recall': 13.1}\n",
            "Results saved to ./results/llama32_3B_baseline.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 11.0, 'F1 Score': 15.688383838383839, 'Answer Recall': 15.208333333333336}\n",
            "Results saved to ./results/llama31_8B_baseline.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 17.0, 'F1 Score': 23.05098039215686, 'Answer Recall': 22.95980392156863}\n",
            "Results saved to ./results/gemini_2_flash_baseline.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 15.0, 'F1 Score': 21.514637053767487, 'Answer Recall': 21.101960784313725}\n",
            "Results saved to ./results/gemini_2_flash_thinking_baseline.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 35.86206896551724, 'F1 Score': 43.73254327845981, 'Answer Recall': 44.35193905148266}\n",
            "Results saved to ./results/llama3_recursive_chroma_top3.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 34.827586206896555, 'F1 Score': 42.78371733294602, 'Answer Recall': 43.47728919153869}\n",
            "Results saved to ./results/llama3_recursive_chunk500_chroma_top3.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 34.0, 'F1 Score': 41.43821529347845, 'Answer Recall': 42.53382352941176}\n",
            "Results saved to ./results/llama3_recursive_chunk700_chroma_top3.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 33.0, 'F1 Score': 43.188807026307025, 'Answer Recall': 44.31470588235294}\n",
            "Results saved to ./results/llama3_recursive_chunk1000_chroma_top3.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 36.0, 'F1 Score': 43.03324802705608, 'Answer Recall': 43.687990196078424}\n",
            "Results saved to ./results/llama3_recursive_chunk1500_chroma_top3.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 36.0, 'F1 Score': 43.858048864924925, 'Answer Recall': 44.543907563025215}\n",
            "Results saved to ./results/llama3_recursive_chunk2000_chroma_top3.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 35.86206896551724, 'F1 Score': 43.73254327845981, 'Answer Recall': 44.35193905148266}\n",
            "Results saved to ./results/llama3_recursive_chroma_top3.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 34.0, 'F1 Score': 40.91700074119429, 'Answer Recall': 41.88504901960784}\n",
            "Results saved to ./results/llama3_character_chroma_top3.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 33.0, 'F1 Score': 41.65492323768185, 'Answer Recall': 43.095378151260505}\n",
            "Results saved to ./results/llama3_token_chroma_top3.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 35.0, 'F1 Score': 42.90643596169911, 'Answer Recall': 43.60273109243698}\n",
            "Results saved to ./results/llama3_semantic_chroma_top3.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 30.360934182590235, 'F1 Score': 35.704530957434685, 'Answer Recall': 35.91648973814579}\n",
            "Results saved to ./results/llama3_FAISS_all-mpnet-base-v2_top3.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 29.0, 'F1 Score': 35.34365079365079, 'Answer Recall': 35.99166666666666}\n",
            "Results saved to ./results/llama3_CHROMA_all-mpnet-base-v2_top3.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 37.567084078711986, 'F1 Score': 44.962934352469254, 'Answer Recall': 45.50633133396471}\n",
            "Results saved to ./results/llama3_FAISS_all-MiniLM-L6-v2_top3.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 35.031847133757964, 'F1 Score': 43.07032883869852, 'Answer Recall': 43.604197517648245}\n",
            "Results saved to ./results/llama3_CHROMA_all-MiniLM-L6-v2_top3.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 36.0, 'F1 Score': 44.33248556998556, 'Answer Recall': 45.5921568627451}\n",
            "Results saved to ./results/llama3_faiss_similarity_top3.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 30.0, 'F1 Score': 36.3375, 'Answer Recall': 37.435014005602234}\n",
            "Results saved to ./results/llama3_faiss_mmr_top3.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 40.0, 'F1 Score': 49.71904761904761, 'Answer Recall': 49.70563725490196}\n",
            "Results saved to ./results/llama3_faiss_no_rerank.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 33.0, 'F1 Score': 41.60873015873015, 'Answer Recall': 42.27647058823529}\n",
            "Results saved to ./results/llama3_faiss_rerank_ms-marco-MultiBERT-L-12.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 36.0, 'F1 Score': 45.173224728487874, 'Answer Recall': 44.16267507002801}\n",
            "Results saved to ./results/llama3_faiss_rerank_ms-marco-MiniLM-L-12-v2.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 29.0, 'F1 Score': 37.89743867243866, 'Answer Recall': 39.55098039215686}\n",
            "Results saved to ./results/llama3_faiss_test_no_hypo.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 30.0, 'F1 Score': 38.307503607503605, 'Answer Recall': 38.90364145658263}\n",
            "Results saved to ./results/llama3_faiss_test_hypo.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 41.0, 'F1 Score': 51.05185960992413, 'Answer Recall': 51.08529411764707}\n",
            "Results saved to ./results/Llama-3.2-3B-Instruct_rerank_false.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 43.0, 'F1 Score': 51.73331814384446, 'Answer Recall': 52.234418767507016}\n",
            "Results saved to ./results/Llama-3.1-8B-Instruct_rerank_false.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 33.0, 'F1 Score': 44.475361480624635, 'Answer Recall': 44.496008403361344}\n",
            "Results saved to ./results/Llama-3.2-3B-Instruct_rerank_ms-marco-MiniLM-L-12-v2.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 40.0, 'F1 Score': 48.2369252007023, 'Answer Recall': 49.26299019607843}\n",
            "Results saved to ./results/Llama-3.1-8B-Instruct_rerank_ms-marco-MiniLM-L-12-v2.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 37.0, 'F1 Score': 48.39047965425296, 'Answer Recall': 50.12916666666666}\n",
            "Results saved to ./results/gemini-2.0-flash_rerank_false.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 36.0, 'F1 Score': 44.153174603174605, 'Answer Recall': 45.04285714285714}\n",
            "Results saved to ./results/gemini-2.0-flash_rerank_ms-marco-MiniLM-L-12-v2.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 38.0, 'F1 Score': 47.588809338809334, 'Answer Recall': 49.5}\n",
            "Results saved to ./results/gemini-2.0-flash-thinking-exp-01-21_rerank_false.json\n",
            "[['April 26, 29, May 2, 4, 2025'], ['Dr. Kathleen Newman'], ['Frankel'], ['Nov 29, 2024 - Jan 1, 2025'], ['(Not specified in the provided text, but can be inferred as Duquesne Water Company in the original article)']]\n",
            "Loaded combined gold and generated answers from CSV files.\n",
            "Evaluation results: {'Exact Match': 40.0, 'F1 Score': 48.07936507936508, 'Answer Recall': 48.83333333333333}\n",
            "Results saved to ./results/gemini-2.0-flash-thinking-exp-01-21_rerank_ms-marco-MiniLM-L-12-v2.json\n"
          ]
        }
      ],
      "source": [
        "# Baseline performance\n",
        "run_evaluate(combined_dir=\"./output/llama32_3B_baseline.csv\", output_dir=\"./results/llama32_3B_baseline.json\")\n",
        "run_evaluate(combined_dir=\"./output/llama31_8B_baseline.csv\", output_dir=\"./results/llama31_8B_baseline.json\")\n",
        "run_evaluate(combined_dir=\"./output/gemini_2_flash_baseline.csv\", output_dir=\"./results/gemini_2_flash_baseline.json\")\n",
        "run_evaluate(combined_dir=\"./output/gemini_2_flash_thinking_baseline.csv\", output_dir=\"./results/gemini_2_flash_thinking_baseline.json\")\n",
        "run_evaluate(combined_dir=\"./output/llama3_recursive_chroma_top3.csv\", output_dir=\"./results/llama3_recursive_chroma_top3.json\")\n",
        "\n",
        "\n",
        "# For hyperparameter tuning on chunk size\n",
        "run_evaluate(combined_dir=\"./output/llama3_recursive_chunk500_chroma_top3.csv\", output_dir=\"./results/llama3_recursive_chunk500_chroma_top3.json\")\n",
        "run_evaluate(combined_dir=\"./output/llama3_recursive_chunk700_chroma_top3.csv\", output_dir=\"./results/llama3_recursive_chunk700_chroma_top3.json\")\n",
        "run_evaluate(combined_dir=\"./output/llama3_recursive_chunk1000_chroma_top3.csv\", output_dir=\"./results/llama3_recursive_chunk1000_chroma_top3.json\")\n",
        "run_evaluate(combined_dir=\"./output/llama3_recursive_chunk1500_chroma_top3.csv\", output_dir=\"./results/llama3_recursive_chunk1500_chroma_top3.json\")\n",
        "run_evaluate(combined_dir=\"./output/llama3_recursive_chunk2000_chroma_top3.csv\", output_dir=\"./results/llama3_recursive_chunk2000_chroma_top3.json\")\n",
        "\n",
        "\n",
        "# For hyperparameter tuning on splitter\n",
        "run_evaluate(combined_dir=\"./output/llama3_recursive_chroma_top3.csv\", output_dir=\"./results/llama3_recursive_chroma_top3.json\")\n",
        "run_evaluate(combined_dir=\"./output/llama3_character_chroma_top3.csv\", output_dir=\"./results/llama3_character_chroma_top3.json\")\n",
        "run_evaluate(combined_dir=\"./output/llama3_token_chroma_top3.csv\", output_dir=\"./results/llama3_token_chroma_top3.json\")\n",
        "run_evaluate(combined_dir=\"./output/llama3_semantic_chroma_top3.csv\", output_dir=\"./results/llama3_semantic_chroma_top3.json\")\n",
        "\n",
        "\n",
        "# For tuning embedding models and retriever types\n",
        "run_evaluate(combined_dir=\"./output/llama3_FAISS_all-mpnet-base-v2_top3.csv\", output_dir=\"./results/llama3_FAISS_all-mpnet-base-v2_top3.json\")\n",
        "run_evaluate(combined_dir=\"./output/llama3_CHROMA_all-mpnet-base-v2_top3.csv\", output_dir=\"./results/llama3_CHROMA_all-mpnet-base-v2_top3.json\")\n",
        "run_evaluate(combined_dir=\"./output/llama3_FAISS_all-MiniLM-L6-v2_top3.csv\", output_dir=\"./results/llama3_FAISS_all-MiniLM-L6-v2_top3.json\")\n",
        "run_evaluate(combined_dir=\"./output/llama3_CHROMA_all-MiniLM-L6-v2_top3.csv\", output_dir=\"./results/llama3_CHROMA_all-MiniLM-L6-v2_top3.json\")\n",
        "\n",
        "\n",
        "# For tuning retriever algorithms\n",
        "run_evaluate(combined_dir=\"./output/llama3_faiss_similarity_top3.csv\", output_dir=\"./results/llama3_faiss_similarity_top3.json\")\n",
        "run_evaluate(combined_dir=\"./output/llama3_faiss_mmr_top3.csv\", output_dir=\"./results/llama3_faiss_mmr_top3.json\")\n",
        "\n",
        "\n",
        "# For tuning reranking using faiss\n",
        "run_evaluate(combined_dir=\"./output/llama3_faiss_no_rerank.csv\", output_dir=\"./results/llama3_faiss_no_rerank.json\")\n",
        "run_evaluate(combined_dir=\"./output/llama3_faiss_rerank_ms-marco-MultiBERT-L-12.csv\", output_dir=\"./results/llama3_faiss_rerank_ms-marco-MultiBERT-L-12.json\")\n",
        "run_evaluate(combined_dir=\"./output/llama3_faiss_rerank_ms-marco-MiniLM-L-12-v2.csv\", output_dir=\"./results/llama3_faiss_rerank_ms-marco-MiniLM-L-12-v2.json\")\n",
        "\n",
        "\n",
        "# For tuning hypo_doc retrieval\n",
        "run_evaluate(combined_dir=\"./output/llama3_faiss_test_no_hypo.csv\", output_dir=\"./results/llama3_faiss_test_no_hypo.json\")\n",
        "run_evaluate(combined_dir=\"./output/llama3_faiss_test_hypo.csv\", output_dir=\"./results/llama3_faiss_test_hypo.json\")\n",
        "\n",
        "\n",
        "# Model comparsion\n",
        "run_evaluate(combined_dir=\"./output/Llama-3.2-3B-Instruct_rerank_false.csv\", output_dir=\"./results/Llama-3.2-3B-Instruct_rerank_false.json\")\n",
        "run_evaluate(combined_dir=\"./output/Llama-3.1-8B-Instruct_rerank_false.csv\", output_dir=\"./results/Llama-3.1-8B-Instruct_rerank_false.json\")\n",
        "run_evaluate(combined_dir=\"./output/Llama-3.2-3B-Instruct_rerank_ms-marco-MiniLM-L-12-v2.csv\", output_dir=\"./results/Llama-3.2-3B-Instruct_rerank_ms-marco-MiniLM-L-12-v2.json\")\n",
        "run_evaluate(combined_dir=\"./output/Llama-3.1-8B-Instruct_rerank_ms-marco-MiniLM-L-12-v2.csv\", output_dir=\"./results/Llama-3.1-8B-Instruct_rerank_ms-marco-MiniLM-L-12-v2.json\")\n",
        "run_evaluate(combined_dir=\"./output/gemini-2.0-flash_rerank_false.csv\", output_dir=\"./results/gemini-2.0-flash_rerank_false.json\")\n",
        "run_evaluate(combined_dir=\"./output/gemini-2.0-flash_rerank_ms-marco-MiniLM-L-12-v2.csv\", output_dir=\"./results/gemini-2.0-flash_rerank_ms-marco-MiniLM-L-12-v2.json\")\n",
        "run_evaluate(combined_dir=\"./output/gemini-2.0-flash-thinking-exp-01-21_rerank_false.csv\", output_dir=\"./results/gemini-2.0-flash-thinking-exp-01-21_rerank_false.json\")\n",
        "run_evaluate(combined_dir=\"./output/gemini-2.0-flash-thinking-exp-01-21_rerank_ms-marco-MiniLM-L-12-v2.csv\", output_dir=\"./results/gemini-2.0-flash-thinking-exp-01-21_rerank_ms-marco-MiniLM-L-12-v2.json\")\n",
        "\n",
        "\n",
        "# For running evaluation on the full 3900 test set\n",
        "# run_evaluate(combined_dir=\"./output/qa3000/llama3_faiss_rerank.csv\", output_dir=\"./results/qa3000/llama3_faiss_rerank.json\")\n",
        "# run_evaluate(combined_dir=\"./output/qa3000/llama3_faiss_rerank_sublink.csv\", output_dir=\"./results/qa3000/llama3_faiss_rerank_sublink.json\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
