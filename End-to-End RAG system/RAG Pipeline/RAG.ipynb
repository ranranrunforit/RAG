{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoJO-2CLihs3"
      },
      "source": [
        "# RAG Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuzVZ_C6qXCO",
        "outputId": "d956224e-e8af-44f4-d29e-12704e7f751d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "Successfully installed numpy-1.26.4\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.26.4 # downgrading the numpy version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP2p28VOvibg"
      },
      "source": [
        "After installing 1.26.4, you need to \"Restart Session\" and re-import numpy. It's strange that they haven't updated their release notes yet (https://colab.research.google.com/notebooks/relnotes.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PpEBqTFEsC4z"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YA9iyCbgDHV",
        "outputId": "8332e92c-3d15-4d07-9bd6-36f34c84bf0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n",
            "Collecting faiss-gpu-cu12\n",
            "  Downloading faiss_gpu_cu12-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (24.2)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (12.5.82)\n",
            "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from faiss-gpu-cu12) (12.5.3.2)\n",
            "Downloading faiss_gpu_cu12-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.9/47.9 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu-cu12\n",
            "Successfully installed faiss-gpu-cu12-1.10.0\n",
            "Collecting dotenv\n",
            "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
            "Collecting python-dotenv (from dotenv)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, dotenv\n",
            "Successfully installed dotenv-0.9.9 python-dotenv-1.1.0\n",
            "Collecting langchain_chroma\n",
            "  Downloading langchain_chroma-0.2.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 in /usr/local/lib/python3.11/dist-packages (from langchain_chroma) (0.3.49)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain_chroma) (1.26.4)\n",
            "Collecting chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0 (from langchain_chroma)\n",
            "  Downloading chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting build>=1.0.3 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.11.1)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading posthog-3.23.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (4.13.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.31.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.31.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.21.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.71.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.15.2)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (9.1.2)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.10.16)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (13.9.4)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain_chroma) (0.3.22)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain_chroma) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain_chroma) (24.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.95.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain_chroma) (3.0.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.3.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain_chroma) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain_chroma) (0.23.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.69.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading opentelemetry_proto-1.31.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-instrumentation==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.52b1)\n",
            "Collecting opentelemetry-util-http==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.17.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.30.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.1.0)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (2025.3.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain_chroma) (0.6.1)\n",
            "Downloading langchain_chroma-0.2.2-py3-none-any.whl (11 kB)\n",
            "Downloading chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.31.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl (31 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl (7.3 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.23.0-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53800 sha256=a279982e63ca9b6d023f954665ef1ab85258b194bccd9706b49b1007403ac876\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, uvloop, uvicorn, pyproject_hooks, overrides, opentelemetry-util-http, opentelemetry-proto, mmh3, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, coloredlogs, build, onnxruntime, kubernetes, fastapi, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb, langchain_chroma\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.6.3 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.12 httptools-0.6.4 humanfriendly-10.0 kubernetes-32.0.1 langchain_chroma-0.2.2 mmh3-5.1.0 monotonic-1.6 onnxruntime-1.21.0 opentelemetry-exporter-otlp-proto-common-1.31.1 opentelemetry-exporter-otlp-proto-grpc-1.31.1 opentelemetry-instrumentation-0.52b1 opentelemetry-instrumentation-asgi-0.52b1 opentelemetry-instrumentation-fastapi-0.52b1 opentelemetry-proto-1.31.1 opentelemetry-util-http-0.52b1 overrides-7.7.0 posthog-3.23.0 pypika-0.48.9 pyproject_hooks-1.2.0 starlette-0.46.1 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.4\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.51 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.51-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain<1.0.0,>=0.3.23 (from langchain-community)\n",
            "  Downloading langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.22)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain<1.0.0,>=0.3.23->langchain-community)\n",
            "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (2.11.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (4.13.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (0.4.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.23-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.51-py3-none-any.whl (423 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.3/423.3 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.49\n",
            "    Uninstalling langchain-core-0.3.49:\n",
            "      Successfully uninstalled langchain-core-0.3.49\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.7\n",
            "    Uninstalling langchain-text-splitters-0.3.7:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.7\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.22\n",
            "    Uninstalling langchain-0.3.22:\n",
            "      Successfully uninstalled langchain-0.3.22\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.23 langchain-community-0.3.21 langchain-core-0.3.51 langchain-text-splitters-0.3.8 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 typing-inspect-0.9.0\n",
            "Collecting langchain_experimental\n",
            "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain_experimental) (0.3.21)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.28 in /usr/local/lib/python3.11/dist-packages (from langchain_experimental) (0.3.51)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.8.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.22)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.26.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (4.13.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (2.11.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.8)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (0.4.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.14.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.3.1)\n",
            "Downloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_experimental\n",
            "Successfully installed langchain_experimental-0.3.4\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.12-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.3.51)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.70.0)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_openai) (0.3.22)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_openai) (4.13.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain_openai) (2.11.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain_openai) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain_openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.49->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.49->langchain_openai) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.49->langchain_openai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.3.0)\n",
            "Downloading langchain_openai-0.3.12-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain_openai\n",
            "Successfully installed langchain_openai-0.3.12 tiktoken-0.9.0\n",
            "Collecting flashrank\n",
            "  Downloading FlashRank-0.2.10-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from flashrank) (0.21.1)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.11/dist-packages (from flashrank) (1.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from flashrank) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from flashrank) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from flashrank) (4.67.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime->flashrank) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime->flashrank) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime->flashrank) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime->flashrank) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime->flashrank) (1.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->flashrank) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->flashrank) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->flashrank) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->flashrank) (2025.1.31)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->flashrank) (0.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->flashrank) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->flashrank) (2025.3.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->flashrank) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->flashrank) (4.13.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime->flashrank) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime->flashrank) (1.3.0)\n",
            "Downloading FlashRank-0.2.10-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: flashrank\n",
            "Successfully installed flashrank-0.2.10\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu\n",
        "!pip install faiss-gpu-cu12 # CUDA 12.x, Python 3.8+\n",
        "!pip install dotenv\n",
        "!pip install langchain_chroma\n",
        "!pip install langchain-community\n",
        "!pip install langchain_experimental\n",
        "!pip install langchain_openai\n",
        "!pip install flashrank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jyTcKVNM8UV1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "from flashrank import Ranker\n",
        "from langchain.retrievers.document_compressors import FlashrankRerank\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "def load_text_files(path: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Load text files from the given path.\n",
        "\n",
        "    Args:\n",
        "        path (str): The path to the directory or file containing the text files.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of text documents.\n",
        "    \"\"\"\n",
        "    docs = []\n",
        "\n",
        "    try:\n",
        "        if os.path.isdir(path):\n",
        "            # Iterate over files in the directory\n",
        "            for file_name in os.listdir(path):\n",
        "                if file_name.endswith(\".txt\"):\n",
        "                    file_path = os.path.join(path, file_name)\n",
        "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                        docs.append(file.read())\n",
        "        elif os.path.isfile(path) and path.endswith(\".txt\"):\n",
        "            # If the path is a file, directly read it\n",
        "            with open(path, 'r', encoding='utf-8') as file:\n",
        "                docs.append(file.read())\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading text files from {path}: {e}\")\n",
        "\n",
        "    return docs\n",
        "\n",
        "def format_retrieved_docs(docs: List[str]) -> str:\n",
        "    \"\"\"\n",
        "    Format the retrieved documents in reverse order.\n",
        "\n",
        "    Args:\n",
        "        docs (list): A list of documents.\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted string with contexts.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        docs = reversed(docs)\n",
        "        return \"\\n\\n\".join([f\"Context {i+1}: {doc}\" for i, doc in enumerate(docs)])\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error formatting retrieved documents: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def rerank_docs(query: str, retriever, rerank_model_name: str, k: int = 3) -> List[str]:\n",
        "    \"\"\"\n",
        "    Rerank the retrieved documents based on the query using Flashrank.\n",
        "\n",
        "    Args:\n",
        "        query (str): The query to rerank documents for.\n",
        "        retriever: The base retriever object.\n",
        "        rerank_model_name (str): The name of the rerank model.\n",
        "        k (int): The number of top documents to rerank.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of reranked documents.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ranker = Ranker(model_name=rerank_model_name)\n",
        "        compressor = FlashrankRerank(top_n=k, model=rerank_model_name)\n",
        "        compression_retriever = ContextualCompressionRetriever(\n",
        "            base_compressor=compressor, base_retriever=retriever\n",
        "        )\n",
        "        return compression_retriever.invoke(query)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error reranking documents for query '{query}': {e}\")\n",
        "        return []\n",
        "\n",
        "def get_hypo_doc(query: str, generation_pipe) -> str:\n",
        "    \"\"\"\n",
        "    Generate a hypothesis document for the given query using the language model.\n",
        "\n",
        "    Args:\n",
        "        query (str): The query to generate a hypothesis for.\n",
        "        generation_pipe: The language model pipeline.\n",
        "\n",
        "    Returns:\n",
        "        str: The hypothesis document or the original query if unavailable.\n",
        "    \"\"\"\n",
        "    template = \"\"\"Imagine you are an expert providing a detailed and factual explanation in response to the query '{query}'.\n",
        "    Your response should include all key points that would be found in a top search result, without adding any personal opinions, commentary, or experiences.\n",
        "    Do not include any subjective phrases such as 'I think', 'I believe', or 'I am not sure'. Do not apologize, hedge, or express uncertainty.\n",
        "    The response should be structured as an objective, factual explanation only, without any conversational elements or chatting.\n",
        "    If you are truly uncertain and cannot provide an accurate answer, simply respond with: 'Unavailable: {query}'.\n",
        "    Otherwise, answer confidently with only the relevant information.\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": template.format(query=query)}]\n",
        "\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            hypo_doc = generation_pipe(messages, max_new_tokens=100, return_full_text=False)[0][\"generated_text\"]\n",
        "        logging.info(f\"Generated hypothesis document for query: {query}\")\n",
        "        # print(\"Question:\", query)\n",
        "        # print(\"Hypothesis Document:\", hypo_doc)\n",
        "        if hypo_doc.startswith(\"Unavailable\"):\n",
        "            logging.warning(f\"Hypothesis unavailable for query: {query}\")\n",
        "            return query\n",
        "        return hypo_doc\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating hypothesis document for query '{query}': {e}\")\n",
        "        return query\n",
        "\n",
        "def answer_generation(\n",
        "    qa_df: pd.DataFrame, output_file: str, retriever, generation_pipe,\n",
        "    prompt, rerank: bool, rerank_model_name: str, hypo: bool, top_k_rerank: int = 3\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate answers for the given questions using the retriever and the generation pipeline.\n",
        "\n",
        "    Args:\n",
        "        qa_df (pd.DataFrame): DataFrame containing questions and other metadata.\n",
        "        output_file (str): Path to save the generated answers.\n",
        "        retriever: A retriever object to retrieve documents.\n",
        "        generation_pipe: A pipeline object for text generation.\n",
        "        prompt: A ChatPromptTemplate object for generating prompts.\n",
        "        rerank (bool): Whether to rerank retrieved documents.\n",
        "        rerank_model_name (str): The name of the rerank model.\n",
        "        hypo (bool): Whether to generate a hypothesis document.\n",
        "        top_k_rerank (int): Number of top documents to rerank.\n",
        "    \"\"\"\n",
        "    logging.info(\"Starting answer generation...\")\n",
        "\n",
        "    # Check if the output file exists\n",
        "    if not os.path.exists(output_file):\n",
        "        with open(output_file, 'w') as f_out:\n",
        "            f_out.write(\",\".join(list(qa_df.columns) + [\"Generated_Answer\"]) + \"\\n\")\n",
        "            start_idx = 0\n",
        "    else:\n",
        "        # Calculate the number of rows in the output file\n",
        "        with open(output_file, 'r') as f_out:\n",
        "            num_rows = sum(1 for line in f_out)\n",
        "            start_idx = num_rows - 1\n",
        "\n",
        "    # Iterate over the DataFrame\n",
        "    with open(output_file, 'a') as f_out:\n",
        "        for idx, row in tqdm(qa_df.iterrows(), total=len(qa_df)):\n",
        "            if idx < start_idx:\n",
        "                continue\n",
        "\n",
        "            query = row[\"Question\"]\n",
        "            if hypo:\n",
        "                query = get_hypo_doc(query, generation_pipe)\n",
        "\n",
        "            # Retrieve documents\n",
        "            try:\n",
        "                if rerank:\n",
        "                    logging.info(f\"Reranking documents for query: {query}\")\n",
        "                    retrieved_docs = rerank_docs(query, retriever, rerank_model_name, k=top_k_rerank)\n",
        "                else:\n",
        "                    retrieved_docs = retriever.invoke(query)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error retrieving documents for query '{query}': {e}\")\n",
        "                continue\n",
        "\n",
        "            # Format the documents\n",
        "            context = format_retrieved_docs(retrieved_docs)\n",
        "\n",
        "            # Create the full prompt\n",
        "            prompt_messages = prompt.format_messages(context=context, question=row[\"Question\"])\n",
        "            full_prompt = \"\\n\".join(message.content for message in prompt_messages)\n",
        "\n",
        "            # Generate the answer\n",
        "            try:\n",
        "                messages = [{\"role\": \"user\", \"content\": full_prompt}]\n",
        "                with torch.no_grad():\n",
        "                    llm_output = generation_pipe(\n",
        "                        messages, max_new_tokens=20, return_full_text=False\n",
        "                    )[0][\"generated_text\"]\n",
        "\n",
        "                row[\"Generated_Answer\"] = llm_output\n",
        "                pd.DataFrame([row]).to_csv(f_out, header=False, index=False)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error generating answer for query '{query}': {e}\")\n",
        "                continue\n",
        "\n",
        "            # Clear cache\n",
        "            del retrieved_docs, context, prompt_messages, full_prompt, messages, llm_output\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# Constants\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You are an expert assistant answering factual questions about Pittsburgh or Carnegie Mellon University (CMU).\n",
        "Use the retrieved information to give a detailed and helpful answer. If the provided context does not contain the answer, leverage your pretraining knowledge to provide the correct answer.\n",
        "If you truly do not know, just say \"I don't know.\"\n",
        "\n",
        "Important Instructions:\n",
        "- Answer concisely without repeating the question.\n",
        "- Use the provided context if relevant; otherwise, rely on your pretraining knowledge.\n",
        "- Do **not** use complete sentences. Provide only the word, name, date, or phrase that directly answers the question. For example, given the question \"When was Carnegie Mellon University founded?\", you should only answer \"1900\".\n",
        "\n",
        "Examples:\n",
        "Question: Who is Pittsburgh named after?\n",
        "Answer: William Pitt\n",
        "Question: What famous machine learning venue had its first conference in Pittsburgh in 1980?\n",
        "Answer: ICML\n",
        "Question: What musical artist is performing at PPG Arena on October 13?\n",
        "Answer: Billie Eilish\n",
        "\n",
        "Context: \\n\\n {context} \\n\\n\n",
        "Question: {question} \\n\\n\n",
        "Answer:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfgwHq9Kk5hV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "from dotenv import load_dotenv\n",
        "from huggingface_hub import login\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import (\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    CharacterTextSplitter,\n",
        "    TokenTextSplitter\n",
        ")\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    PromptTemplate\n",
        ")\n",
        "\n",
        "# ========================================\n",
        "# Helper Functions\n",
        "# ========================================\n",
        "def str2bool(value):\n",
        "    if isinstance(value, bool):\n",
        "        return value\n",
        "    if value.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif value.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "        return False\n",
        "    else:\n",
        "        raise ValueError('Boolean value expected.')\n",
        "\n",
        "# ========================================\n",
        "# Main Function for Jupyter/IPython\n",
        "# ========================================\n",
        "def run_RAG(\n",
        "    model_name = \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    dtype = \"float16\", # or torch.bfloat16\n",
        "    embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    embedding_dim = 384,\n",
        "    splitter_type = \"recursive\", # or \"character\", \"token\", \"semantic\"\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 200,\n",
        "    text_files_path = \"./data/scraped/scraped_all\",\n",
        "    sublink_files_path = \"./data/scraped/scraped_text_data\",\n",
        "    sublink_files_nums = 0,\n",
        "    retriever_type = \"FAISS\", # or \"CHROMA\"\n",
        "    retriever_algorithm = \"similarity\", # or \"mmr\"\n",
        "    rerank = False,\n",
        "    rerank_model_name = \"ms-marco-MultiBERT-L-12\",\n",
        "    top_k_search = 3,\n",
        "    top_k_rerank = 3,\n",
        "    hypo = False,\n",
        "    qes_file_path = \"./data/annotated/QA_pairs_1.csv\",\n",
        "    output_file = \"./output/results.json\",\n",
        "    qa_nums = 100\n",
        "):\n",
        "    # Step 0: Load environment variables\n",
        "    load_dotenv()\n",
        "\n",
        "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "    os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv('LANGCHAIN_API_KEY') # os.getenv('LANGCHAIN_API_KEY')\n",
        "    os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "    os.environ[\"LANGCHAIN_PROJECT\"] = \"RAGmodel\"\n",
        "    os.environ[\"USER_AGENT\"] = \"LangChain/1.0 (+https://www.langchain.com)\"\n",
        "\n",
        "    login(token = os.getenv('HUGGINGFACE_TOKEN')) # os.getenv('HUGGINGFACE_TOKEN')\n",
        "\n",
        "    # Set model name, precision, and other parameters\n",
        "    dtype = torch.float16 if dtype == \"float16\" else torch.bfloat16\n",
        "    random.seed(42)\n",
        "\n",
        "    # Check if rerank is set to True\n",
        "    if rerank:\n",
        "        print(\"Reranking is set to True.\")\n",
        "\n",
        "    # Step 1: Initialize the Hugging Face model as your LLM\n",
        "    print(\"Initializing the Hugging Face model...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name, torch_dtype=dtype, device_map=\"cuda:0\"\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"left\"\n",
        "\n",
        "    generation_pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        torch_dtype=dtype\n",
        "    )\n",
        "    print(\"Model initialized successfully!\")\n",
        "\n",
        "    # Step 2: Load the Sentence Transformers model for embeddings\n",
        "    docs_length = f\"main160_sublink{sublink_files_nums}\"\n",
        "    model_name_str = embedding_model_name.split('/')[-1]\n",
        "    embeddings_file_path = f\"./data/embeddings/embeddings_{model_name_str}_{docs_length}_{splitter_type}_{retriever_type}_{chunk_size}_{chunk_overlap}.npy\"\n",
        "    splits_file_path = f\"./data/embeddings/splits_{model_name_str}_{docs_length}_{splitter_type}_{retriever_type}_{chunk_size}_{chunk_overlap}.pkl\"\n",
        "    embeddings = None\n",
        "    splits = None\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
        "    print(f\"Start loading QA from {qes_file_path}\")\n",
        "    qa_test_data_path = qes_file_path\n",
        "    qa_df = pd.read_csv(qa_test_data_path)\n",
        "    print(len(qa_df))\n",
        "    if len(qa_df) != 574:\n",
        "        qa_df = qa_df.sample(qa_nums, random_state=221)\n",
        "    print(f\"Loaded {len(qa_df)} QAs\")\n",
        "\n",
        "    # Dynamically determine embedding dimensionality\n",
        "    embedding_dim = embedding_model.client.get_sentence_embedding_dimension()\n",
        "\n",
        "    if not os.path.exists(embeddings_file_path):\n",
        "        # Step 3: Load the text files for building the index and QA evaluation\n",
        "        print(f\"Start loading texts from {text_files_path}\")\n",
        "        # Step 4: Split the documents into smaller chunks\n",
        "        # Wrap text strings in Document objects\n",
        "        docs = load_text_files(path=text_files_path)\n",
        "        documents = [Document(page_content=text) for text in tqdm(docs, desc=\"Wrapping text in Document objects\")]\n",
        "        del docs\n",
        "\n",
        "        if sublink_files_nums != 0:\n",
        "            sublink_file_store_path = \"./data/embeddings/sublink_docs.pkl\"\n",
        "            if os.path.exists(sublink_file_store_path):\n",
        "                print(f\"Start loading sublink files from {sublink_file_store_path}\")\n",
        "                with open(sublink_file_store_path, \"rb\") as f:\n",
        "                    all_sublink_docs = pickle.load(f)\n",
        "            else:\n",
        "                print(f\"Start reading all sublink files\")\n",
        "                all_sublink_docs = load_text_files(path=sublink_files_path)\n",
        "                print(f\"Finish loading {len(all_sublink_docs)} sublinks, now store it\")\n",
        "                with open(sublink_file_store_path, 'wb') as f:\n",
        "                    pickle.dump(all_sublink_docs, f)\n",
        "                print(f\"Store all sublink file in {sublink_file_store_path}\")\n",
        "\n",
        "            sampled_sublink_docs = random.sample(all_sublink_docs, sublink_files_nums)\n",
        "            documents.extend([Document(page_content=text) for text in tqdm(sampled_sublink_docs, desc=\"Wrapping text in Document objects\")])\n",
        "            del sampled_sublink_docs\n",
        "            del all_sublink_docs\n",
        "\n",
        "        if splitter_type == \"recursive\":\n",
        "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        elif splitter_type == \"character\":\n",
        "            text_splitter = CharacterTextSplitter(separator=\" \", chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        elif splitter_type == \"token\":\n",
        "            text_splitter = TokenTextSplitter(chunk_size=int(chunk_size / 4), chunk_overlap=int(chunk_overlap / 4))\n",
        "        elif splitter_type == \"semantic\":\n",
        "            text_splitter = SemanticChunker(\n",
        "                embeddings=embedding_model,\n",
        "                breakpoint_threshold_type=\"percentile\",\n",
        "                breakpoint_threshold_amount=80\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"Invalid splitter type. Please choose between recursive, character, token, or semantic.\")\n",
        "\n",
        "        splits = text_splitter.split_documents(documents)\n",
        "        del documents\n",
        "        print(f\"End splitting texts -- Number of splits: {len(splits)}\")\n",
        "\n",
        "        # Step 5: Create Chroma vectorstore with embeddings from Sentence Transformers\n",
        "        # Generate embeddings with the correct dimensionality\n",
        "        embeddings = embedding_model.embed_documents([doc.page_content for doc in tqdm(splits, desc=\"Embedding texts\")])\n",
        "        print(f\"Generated embeddings with dimensionality: {embedding_dim}\")\n",
        "        print(f\"End embedding texts\")\n",
        "\n",
        "        # Free GPU cache after generating embeddings\n",
        "        torch.cuda.empty_cache()\n",
        "        print(f\"Start saving embeddings and splits\")\n",
        "        np.save(embeddings_file_path, embeddings)\n",
        "        with open(splits_file_path, 'wb') as f:\n",
        "            pickle.dump(splits, f)\n",
        "        print(f\"Embeddings saved in {embeddings_file_path}, splits saved in {splits_file_path}\")\n",
        "    else:\n",
        "        print(f\"Embeddings already exist! Loading embeddings with dimensionality: {embedding_dim}\")\n",
        "        # Step 1: Load embeddings from the saved NumPy file\n",
        "        embeddings = np.load(embeddings_file_path)\n",
        "        with open(splits_file_path, 'rb') as f:\n",
        "            splits = pickle.load(f)\n",
        "        # Step 2: Load document metadata if needed\n",
        "        # doc_metadata = np.load(\"doc_metadata.npy\", allow_pickle=True)\n",
        "        print(\"End loading\")\n",
        "\n",
        "    # Step 6: Create the RAG prompting pipeline\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=['context', 'question'],\n",
        "        template=PROMPT_TEMPLATE\n",
        "    )\n",
        "\n",
        "    # Update the HumanMessagePromptTemplate with the new PromptTemplate\n",
        "    human_message_template = HumanMessagePromptTemplate(prompt=prompt_template)\n",
        "\n",
        "    # Update the ChatPromptTemplate with the modified message\n",
        "    chat_prompt_template = ChatPromptTemplate(\n",
        "        input_variables=['context', 'question'],\n",
        "        messages=[human_message_template]\n",
        "    )\n",
        "    prompt = chat_prompt_template\n",
        "\n",
        "\n",
        "    # Step 7: Generate answers for the questions\n",
        "    print(\"Building the vectorstore \", retriever_type, \"...\")\n",
        "    if retriever_type == \"CHROMA\":\n",
        "        retriever = Chroma.from_documents(documents=dsplits, embeding=embedding_model, collection_name=\"collectionChroma\").as_retriever(search_type=retriever_algorithm, search_kwargs={'k': top_k_search})\n",
        "    elif retriever_type == \"FAISS\":\n",
        "        # embeddings_np = np.array(embeddings).astype(\"float32\")\n",
        "        retriever = FAISS.from_documents(splits, embedding_model).as_retriever(search_type=retriever_algorithm, search_kwargs={\"k\": top_k_search})\n",
        "    else:\n",
        "        raise ValueError(\"Invalid retriever type. Please choose between FAISS or CHROMA.\")\n",
        "\n",
        "    print(\"Retriever built successfully!\")\n",
        "    # Free GPU cache after generating embeddings\n",
        "    torch.cuda.empty_cache()\n",
        "    del splits\n",
        "\n",
        "    answer_generation(\n",
        "        qa_df, output_file, retriever,\n",
        "        generation_pipe, prompt, rerank, rerank_model_name, hypo, top_k_rerank=top_k_rerank\n",
        "    )\n",
        "\n",
        "    print(f\"QA evaluation completed! Results saved to {output_file}\")\n",
        "\n",
        "\n",
        "\n",
        "# The all-* models were trained on all available training data (more than 1 billion training pairs)\n",
        "# and are designed as general purpose models.\n",
        "# The all-mpnet-base-v2 model provides the best quality, while all-MiniLM-L6-v2 is 5 times faster and still offers good quality.\n",
        "\n",
        "# run_RAG(\n",
        "#     model_name=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "#     dtype=\"float16\",\n",
        "#     embedding_model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
        "#     embedding_dim = 768,\n",
        "#     text_files_path=\"./data/scraped/scraped_all\",\n",
        "#     qes_file_path=\"./data/annotated/QA_pairs_1.csv\",\n",
        "#     output_file=\"./output/results.json\",\n",
        "#     qa_nums=100\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxPDfgizjbqt"
      },
      "outputs": [],
      "source": [
        "def run_rag_strategy(strategy):\n",
        "    \"\"\"\n",
        "    Run the RAG pipeline with different strategies.\n",
        "\n",
        "    Args:\n",
        "        strategy (int): The strategy number to execute.\n",
        "    \"\"\"\n",
        "    if strategy == 0:\n",
        "        # Strategy 0: Default setting\n",
        "        print(f\"\\n==============================\")\n",
        "        print(f\"Running Strategy 0 Default setting.\")\n",
        "        print(f\"==============================\\n\")\n",
        "        run_RAG(\n",
        "            model_name=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "            dtype=\"float16\",\n",
        "            embedding_model_name=\"sentence-transformers/all-MiniLM-L12-v2\",\n",
        "            embedding_dim=384,\n",
        "            splitter_type=\"recursive\",\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            text_files_path=\"./data/scraped/scraped_all\",\n",
        "            retriever_type=\"CHROMA\",\n",
        "            top_k_search=3,\n",
        "            output_file=\"./output/llama3_recursive_chroma_top3.csv\"\n",
        "        )\n",
        "\n",
        "    elif strategy == 1:\n",
        "        # Strategy 1: Compare different chunk sizes\n",
        "        print(f\"\\n==============================\")\n",
        "        print(f\"Running Strategy 1 Compare different chunk sizes.\")\n",
        "        print(f\"==============================\\n\")\n",
        "        chunk_sizes = [500, 700, 1000, 1500, 2000]\n",
        "        for chunk_size in chunk_sizes:\n",
        "            output_file = f\"./output/llama3_recursive_chunk{chunk_size}_chroma_top3.csv\"\n",
        "            run_RAG(\n",
        "                model_name=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "                dtype=\"float16\",\n",
        "                embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                embedding_dim=384,\n",
        "                splitter_type=\"recursive\",\n",
        "                chunk_size=chunk_size,\n",
        "                chunk_overlap=200,\n",
        "                text_files_path=\"./data/scraped/scraped_all\",\n",
        "                retriever_type=\"CHROMA\",\n",
        "                top_k_search=3,\n",
        "                output_file=output_file\n",
        "            )\n",
        "\n",
        "    elif strategy == 2:\n",
        "        # Strategy 2: Compare different splitter types\n",
        "        print(f\"\\n==============================\")\n",
        "        print(f\"Running Strategy 2 Compare different splitter types.\")\n",
        "        print(f\"==============================\\n\")\n",
        "        splitter_types = [\"recursive\", \"semantic\", \"token\", \"character\"]\n",
        "        for splitter_type in splitter_types:\n",
        "            output_file = f\"./output/llama3_{splitter_type}_chroma_top3.csv\"\n",
        "            run_RAG(\n",
        "                model_name=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "                dtype=\"float16\",\n",
        "                embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                embedding_dim=384,\n",
        "                splitter_type=splitter_type,\n",
        "                chunk_size=1000,\n",
        "                chunk_overlap=200,\n",
        "                text_files_path=\"./data/scraped/scraped_all\",\n",
        "                retriever_type=\"CHROMA\",\n",
        "                top_k_search=3,\n",
        "                output_file=output_file\n",
        "            )\n",
        "\n",
        "    elif strategy == 3:\n",
        "        # Strategy 3 A: Compare retriever types and embedding models\n",
        "        print(f\"\\n==============================\")\n",
        "        print(f\"Running Strategy 3A Compare retriever types and embedding models.\")\n",
        "        print(f\"==============================\\n\")\n",
        "        retriever_types = [\"FAISS\", \"CHROMA\"]\n",
        "        for retriever_type in retriever_types:\n",
        "            output_file = f\"./output/llama3_{retriever_type}_all-MiniLM-L6-v2_top3.csv\"\n",
        "            print(f\"Running with retriever_type={retriever_type}, embedding_model=all-MiniLM-L6-v2, saving to {output_file}\")\n",
        "            run_RAG(\n",
        "                model_name=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "                dtype=\"float16\",\n",
        "                embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                embedding_dim=384,\n",
        "                splitter_type=\"recursive\",\n",
        "                chunk_size=1000,\n",
        "                chunk_overlap=200,\n",
        "                text_files_path=\"./data/scraped/scraped_all\",\n",
        "                retriever_type=retriever_type,\n",
        "                top_k_search=3,\n",
        "                output_file=output_file\n",
        "            )\n",
        "\n",
        "    elif strategy == 4:\n",
        "        # Strategy 3 B: Compare retriever types and embedding models\n",
        "        print(f\"\\n==============================\")\n",
        "        print(f\"Running Strategy 3B Compare retriever types and embedding models.\")\n",
        "        print(f\"==============================\\n\")\n",
        "        retriever_types = [\"FAISS\", \"CHROMA\"]\n",
        "        for retriever_type in retriever_types:\n",
        "            output_file = f\"./output/llama3_{retriever_type}_all-mpnet-base-v2_top3.csv\"\n",
        "            print(f\"Running with retriever_type={retriever_type}, embedding_model=all-mpnet-base-v2, saving to {output_file}\")\n",
        "            run_RAG(\n",
        "                model_name=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "                dtype=\"float16\",\n",
        "                embedding_model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
        "                embedding_dim=768,\n",
        "                splitter_type=\"recursive\",\n",
        "                chunk_size=1000,\n",
        "                chunk_overlap=200,\n",
        "                text_files_path=\"./data/scraped/scraped_all\",\n",
        "                retriever_type=retriever_type,\n",
        "                top_k_search=3,\n",
        "                output_file=output_file\n",
        "            )\n",
        "\n",
        "    elif strategy == 5:\n",
        "        # Strategy 4: Compare reranking models and no reranking\n",
        "        print(f\"\\n==============================\")\n",
        "        print(f\"Running Strategy 4 Compare reranking models and no reranking.\")\n",
        "        print(f\"==============================\\n\")\n",
        "        rerank_models = [\"ms-marco-MiniLM-L-12-v2\", \"ms-marco-MultiBERT-L-12\"]\n",
        "\n",
        "        for rerank in [True, False]:\n",
        "            if rerank:\n",
        "                # When rerank=True, iterate over the rerank models\n",
        "                for rerank_model_name in rerank_models:\n",
        "                    output_file = f\"./output/llama3_faiss_rerank_{rerank_model_name}.csv\"\n",
        "                    print(f\"Running with rerank=True, rerank_model={rerank_model_name}, saving to {output_file}\")\n",
        "                    run_RAG(\n",
        "                        model_name=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "                        dtype=\"float16\",\n",
        "                        embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                        embedding_dim=384,\n",
        "                        splitter_type=\"recursive\",\n",
        "                        chunk_size=1000,\n",
        "                        chunk_overlap=200,\n",
        "                        text_files_path=\"./data/scraped/scraped_all\",\n",
        "                        qes_file_path=\"./data/annotated/QA_pairs_1.csv\",\n",
        "                        qa_nums=100,\n",
        "                        retriever_type=\"FAISS\",\n",
        "                        top_k_search=10,\n",
        "                        top_k_rerank=3,\n",
        "                        rerank=True,\n",
        "                        rerank_model_name=rerank_model_name,\n",
        "                        output_file=output_file\n",
        "                    )\n",
        "            else:\n",
        "                # When rerank=False, no rerank model is used\n",
        "                output_file = \"./output/llama3_faiss_no_rerank.csv\"\n",
        "                print(f\"Running with rerank=False, saving to {output_file}\")\n",
        "                run_RAG(\n",
        "                    model_name=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "                    dtype=\"float16\",\n",
        "                    embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                    embedding_dim=384,\n",
        "                    splitter_type=\"recursive\",\n",
        "                    chunk_size=1000,\n",
        "                    chunk_overlap=200,\n",
        "                    text_files_path=\"./data/scraped/scraped_all\",\n",
        "                    qes_file_path=\"./data/annotated/QA_pairs_1.csv\",\n",
        "                    qa_nums=100,\n",
        "                    retriever_type=\"FAISS\",\n",
        "                    top_k_search=10,\n",
        "                    rerank=False,\n",
        "                    output_file=output_file\n",
        "                )\n",
        "\n",
        "    elif strategy == 6:\n",
        "        # Strategy 5: Compare retriever algorithms\n",
        "        print(f\"\\n==============================\")\n",
        "        print(f\"Running Strategy 5 Compare retriever algorithms.\")\n",
        "        print(f\"==============================\\n\")\n",
        "        retriever_algorithms = [\"similarity\", \"mmr\"]\n",
        "        for retriever_algorithm in retriever_algorithms:\n",
        "            output_file = f\"./output/llama3_faiss_{retriever_algorithm}_top3.csv\"\n",
        "            run_RAG(\n",
        "                model_name=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "                dtype=\"float16\",\n",
        "                embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                embedding_dim=384,\n",
        "                splitter_type=\"recursive\",\n",
        "                chunk_size=1000,\n",
        "                chunk_overlap=200,\n",
        "                text_files_path=\"./data/scraped/scraped_all\",\n",
        "                retriever_type=\"FAISS\",\n",
        "                top_k_search=3,\n",
        "                retriever_algorithm=retriever_algorithm,\n",
        "                output_file=output_file\n",
        "            )\n",
        "\n",
        "    elif strategy == 7:\n",
        "        # Strategy 6: Compare with/without hypothesis generation\n",
        "        print(f\"\\n==============================\")\n",
        "        print(f\"Running Strategy 6 Compare with/without hypothesis generation.\")\n",
        "        print(f\"==============================\\n\")\n",
        "        for hypo in [True, False]:\n",
        "              hypo_str = \"hypo\" if hypo else \"no_hypo\"\n",
        "              output_file = f\"./output/llama3_faiss_test_{hypo_str}.csv\"\n",
        "              print(f\"Running with hypo={hypo}, saving to {output_file}\")\n",
        "              run_RAG(\n",
        "                  model_name=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "                  dtype=\"float16\",\n",
        "                  embedding_model_name=\"sentence-transformers/all-MiniLM-L12-v2\",\n",
        "                  embedding_dim=384,\n",
        "                  splitter_type=\"recursive\",\n",
        "                  chunk_size=1000,\n",
        "                  chunk_overlap=200,\n",
        "                  text_files_path=\"./data/scraped/scraped_all\",\n",
        "                  retriever_type=\"FAISS\",\n",
        "                  top_k_search=3,\n",
        "                  hypo=hypo,\n",
        "                  output_file=output_file\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid strategy number: {strategy}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fee76b55e76e493281fd1ef9e29866ea",
            "90a1c9987805401ba23a9989c644e0be",
            "b1944756dd4c472cae4e2ffb8095bece",
            "35f02535e75348ef8bfc81499b57bee5",
            "281e27a0671a4194812bbda8b4e95dd0",
            "731bedb25efb4a3ea6da2cdf90a94e4b",
            "164be7ae4229415bbd1a0b0ff5d2d7e7",
            "d40c37839d344faf98e45fa24eb9fe11",
            "0231327f6ac64e4dbbfc12c46fc56fa5",
            "1ca5d9faf2934725a7fa2a080e1874db",
            "0000febcd3bf4af7a1cc9dfd0b816cc4",
            "872fe11395044138ac44658fc2864fad",
            "a6e3af4fd65a448eafa312500e3d5cc4",
            "405e05d24ed8456c9b617e386c5071c7",
            "56e76348f4604d3085270a6bc2c725ba",
            "22ee5a5228cc48c8b35a99d5fffbe521",
            "61c0fcf283c340c09b23ad49ec1afdde",
            "2ff930f97f8b4d93870b88efbb1b9114",
            "3ad87e039e6a42fba6978f14bf58e2cc",
            "403e449e820a4839aabb763498bb1117",
            "4ec697fef27648b986dcc9d58bbe0204",
            "00e637794edb4a74b7ee4a3453a6baa8",
            "0b54860eec4843b6997ce7cdd668c253",
            "c6c58c19daaf4fce93b0fc7b2feaa155",
            "81afad224c194dc18aabb5b7ee666a1d",
            "aeb2bdac866e4f2d9c5e5f1d32bcc561",
            "05187dbc933c4f5bb31f831a63fda45b",
            "3ca9059014e147a9901e7654c15cd61c",
            "f343a001e94644238fc34be1749344c6",
            "a29c8795120d42c38b92cd10c9357e1e",
            "3cb3a563aa1642698b3e9f19680ee59d",
            "1a5c339638d04fdcb4c16ef6737208d5",
            "c0db244a837e4e34a662f540793abb38",
            "9fea5d1879a344a49ffcaa048fa5b6a4",
            "bd449d50a93e424bbe39eb2eb8ef4062",
            "cf68d7dd51f041db9d1d56b562788f95",
            "19f3f8a056c747d49c941b2ebaf1de39",
            "95741d00b9f94ca395a52e176d67e73f",
            "c39e1dfbaaa04a969285549bb63a90b3",
            "707906e4d61e44cebe532fad9f627abd",
            "a1fd6a876646482b9c60fd0dc0413c5c",
            "1c3ce4a40d184747ba3681c1290385ce",
            "6ecad364423b4c9aab2208c919383827",
            "7ee03f4e8bad4216b3c87690e20a174f",
            "a3f9182e6e2540e89bf491785d6853c0",
            "fb01c9003c0c4a26b7ee83e7b742195d",
            "f46b583e947343418ad42da43f3d6833",
            "02cf1d5db8c24662b4d08469dd87d240",
            "03cff8b19d17403d97a5f1975b52877d",
            "ef4b0417654d400d8f03d9191ff7d4ff",
            "44cf7fb9a2ea46258825bf70471acbe8",
            "88251a2ddf5a45749ab689d4c9e5e035",
            "85aa1504ff134514a55a54141e9031fc",
            "b637fc5dffca4c9dbc3ecb0a2ab6f4d8",
            "70bfb6ff9907412da617c9ed4b2081cd",
            "b29fd846cd5941ebba782caf8afdf477",
            "a675a00c805f4761bca41956e47aada4",
            "fa31b1919f26480abc41797c2c7c6f2c",
            "c9abc4f7a00a4910a6c64dcd2004162f",
            "dfcc6861022e464094b17ac6198c6be4",
            "f7e41fe5bade46aabae11320ed16a804",
            "2ff9a88c70ce42198a663d53fb432db1",
            "d7bb77aff18e4f85a7d03ef404e936a8",
            "aae67f7961084fce92a766432fcdb9fd",
            "036f32074c094d9cbd7407ac4e6997bc",
            "de733ee808854c9388e9aa2a65ac25d2",
            "d89c9c70a7f4424d94f61e9e3b7d488f",
            "85ba12bcd48a484e96eb00c719936207",
            "8edc621f53564fa78b613ed65091bce7",
            "fdfcf9fcea964959903e21fd1f447864",
            "a532f9f14c224cdb920cac148602e319",
            "4cb4205f55db43a996684e5962575b79",
            "8a8edba2997643eeb6910879c7fcdc94",
            "4210c5c6da06489797dbac2f72527b18",
            "9e031d96614f4fc299959881486d2fe7",
            "787f4cbdcb0c4eb19d9353b1292370f1",
            "784adf4e9ee244748d9aa91baa15d86f",
            "e486c58b91bc407d9ef4c5c82d6bf1bc",
            "f9381163b6184de7aca94ad05d8fd851",
            "5dba9162847d4d7381bbdf7711c82269",
            "154f404e0ef84f988531d6bfa86389f0",
            "1867cd9611dc4f4c8473e1ffa31cb613",
            "1ed9d70697bf47cb84595844f2f13ff6",
            "ef6476dde38243d08b1246172e313a79",
            "3fceb0006b8949acba9f7250f1e2d8a0",
            "9d28a5057b8344e38968fb7299c18c11",
            "5abec53588764856a68bd2bc05efc54b",
            "388f4fa0264645548574ad8c9decd0a5",
            "4fc0754ad4ba472caa76212323f05fd7",
            "7b8364eb6f1b4949995344a86a8c7ffc",
            "adc8c3b97ab0413580512bbaf45559ba",
            "e0894c49925941a3a4aac5a4a81242b8",
            "580cdf0ae0694aefa1a56fb5212961f9",
            "d218c095c9b14769bbf0e34cabefccef",
            "15a300eace7948ac92202ecccdb39600",
            "eecc3d40fab34cf592e96f0c6ddd8271",
            "0d359c6677584dbeb2e051e4ae67f50e",
            "ef8f5d276db44c828aaf1efa3976faca",
            "662d7d077a874fea869663edea04e2a4",
            "5f7254ef1ac744d1bf98539b8472c689",
            "d4560152be5d4bf69d7f45f2be105c02",
            "7ee95b070d8a45e7a48dfb1c62820b2c",
            "40f807f3ab554843a22688c5d367c909",
            "72a323adab1c44be9930ebaff0655fca",
            "abbbbf502593491b8c13ab9d8277d8dc",
            "2b04ef8eb6454f959a47f721f3a6f24c",
            "9cd6a228031541ecb47ae66d4640ebe1",
            "3e6777263a2c43ee8ec950dc3e5c34a2",
            "9de0c8ba5edd4bc4bc3b30cf8aac5a0b",
            "3af2ab810316402da66989d70859e5cb",
            "e0e4baf9f33240789d7a722a2dfdb176",
            "9d8163452cb44355a0802390df8db892",
            "d3cd6caba49a4513a4cfbc851c924f25",
            "8b99452ed848449290d66e46a5656229",
            "94d7a76fde2d4fbe93f3002cf7ed0f4b",
            "2098ef59ae384f94aacc3e6f2c049bfa",
            "92cb5905cab942cd99086812023ca42d",
            "240b772769f940f49243832c9c99dff6",
            "c7fb6bbcec7e492c96b2da7843e540c3",
            "ff7aa66c78b34f02a0edd30a5ca4fb9c",
            "a0605141a4094a7a821441fed37fa9ea",
            "eeec557178e94cbcac7d83a28eec6835",
            "ebd4ef56feb64f0f9e47c879c251df23",
            "22d6fced766e4f4fb43781c2f242a220",
            "0ddeba1a6376451a86c7d4bbc3b2c947",
            "a8f22ae91e6a447c9e0c2b8c26c8284e",
            "d6d552aa4623438d8d458483f20fe2cf",
            "47837b16cf1441a4b512be26b38c4282",
            "a2a9b7e5a6fa4f3196072fe4e6080f41",
            "614cfa69df29443382fe138f3c448cce",
            "80a78837a9304221a496e92d6440cfa1",
            "95d8aff49add4f85a130efd0a7c7915e",
            "f6048706cde94ad29b94ab8dba939994",
            "d15811e3ae2b47d1b6301b884433c9b5",
            "88955536ce444c52a085c351c712b9be",
            "a1a04ab1ef574edcbebb0e11aa69fc23",
            "6a1516f6ee38479284eb4f6fac49646c",
            "6a86882fa683475ba96f809b14afaee6",
            "906f57399f55423ebb88bd98e7ae4fb0",
            "a8c1546721b7490f9db2d826d0df559d",
            "46a05a452ff5451cb645fece969168d9",
            "05917e7d577743da89f954014fd93a69",
            "a29fcec56cbe464b9f6829796d53a3eb",
            "49d8d6ddf99a4a79811563eb74cb69ef",
            "ae23d707c38f4e4ab7c541781cb940a1",
            "4448994fcd3b4ac9ba13ad4a3d167b07",
            "9570fc01ae3e4e529138a7cea618d09d",
            "f1cd0540f78e4ae1b0e8ca82bcba4446",
            "4b649332b5e441478eaa1fa4c220455b",
            "d50bb791f90f4953a461a47fde08954b",
            "031f8959d57b4506aa79847bf6158fdc",
            "1a7948bb883143e6818c23aee0355589",
            "98a0f873aa5c4b1b87a293b796dec9c8",
            "bbd0a1263c1e49f18ef081e413a45626",
            "3a2170e8e2f140379376cfb62c86f954",
            "a502f5fb647a481896dcaaee4ed00c79",
            "f0c438ace933448782d01ce6640a55d5",
            "54655c5d5e3d4fefa204ba0357b3c914",
            "5973ce96a31240df8379e1c781830b5e",
            "572b8140aed44ed6a0e7d172cbbe5ba2",
            "a81127f6bc374eadb4c59c89299bd337",
            "a7caf2d0e8e94818b49e7a485d7c34ce",
            "b63708bfacb04d5ca65ddf4b7f0d0c72",
            "0e25cfbeb354431691ff49b2906e509c",
            "25d3dad1d4314918a129b05fd5eed947",
            "bce91be010e64789aa5b74767ce2ad47",
            "f5c38112d2f04871b482b4a9264531a7",
            "a8940d86b6c24cc5af3b6c3066251a34",
            "1c9ae7ae2ed34314a15be4fbbd238ee3",
            "f11ce762e8b44e6d8e5f41c768be6f87",
            "3470defc97fd44ecbaf5b6b0598d4096",
            "c7103bf648b24dd7ac922e8c00a11140",
            "8d9ce9b51bad4d9ca9d1819f8681defd",
            "76a73024c3dc4378aea160f7992c7436",
            "d10be523d2bf478fa75bcc0d14f05308",
            "a74af5f99a9b43ea8318d95edaa98c17",
            "0810f69eae9946e88318ac0b87eff84d",
            "3640687a96994e248f87f47b12672319",
            "bd2545c7b7894c0d967ee29a04e0331e",
            "6459fdf05cd146c797394680604d875e",
            "0b27d2bda7b345e3bff2925b6084f15a",
            "7bc4b8f73a314277ae7f746fa906ccd1",
            "91b0f3f8b121432290f698c9f3924e6b",
            "08206054b552405bb7949099ba6e653e",
            "f190c7a1aadd47c4965180f2f7d9faa6",
            "4a98d8b8c4724470bbaf092778b6f7ca",
            "3ee0e2b5bd5949c6a76e5d0748db387c",
            "ec1ad53f657c4e45bcbc706628ce8bf4",
            "793fb984c8d1444d853074f145079198",
            "00b31e0c738a4067965db95d554ea10d",
            "0c9ebd88930b4c5880358ac0e41b8c13",
            "c69abd59883549e6a4a57e3caaba4cb7",
            "4663508cc40a4e718eb596e1067e2c49",
            "d8e906190c1040559b0a8ddd220bcb76",
            "266ec94c3425486a90525cd92044fad9",
            "ca6e2266f6a24fe9ac56f88aaa0aca1f",
            "2636d6b319044891999840970928e320",
            "04b0b322e1a2487a9e1e868cd9117bdc",
            "8d6bce670454454fa1b3fe117e5e463a",
            "c03830b41c5e4389a9ce8894ef03f391",
            "bd62ae761f14485ba67d7087658addf3",
            "312fe3a88a52414e9eff91ec70ecd779",
            "d2a72deb5f37434ba1796c23f4fd5405",
            "2c62038f0c784230944e2553872b9705",
            "0d6766d9a927440c820dc3decfd8cd65",
            "b2c10957fae8476bb058a24f1ab963d6",
            "44d47604914145e2a83b862b96012e0c",
            "3eb2812e41784990826ae7c3fde8fd83",
            "16823c13c7d144cdbb688f7a9e28c6e0",
            "8bda80c334ec40caadbbd94ef9e934cb",
            "3bde2e1bddc24e9789cb7d7bceb4470d",
            "88521058214f4de9837c9c51db1f2026",
            "ea9a3000bc744c56a002035f2e3432db",
            "b18c7dd4016a43c1b1948ee58f03aef6",
            "ea1894e9189340ccbfec972b2d52e26f",
            "ac732c69870d465da7cb04f36814e782",
            "baba3c995694420480c51552f77d02f3",
            "5621782d6e27418cb918988093342203",
            "78b10f96461e4524a87186f057c80b7d",
            "0893fd4f54f24c889dbb4cc1ee568205",
            "6749ff1fbe474352a940b9e1db924036",
            "1cc0fa1de88949d891dd83dbc3a29843",
            "f8d81343b6474ee38141146dae8c9faa",
            "d0f6818b5bfa41999daaf9ceb13b80ec",
            "6ad60285f5f445c195daa4977579b64d",
            "ee87906d83c4448ea906acdec4da3019",
            "5c312273148d40a4946164dbfa7b47a3",
            "a599cce14d7d463d867a4afaf300c633",
            "4102c112378b499a910721349bbb90b3",
            "3ed6c5f26859446caf28c6258cef8859",
            "500d46d3f543458f839202575b015412",
            "cc6fa566a6a44013a98386e1ebfda849",
            "cdd1b73607d946abba57ae9e88720907",
            "3174d46aa2014f12b759a121a9a83813",
            "c1f14c7f255240f48cfffebe800427ae",
            "ba2cf80d9ddd485d9afa21901306b528",
            "d99f8abd18e749b28e592e5e895bc004",
            "fccfe2bab8df4516b3dd223ee3fdde7b",
            "c8c89211a3344ab690bf10da4aa986fc",
            "e5283346b34b4b0da37762f72b216c6c",
            "dd410fab07b64c91a1e44a790ab02d37",
            "b7a6b89e4e9c4bfb9ece962a153518ac",
            "ea6b6708f7d945d4a3b1e0da43b70254",
            "7eb3fb4f3c944d2582d758b12991aa08",
            "78fb974dc20946aeb1f9f79f7bfa867f",
            "0575e239128b43fca7420dc7b868f8a8",
            "84c00c3d1a4743319e127fdcc4976638",
            "033308b7c1e145d18afa66d1d4af18d9",
            "368592a09212434f826943ed74496c3b",
            "fb1d0c9e4c894c4482d00d96238f07aa",
            "e50cba7941244fb58e4405d9c90b7691",
            "172aa337ed6f43c092620115e3f10258",
            "c3479518932a4e49ace632f1f61ad5fe",
            "5a1bdcf17cda488a9c34887e9c84c5af",
            "8fe2a581f6994e8284e157ab8b134ba2",
            "fedee99b0a6c48c4a9be9610ce863046",
            "7b0ce357600145fc9c7cd5fd8e79d021",
            "aa6f6c72bdc44048a7d8e2283cb24098",
            "d8ed0101fc1c4f82b3517c0c70c68f4c",
            "2a8b8d4bd5184f409c2a9cf7ca48da20",
            "59d4cf9ab5a1464c8e46a8da8d27b707",
            "9edad751eb454f2c9d0c19ffb8a33ddd",
            "141623d64369445bbe55fca835bc4017",
            "4736b634a78441f8b29432d9b54030b7",
            "e1df7b68945646ae94d8ec8dd2b01bd2",
            "8b421db3431643908c58f576d20ea46b",
            "2e3e1edb26d34e02813a0ff86208c6a1",
            "6c0fa363325045498597c5c78d1a0bff",
            "f96fd3209c9d4f959d584b6bef4de8a5",
            "0cc7099790b94599a1bdf948804bfa56",
            "09372bcb4140441e91becdacdb1ae243",
            "ed3e41444395431aacf55ffe2a7c59cd",
            "37dfb6e32c8142378692e784475eea58",
            "71bb0871e48043a6a56bc8e45d840650",
            "25e78995304f4cef88c964c7623c723c"
          ]
        },
        "id": "y51xzchXlrtZ",
        "outputId": "b945546f-860d-41c3-ac9d-cddd87efec98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing the Hugging Face model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fee76b55e76e493281fd1ef9e29866ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "872fe11395044138ac44658fc2864fad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b54860eec4843b6997ce7cdd668c253",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9fea5d1879a344a49ffcaa048fa5b6a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3f9182e6e2540e89bf491785d6853c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b29fd846cd5941ebba782caf8afdf477",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d89c9c70a7f4424d94f61e9e3b7d488f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e486c58b91bc407d9ef4c5c82d6bf1bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4fc0754ad4ba472caa76212323f05fd7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f7254ef1ac744d1bf98539b8472c689",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "<ipython-input-6-ca7b435ba96a>:110: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized successfully!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0e4baf9f33240789d7a722a2dfdb176",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eeec557178e94cbcac7d83a28eec6835",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6048706cde94ad29b94ab8dba939994",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49d8d6ddf99a4a79811563eb74cb69ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a2170e8e2f140379376cfb62c86f954",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bce91be010e64789aa5b74767ce2ad47",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0810f69eae9946e88318ac0b87eff84d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec1ad53f657c4e45bcbc706628ce8bf4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d6bce670454454fa1b3fe117e5e463a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8bda80c334ec40caadbbd94ef9e934cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6749ff1fbe474352a940b9e1db924036",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start loading QA from ./data/annotated/QA_pairs_1.csv\n",
            "3938\n",
            "Loaded 100 QAs\n",
            "Start loading texts from ./data/scraped/scraped_all\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Wrapping text in Document objects: 100%|██████████| 172/172 [00:00<00:00, 199287.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "End splitting texts -- Number of splits: 14099\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Embedding texts: 100%|██████████| 14099/14099 [00:00<00:00, 2116820.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated embeddings with dimensionality: 384\n",
            "End embedding texts\n",
            "Start saving embeddings and splits\n",
            "Embeddings saved in ./data/embeddings/embeddings_all-MiniLM-L6-v2_main160_sublink0_recursive_500_200.npy, splits saved in ./data/embeddings/splits_all-MiniLM-L6-v2_main160_sublink0_recursive_500_200.pkl\n",
            "Building the vectorstore  CHROMA ...\n",
            "Retriever built successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 11/100 [00:04<00:24,  3.58it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "100%|██████████| 100/100 [00:24<00:00,  4.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA evaluation completed! Results saved to ./output/llama3_recursive_chunk500_chroma_top3.csv\n",
            "Initializing the Hugging Face model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc6fa566a6a44013a98386e1ebfda849",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized successfully!\n",
            "Start loading QA from ./data/annotated/QA_pairs_1.csv\n",
            "3938\n",
            "Loaded 100 QAs\n",
            "Start loading texts from ./data/scraped/scraped_all\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Wrapping text in Document objects: 100%|██████████| 172/172 [00:00<00:00, 305040.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "End splitting texts -- Number of splits: 8491\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Embedding texts: 100%|██████████| 8491/8491 [00:00<00:00, 2237331.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated embeddings with dimensionality: 384\n",
            "End embedding texts\n",
            "Start saving embeddings and splits\n",
            "Embeddings saved in ./data/embeddings/embeddings_all-MiniLM-L6-v2_main160_sublink0_recursive_700_200.npy, splits saved in ./data/embeddings/splits_all-MiniLM-L6-v2_main160_sublink0_recursive_700_200.pkl\n",
            "Building the vectorstore  CHROMA ...\n",
            "Retriever built successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:23<00:00,  4.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA evaluation completed! Results saved to ./output/llama3_recursive_chunk700_chroma_top3.csv\n",
            "Initializing the Hugging Face model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea6b6708f7d945d4a3b1e0da43b70254",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized successfully!\n",
            "Start loading QA from ./data/annotated/QA_pairs_1.csv\n",
            "3938\n",
            "Loaded 100 QAs\n",
            "Embeddings already exist! Loading embeddings with dimensionality: 384\n",
            "End loading\n",
            "Building the vectorstore  CHROMA ...\n",
            "Retriever built successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA evaluation completed! Results saved to ./output/llama3_recursive_chunk1000_chroma_top3.csv\n",
            "Initializing the Hugging Face model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a1bdcf17cda488a9c34887e9c84c5af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized successfully!\n",
            "Start loading QA from ./data/annotated/QA_pairs_1.csv\n",
            "3938\n",
            "Loaded 100 QAs\n",
            "Start loading texts from ./data/scraped/scraped_all\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Wrapping text in Document objects: 100%|██████████| 172/172 [00:00<00:00, 197163.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "End splitting texts -- Number of splits: 3306\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Embedding texts: 100%|██████████| 3306/3306 [00:00<00:00, 2251399.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated embeddings with dimensionality: 384\n",
            "End embedding texts\n",
            "Start saving embeddings and splits\n",
            "Embeddings saved in ./data/embeddings/embeddings_all-MiniLM-L6-v2_main160_sublink0_recursive_1500_200.npy, splits saved in ./data/embeddings/splits_all-MiniLM-L6-v2_main160_sublink0_recursive_1500_200.pkl\n",
            "Building the vectorstore  CHROMA ...\n",
            "Retriever built successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA evaluation completed! Results saved to ./output/llama3_recursive_chunk1500_chroma_top3.csv\n",
            "Initializing the Hugging Face model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1df7b68945646ae94d8ec8dd2b01bd2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized successfully!\n",
            "Start loading QA from ./data/annotated/QA_pairs_1.csv\n",
            "3938\n",
            "Loaded 100 QAs\n",
            "Start loading texts from ./data/scraped/scraped_all\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Wrapping text in Document objects: 100%|██████████| 172/172 [00:00<00:00, 295421.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "End splitting texts -- Number of splits: 2413\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Embedding texts: 100%|██████████| 2413/2413 [00:00<00:00, 1588083.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated embeddings with dimensionality: 384\n",
            "End embedding texts\n",
            "Start saving embeddings and splits\n",
            "Embeddings saved in ./data/embeddings/embeddings_all-MiniLM-L6-v2_main160_sublink0_recursive_2000_200.npy, splits saved in ./data/embeddings/splits_all-MiniLM-L6-v2_main160_sublink0_recursive_2000_200.pkl\n",
            "Building the vectorstore  CHROMA ...\n",
            "Retriever built successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA evaluation completed! Results saved to ./output/llama3_recursive_chunk2000_chroma_top3.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "strategy_number = 1  # Change this to 0, 1, 2, 3, 4, 5, or 6 to test other strategies\n",
        "run_rag_strategy(strategy_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "12fe91fd4927413c9e039d752f39ba25",
            "42e8d33947f04b10833bbedd8a46ad5e",
            "bb3a6b9d01fc47da83047c36c3bb3cb1",
            "63f2d00d4a754035931ec4b2f3d407eb",
            "f1e2b880dc89441689f013ac9cc69da3",
            "b7f9223942d14fdf879079332f223c1a",
            "450cf70ec2cb4692877e1f0516c0477c",
            "d5e3daa0103346fe8d582155c0c5bd9d",
            "db1c64b2db0846e0a5dcfa2865d874b0",
            "1131e492cce447a19e478b1b188884df",
            "0fcf736525f24218aa7593c5f0fa17dc",
            "cd9a2ada862d421da2d95cb403b7e999",
            "a5ec4ed0763342e4aed7bc2a2dea31f4",
            "994d25243a4549f18922e7508d7cefc3",
            "6418bb4521b64bcda00da7706e40eadc",
            "9049e171034a427f8f62463c1652ebbd",
            "5d467966268e4fff85a523228707ad4c",
            "25a6e4856e7543c5a203d1f172fa7825",
            "5f34b8e5b7424ecda964359e7824d45c",
            "ba9ebdd6b5b34dccb5573cd15c8d4175",
            "220db97557b44e5f8ec65b40564c11b4",
            "3ab05189baa04c059acc968d17b30c70",
            "7d2cb5ab732f484ea26ec6086db0a376",
            "00a99ccd19e3474a97895e315b4b4297",
            "b5812a5657374f9089a563c766a236dd",
            "1f0055a00295410986eb481f60ec750b",
            "5d23d7b16d7b4087bd56071f679a6b18",
            "3d38bad7cdf74cda82d6578dfcc121ea",
            "819d6295e28a4b4bb18c07c6ba4ffc17",
            "719d81ac53c0420b9ccb929441dc83b8",
            "9c077267edba4e37abdeffddea832c1f",
            "1e5c8c0cd0cd438b9543ebdbddf2178a",
            "a5e8e505589f4aa581d02541aaaa9939",
            "6c3aea25a19b46ffac9742b4901c5c84",
            "6aa2c53a440b48d8b610b3ad02ca31d4",
            "24992c327f774333ac2f901e2b66ecd4",
            "d7474fb0a61947ed9de517f454bbcb77",
            "3908205502e546cda1853ed910b195c2",
            "1aef24e0b8c5417da53e90a1035708b7",
            "58eae76dafc54b1f8e1426e93e845c0f",
            "dc280dd9f9d2498aa5383431f9d21495",
            "99563b1d16e844249da2ec0059a9da74",
            "3e0096c3c02b489a9d3f9e111c721c42",
            "0d2682015aac41ff835113af59706d37"
          ]
        },
        "id": "HwEEo3T-lv4E",
        "outputId": "38a31299-052f-4597-b813-83b80ecd4943"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing the Hugging Face model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12fe91fd4927413c9e039d752f39ba25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized successfully!\n",
            "Start loading QA from ./data/annotated/QA_pairs_1.csv\n",
            "3938\n",
            "Loaded 100 QAs\n",
            "Embeddings already exist! Loading embeddings with dimensionality: 384\n",
            "End loading\n",
            "Building the vectorstore  CHROMA ...\n",
            "Retriever built successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:23<00:00,  4.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA evaluation completed! Results saved to ./output/llama3_recursive_chroma_top3.csv\n",
            "Initializing the Hugging Face model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd9a2ada862d421da2d95cb403b7e999",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized successfully!\n",
            "Start loading QA from ./data/annotated/QA_pairs_1.csv\n",
            "3938\n",
            "Loaded 100 QAs\n",
            "Start loading texts from ./data/scraped/scraped_all\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Wrapping text in Document objects: 100%|██████████| 172/172 [00:00<00:00, 283131.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "End splitting texts -- Number of splits: 4423\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Embedding texts: 100%|██████████| 4423/4423 [00:00<00:00, 1585590.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated embeddings with dimensionality: 384\n",
            "End embedding texts\n",
            "Start saving embeddings and splits\n",
            "Embeddings saved in ./data/embeddings/embeddings_all-MiniLM-L6-v2_main160_sublink0_semantic_1000_200.npy, splits saved in ./data/embeddings/splits_all-MiniLM-L6-v2_main160_sublink0_semantic_1000_200.pkl\n",
            "Building the vectorstore  CHROMA ...\n",
            "Retriever built successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA evaluation completed! Results saved to ./output/llama3_semantic_chroma_top3.csv\n",
            "Initializing the Hugging Face model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d2cb5ab732f484ea26ec6086db0a376",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized successfully!\n",
            "Start loading QA from ./data/annotated/QA_pairs_1.csv\n",
            "3938\n",
            "Loaded 100 QAs\n",
            "Start loading texts from ./data/scraped/scraped_all\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Wrapping text in Document objects: 100%|██████████| 172/172 [00:00<00:00, 291600.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "End splitting texts -- Number of splits: 6470\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Embedding texts: 100%|██████████| 6470/6470 [00:00<00:00, 1946152.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated embeddings with dimensionality: 384\n",
            "End embedding texts\n",
            "Start saving embeddings and splits\n",
            "Embeddings saved in ./data/embeddings/embeddings_all-MiniLM-L6-v2_main160_sublink0_token_1000_200.npy, splits saved in ./data/embeddings/splits_all-MiniLM-L6-v2_main160_sublink0_token_1000_200.pkl\n",
            "Building the vectorstore  CHROMA ...\n",
            "Retriever built successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:23<00:00,  4.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA evaluation completed! Results saved to ./output/llama3_token_chroma_top3.csv\n",
            "Initializing the Hugging Face model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c3aea25a19b46ffac9742b4901c5c84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized successfully!\n",
            "Start loading QA from ./data/annotated/QA_pairs_1.csv\n",
            "3938\n",
            "Loaded 100 QAs\n",
            "Start loading texts from ./data/scraped/scraped_all\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Wrapping text in Document objects: 100%|██████████| 172/172 [00:00<00:00, 297247.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "End splitting texts -- Number of splits: 4941\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Embedding texts: 100%|██████████| 4941/4941 [00:00<00:00, 1984872.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated embeddings with dimensionality: 384\n",
            "End embedding texts\n",
            "Start saving embeddings and splits\n",
            "Embeddings saved in ./data/embeddings/embeddings_all-MiniLM-L6-v2_main160_sublink0_character_1000_200.npy, splits saved in ./data/embeddings/splits_all-MiniLM-L6-v2_main160_sublink0_character_1000_200.pkl\n",
            "Building the vectorstore  CHROMA ...\n",
            "Retriever built successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA evaluation completed! Results saved to ./output/llama3_character_chroma_top3.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "strategy_number = 2  # Change this to 0, 1, 2, 3, 4, 5, or 6 to test other strategies\n",
        "run_rag_strategy(strategy_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587,
          "referenced_widgets": [
            "0362d19b571243148c063581318ae03b",
            "f4de59043b6f440882cfef078196fa2f",
            "a856f749d2f6485ab12ce10d0f3862f0",
            "95ff3659aa954776b4afd4b7be388448",
            "1d82842b85b74bf499be092a9ad94d84",
            "5e1570d0280b4de08fe10b366872fda6",
            "fde7a2bf5dbd4443820b373fd7def4c4",
            "7ac4ecc05f8f4c8297fd827bc0128d50",
            "70288c2b4a8b43d4b225522c0cc3b2ce",
            "bcb403a696994b1999e121f0ae85474c",
            "6b8d478c4ddd4af5b7d26a9ece8d28dc",
            "2a561a8f3e98496b90f52a0de89bafe7",
            "486ee69678b2454ba291bb300f38639b",
            "4d82f8e8205743dbbaab7dd4ac99cb5c",
            "becb6e2d323d4833be64f2449203b64a",
            "9e3f9d02cd2e460ebc7e41dc02406616",
            "17b4ee5db25f483d80eb986f6434393b",
            "61e4bd5b09a34739a119551840c0b985",
            "14913a595abc4453b05e22357955f8cd",
            "724242dda70e49b38a32ea72ac10460d",
            "416ff9e56c4c4aefa14edc14d141068a",
            "7d574e943825446f93c7241e45d48700"
          ]
        },
        "id": "6aphceKNlx0e",
        "outputId": "65f7f854-137c-4dbe-f577-f01d2e83c5b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Strategy 3 A: Compare retriever types and embedding models.\n",
            "Running with retriever_type=FAISS, embedding_model=all-MiniLM-L6-v2, saving to ./output/llama3_FAISS_all-MiniLM-L6-v2_top3.csv\n",
            "Initializing the Hugging Face model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0362d19b571243148c063581318ae03b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized successfully!\n",
            "Start loading QA from ./data/annotated/QA_pairs_1.csv\n",
            "3938\n",
            "Loaded 100 QAs\n",
            "Embeddings already exist! Loading embeddings with dimensionality: 384\n",
            "End loading\n",
            "Building the vectorstore  FAISS ...\n",
            "Retriever built successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 11/100 [00:03<00:22,  3.97it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "100%|██████████| 100/100 [00:23<00:00,  4.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA evaluation completed! Results saved to ./output/llama3_FAISS_all-MiniLM-L6-v2_top3.csv\n",
            "Running with retriever_type=CHROMA, embedding_model=all-MiniLM-L6-v2, saving to ./output/llama3_CHROMA_all-MiniLM-L6-v2_top3.csv\n",
            "Initializing the Hugging Face model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a561a8f3e98496b90f52a0de89bafe7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized successfully!\n",
            "Start loading QA from ./data/annotated/QA_pairs_1.csv\n",
            "3938\n",
            "Loaded 100 QAs\n",
            "Embeddings already exist! Loading embeddings with dimensionality: 384\n",
            "End loading\n",
            "Building the vectorstore  CHROMA ...\n",
            "Retriever built successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:23<00:00,  4.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA evaluation completed! Results saved to ./output/llama3_CHROMA_all-MiniLM-L6-v2_top3.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "strategy_number = 3  # Change this to 0, 1, 2, 3, 4, 5, or 6 to test other strategies\n",
        "run_rag_strategy(strategy_number)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK-mC7sBZwFf"
      },
      "source": [
        "To resolve the dimension mismatch error and use multiple embedding models with a single ChromaDB collection, you need to ensure that the embeddings from different models have the same dimensionality.\n",
        "\n",
        "* all-MiniLM-L6-v2  dim: 384\n",
        "* all-mpnet-base-v2 dim: 768\n",
        "\n",
        "If using Jupyter Notebook, simply restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "34a7af6a13834248bda6b303372cc2d9",
            "dab441ce31d44551a9b63ff1173b99e6",
            "8b4fa7ffc6224b9e8a8d8cacb6bef528",
            "38ccf182e0034f96a129a261067f6ab7",
            "ea68ab2347264bb58b1ec91abc23d21c",
            "4bbdeb5694884c6b9836e68fd0611415",
            "8828c2a1e3a24f4fb5a0387920ca8ade",
            "05d14a9216374ae1b9c4144c44b006d9",
            "7eebab36f5ad4acf9138aa00840fbc08",
            "e23033fbf0c345e69c10211bb3cd6df1",
            "d6d8c6fbd3574a29960ebbf4fb301ff7",
            "6baa18c1a95849afa0260a839f7bcbac",
            "613555ee5e664474988d5b3040316f50",
            "018bc37e02924ae188faf0945d02931f",
            "987e587118544dbd8014b245ced19539",
            "2aff752fbd6d4e3c8fea58e1ff6d7435",
            "6df29524705346bba7d56640c952d913",
            "053bbd4d392046f3aab63fd616eeec6c",
            "56c15560783e4a199e1de26fbd258d6d",
            "05b9146027fe409b8923ca8347e28731",
            "f7e82a11b1c04b21b6c1f60bc725fd15",
            "bf06ce3806b740fe92bab616d44f0079",
            "41aec3df741a4b4f8f288371f58eddd3",
            "74c1647faaed40ab9c52e90f2ff11153",
            "528a1ba8e516469489239b97aa430447",
            "f85bcbfbaba243848fe51879aa169889",
            "e9ae7a4a6e224f7c9679e3e65e1c595d",
            "76f510cf3aeb4bcca33f664ea35dbaad",
            "480fe252827a4c13b87b1357ff19b764",
            "a5b13adadd26469ea41c920982e2f1ca",
            "e130b9329b4d4a92b5be09b8f264dce7",
            "8e0ce663dc7e42039ecb5e0f717f7249",
            "6676c44020664bd5a11090c4a11c4bac",
            "2d5eb53343bd4f3b9b59d213489acf53",
            "b9be02f2b52e4c07a27f4eac2ad1f44e",
            "1cc1926691f34cfe89d4edeb5d07dbcf",
            "6be0d54ae2bf4902baaf21fbab48b234",
            "a1fa8096d7774dcbbe7c941197add2fd",
            "e11e54415b214197853625ecc4dea3a5",
            "7a33cfcc19e746bfba23f5127d916872",
            "2caca21429c14126a4199c6726cc385a",
            "4722c5218b984f3eabfe31f243cc472e",
            "1cae5a751b83499597f95cf0ec2c06b3",
            "b9b8dac1609248b3bd94061a045ef0b6",
            "ffdb8d47eb7d411e94f495dd389e2420",
            "7139234fb3e94e80b5ba915b0df83cff",
            "17f633e8cc1b48e8ad0835f772fdc0a5",
            "0bf07f55f7c740acb9b2b1f10db7bb9d",
            "0a75f8d8ce3e4eb1b8b444de662a3f5a",
            "65e22863abb841a68b6a890aebb3a063",
            "e439c040a29246ad8f3dd4d8cb281e86",
            "a8fe8f74592047249d150255e30f4278",
            "b54f245405714d5d9845a4a4be617cbc",
            "8d4e4a9e6c144bc4821eb4878d38f2f3",
            "594c7554a0fc4dbe9063ad71205b571d",
            "086ac805b6d843e2a6d236c11150408b",
            "12b9076b64bc4166b771cc650efb67fb",
            "839e445ab94f4be8a30e549c5f775add",
            "1216680685e340b5a95f2ad17bea18e8",
            "4f3835de8fea4b5e9c7ebb8233fd1949",
            "a9f839e6853941d3b92132a3312ea2e1",
            "d700c749324e4adaa7d29e4cb579c593",
            "c078eec3fcdc4028ae15810e8d54889c",
            "ebccc7567403449a9285da96fb7b5b3c",
            "3ded117ef14c4684adf81427c03ef383",
            "19dca3d6762a4af0add2b366c84d8460",
            "0caeb72bf4454234a5a281d97137cd62",
            "be2a137eff1c43daad8bb704de2ff1be",
            "48cb537d969c451b8796b5e1e3895746",
            "26e642a507d945b989c924c960038590",
            "eca73f5a992e4072b9ce0f89a87f5577",
            "ba54f70a87d34aecbfa32a470726602e",
            "47e614b418624add8e188d3fab166285",
            "77a6cb1388c644658af7ffffb5512bd8",
            "48f9bc4835c44763b0920bf179c6a2c3",
            "92be5b978b0e491ba968bf0725bbab99",
            "34c179fdd7394e998c0e0bc0fce0d2fe",
            "0c3748bf4c244b33a9662b547ab26915",
            "2af679ef441941dc92ab53c4d6d36453",
            "0236d626479048c49542b0692a60bfd7",
            "cf56116a643d470bb7cfca4a6a20cf1c",
            "b76ff423e76e4a6dbc4a8b50a02eff98",
            "1d1e8601dc58476d842af8ca34a6da9b",
            "78064f6150354e918c96dbdd5b8abc07",
            "0dcc165cb28646318df136a6fcaf1707",
            "2791b6aab2144b609be17a02fe2bd037",
            "d4777d5dc0c244518879ee3131b56071",
            "e8f5992f6d9d40e5b42883e33ae33d66",
            "abde099f198b45aaa8246de7402e7886",
            "e8b3808b050748dd9b3877e2409dd34e",
            "8ef36fc602ef46d7ad51416a521d7512",
            "421d7229c0eb4980bbd442cbec1df3af",
            "57d68f37464b4611a1a3078ac3ca33f6",
            "3c908b3e471144c2ab2e226c9bc7fb18",
            "9f953524f9a942e697541db21db5c1b4",
            "a3ddafffb675461eadcc3976167478ce",
            "8823a8be8ef84e00894e17f90aa80f00",
            "5c77eb2559904689bc56a7d61d55c56c",
            "7c736eb8bbf24c15b05dd5cfa198b727",
            "f7ec9d7c463c4c62a42f9386b525e9b1",
            "b8a646dca1f1410badb48d73c7d949a8",
            "688684c0ec0346fb996942dacc7fdaf9",
            "75ca36029d2345bcae391554194b6de2",
            "526ee4497759482f88ccbe07bf2ef97d",
            "dc3473950dbf44668d8ab9dc4742b1f3",
            "3617b426d00441719365bdad6a5a8ad0",
            "a148e2affbf34ca08c6f1549f274405d",
            "7ccc01c795744af08e1cbb32499d9c54",
            "78d816b6eec64486aac2854b30dc1c02",
            "1d6357dfc4f54db09f25eb570b2e42f5",
            "943c4a5f27f946f6943802171d38174c",
            "222a670e3b7a482f9e3fdde204966d3d",
            "3370fa31d88e49de9d59104241fe4318",
            "90388174baef4996a0298649b2786809",
            "a68fd68b40a949fcb6d0d447143b64f2",
            "d65450b9eba34977bbc210dd19876904",
            "2fb790f3d8ab498f916794cc04eb5bab",
            "4510f8933e8b4f9f9a2a412f66f3a3d2",
            "09bfc6d285554bb8be0c6c2655a57508",
            "741fde6a7da24eab85ae77c8aa53f4b3",
            "b2ae9cef7728420a874bc57bb69ffc60",
            "63a0092b3a8b40d09f3fd640d44c50f7",
            "00a971924359465a8288981ca2aa2907",
            "72da49cccea64b948e5d69cf176fcefb",
            "938798b609ac4800a4c2bfa3b01008f9",
            "f25aee09f27c49b4a2c6841ec1321270",
            "32064726c3284187a7a815825a70d38a",
            "841af1bfb2384941924fb8fe643e5c48",
            "87a8f12577994e118b8e751bc67fa7e1",
            "ccf2b8c650bf473c8426984e87d5aa94",
            "fdbf9b1340c3446ebedf337185864e9c",
            "e7dd769d84834de2b953940a1e99084c",
            "e2541a6fdab34739a714b5d080e6714f",
            "fa32a62ad8d7449da10bf0b4b4fc4c5f",
            "cf8cf1f549db4b1aaa8fa781dcd05055",
            "2fc585a2d8c948718add8de750e0eedb",
            "a57124cd166a452baf9ee790e991e674",
            "f26eae644c4c43df9ce7cbe087a495f0",
            "ed3c45a88e264e64a9d1939e29890046",
            "103fadb63b1b4c818f42881566bb05c0",
            "9ca4b42cbd274371ba74c82ae6c35ac6",
            "f9e5add6ef634de5b114388532d65164",
            "3ba877483b374230bf74849994c80475"
          ]
        },
        "id": "10QI4-dYXSFh",
        "outputId": "8f2fe279-d5a9-4fe5-eea9-fe1f49e654af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Strategy 3 B: Compare retriever types and embedding models.\n",
            "Running with retriever_type=FAISS, embedding_model=all-mpnet-base-v2, saving to ./output/llama3_FAISS_all-mpnet-base-v2_top3.csv\n",
            "Initializing the Hugging Face model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34a7af6a13834248bda6b303372cc2d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "<ipython-input-5-22b48f56838d>:110: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized successfully!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6baa18c1a95849afa0260a839f7bcbac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41aec3df741a4b4f8f288371f58eddd3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d5eb53343bd4f3b9b59d213489acf53",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ffdb8d47eb7d411e94f495dd389e2420",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "086ac805b6d843e2a6d236c11150408b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0caeb72bf4454234a5a281d97137cd62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c3748bf4c244b33a9662b547ab26915",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "abde099f198b45aaa8246de7402e7886",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7ec9d7c463c4c62a42f9386b525e9b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "943c4a5f27f946f6943802171d38174c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63a0092b3a8b40d09f3fd640d44c50f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start loading QA from ./data/annotated/QA_pairs_1.csv\n",
            "3938\n",
            "Loaded 100 QAs\n",
            "Embeddings already exist! Loading embeddings with dimensionality: 768\n",
            "End loading\n",
            "Building the vectorstore  FAISS ...\n",
            "Retriever built successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 11/100 [00:02<00:18,  4.79it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "100%|██████████| 100/100 [00:22<00:00,  4.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA evaluation completed! Results saved to ./output/llama3_FAISS_all-mpnet-base-v2_top3.csv\n",
            "Running with retriever_type=CHROMA, embedding_model=all-mpnet-base-v2, saving to ./output/llama3_CHROMA_all-mpnet-base-v2_top3.csv\n",
            "Initializing the Hugging Face model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2541a6fdab34739a714b5d080e6714f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized successfully!\n",
            "Start loading QA from ./data/annotated/QA_pairs_1.csv\n",
            "3938\n",
            "Loaded 100 QAs\n",
            "Embeddings already exist! Loading embeddings with dimensionality: 768\n",
            "End loading\n",
            "Building the vectorstore  CHROMA ...\n",
            "Retriever built successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA evaluation completed! Results saved to ./output/llama3_CHROMA_all-mpnet-base-v2_top3.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "strategy_number = 4  # Change this to 0, 1, 2, 3, 4, 5, or 6 to test other strategies\n",
        "run_rag_strategy(strategy_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "af2493af17844bb98c354df6ee5cf029",
            "e75f499b59124de98f9e7bd2bbfcc1a5",
            "8cd26dbbf99b42ffadf0095adcec68d5",
            "3aadbc06e8114c9296aed4649bdc14e0",
            "391b828ea4be4597947930a7283f6c53",
            "24fe59c096e74ea58a61daaedfa9e814",
            "d870fd4e6b8249d4a1c931153a43285b",
            "0964cd2bca3c4746aa25d66e78ec1dc3",
            "4da53885c0c245d8a840f4eb701fa65c",
            "7b546e4e00474103aa60a805c54b8ee6",
            "f44500413d9e49caa95cd41663596290",
            "9a4d6a32581c43e5b75fcaa8225e692a",
            "3abcb537c5e942bf90e6088a395be479",
            "c1fe79812de44f45a0c4461f509bd55c",
            "21a3ba07868247f8b92fa8ad465fb997",
            "240b87f7e6234776bbece9f9d059f4bb",
            "469817c4d4e34849a19bb64e26349c92",
            "da851b1edd8f49ec893506da089a7012",
            "e86e4486db81437eab385578ec96aa65",
            "910d2b92554248c098bcabb915e038a4",
            "8ad0e82394064031a513bbc91323c78e",
            "d47ce1df7a254cd5acf6865f2ec0d136",
            "ba04e092eda64bcd8d2bc615a620dc2e",
            "14b2fd8fafdc47388dd319b40048dd46",
            "d7e4c1a8bc354761a4227da9f6e750c0",
            "7ee37af7bebb4e91bba908a8ed8b3455",
            "f4bd2926ba624993adf08c1b4a969c6e",
            "d4e9611286e540e58a45c783a90a8766",
            "f09254a8fa8a42aa95e3563d24efeff5",
            "d631f72c326f45abae64f400b8572adb",
            "2c27bba93ab741449059d998f11e45ea",
            "7d5bcd310fc04c3baa2cd3b4fb631585",
            "a8f46cf08a184f70aa838a2a38cc6cb7",
            "59542e630e4145d29d9f20b3f0fa6283",
            "0f63d02167e04d34a0ad4833763c92b7",
            "31d2e4bd58824a2aa98963022ca6416b",
            "c4766342d56f4280a8093d146f3e41ec",
            "a056711ea3ec46318eaf9136aadfe5fd",
            "8232f60a318e4e8ca29cd01d3535a6bd",
            "fd2a5cb8f3344e119fb41b5303c4d699",
            "887dee50b09746a2950b4561c527353c",
            "bdc03ffb2f4646aa8c63c86f27e6ccb7",
            "fc95d139e4834721a9e983a10bcd3593",
            "4e60158cceb84630af49909f6339f07d",
            "754df70986ef401285dbbce5c0dc7809",
            "e103ce8e11c84f7db29f23a0c01a746d",
            "c946896a44054c1cab456a9a0d917f0c",
            "963b27fbdb9a4db8b08663bd22641b03",
            "0bdf88e180b14cc1a43891d321757a8a",
            "5c323f91739349eabdaa90e3dd9a03b6",
            "a7a460ac7ddc4f4d9ec2c99573cb9c9d",
            "a725ac1aab2a4c1190bf5f12bdcbe17c",
            "efff384ac03c41349201aef4b46e2788",
            "627a13d180a847b9b3e8e1f2e8a9a782",
            "c0c927581fc94afeb6d7be9ebc0221df",
            "818df5cedb5449a886cfafbe83cd3584",
            "17d3a53412bc485e94ace09582ab7efb",
            "5c62384f8a404761963f31875fc59fa2",
            "32989b6e54e042d8bd33f1844c93d3c8",
            "fa3c8ac7ecd24d6ea023f05384c30bef",
            "5c484cbc447f4473a5dc586867388fdc",
            "8e88f112d28a4413ade47c1ebd1c73a7",
            "6c583ea8f7d741c894bcb6b85df283b7",
            "e1a4203bb80340c08791832f2ed80f3c",
            "702ae23825164d66941d1a3f78acfcf4",
            "e0d1a153296d42b5aad6a621233b14c2",
            "c0d506f15f254152bc9a526246b10fc3",
            "c505c567e1834de9ac9ed7f07e96d6d5",
            "32e5cfe31f9444f8a662a9a156a2da70",
            "3c5249dd26cb4dbeaf1c78ed3ca5382d",
            "129d86c40f89423bade1217b39876dc5",
            "9498970d13a348c1baa682b7804e9b98",
            "00ba163a99174f439ca3bb255e05b0ce",
            "e01586a21a5f4b32a211f2642181c8c6",
            "e011a2cc2fb240e1b5729fd858f9de31",
            "21b408aeea6a43e7a2958e966552cc63",
            "2202faca4eb0424991a852e4dfffc2a2",
            "929622420a3b468e82411d9b37ca47f1",
            "229d3e7cb3f04c2eaa9ec323a73e9b90",
            "c695e20f35d24b0d9f69ed3410c29d17",
            "f2ef33bb35834f60a6b9cc04c264f035",
            "c9391ad3fa6948f4815b663306ccb34f",
            "1aa3c73f038b425fa7021f18539692d9",
            "1f2d4ac9e98743f1997162f612e3e652",
            "559e27d7f5254eda859f56261a81af3f",
            "f3df7cf225704dffb5753b9339e2c619",
            "cfbe08e02bc940cc8cfc9455e2f17e95",
            "fdfc2c5fe3444d7db1d807776fe8cf41",
            "ceae01abc0c84b07b4619804ffc06e5e",
            "b738fb533da34923888a9029e33bbb4c",
            "7b3e15fc1f4c40e9abfe2aba452cf9cc",
            "0926782b5d7844378c4196770f2573f4",
            "346993db8ebd4a96b999492039ecb4fe",
            "85bde02a80154020a74652d57d6765d0",
            "52502901a43f481a967d639e1863136e",
            "dc39a663704f4881ae99df9288a32c7c",
            "ae2fe483b02b4ff3916b52734e200891",
            "b5efdc0c771d44c18d2eae73164df813",
            "a5225f26938f498eb2da8de2556b3708",
            "f7d6f4a727d84b158a8fe6d8a50b0b79",
            "be4db3002a484136b7d53d5372f272e9",
            "9c66270ebb284ca3b683cfaee343f102",
            "95a6bbd9d6bf494897ba9c64459ea3c7",
            "fe619a7326c34e6ab046ef612c430b4b",
            "fd0743c7f2d64e1cac43d14963b880cc",
            "05f4b6ca231e46c9a2a0451ecf854198",
            "91fa29af135945ecaa6f44d8b284070d",
            "56e40f7a5a3b4f9fa75c378d7d65969a",
            "3ca959c60ba94b39bf4fa2965f47744e",
            "219d357182eb464fbe957a8a55ccef36",
            "b265f73a65924168a4523ac4ab55f582",
            "be6fd049bdb34fd8908b4ec69866539e",
            "8b11c7be79974b03ac7b73cc02b762ab",
            "b46738074c5d4ca8ab2b09dc9ea990c0",
            "2d417539676849ea8ebf3a41b54731d8",
            "24ae6a8e44e944c69d677aaa46775c3e",
            "5b9971304a6f4bc9bb6659ecc700d94c",
            "a33ea7194d794177a918110e6fb63ae8",
            "b6b014144d1e4616a34a452c5effa64c",
            "0f5d88071f834c6e9dbed0c80c0ce087",
            "2f440e1b99e14674ae930b36db4c8385",
            "d4326b52930642aeb97b99685f540e7e",
            "e7c75e6ec74b43f19556ab3dfa23e502",
            "546b9acaa582400a9da2cc143f178fd3",
            "6c89c4c4b10b46adacd1c0476ef28ce0",
            "4483bf94e0b648ca8ca7e31cb00219c6",
            "08a778f79efc44febc731471ca4b661a",
            "dfee85bb20134d13b5e628bb0f86ba3b",
            "0cf42c4f5b7a4559b9623285c732df7e",
            "9d481c452003433ba7f5cd10c559a417",
            "66823f15ae864fe883dca958b3e51f21",
            "2c39a097954b4f03b3652e8cb3e0dce4",
            "b808f24e403146de857a07ad7cc4fe70",
            "7af5cfd940894341a99e2882c22b036e",
            "e33c38151260446ea36c9466fbcbf393",
            "ba34f6cfbf7a44cb8e3a57bbc8ccc71f",
            "5468daa93e5f4c25a97eee94b22974db",
            "66b1c522069e434abb2a49a0f4e3797c",
            "c30c60fcd2084447b516a21340e250ca",
            "5599febadee849908e0c98a152ba9a0a",
            "88367a59e1fd451fa7c8b97428b2c81f",
            "a85241ccb20344efad6efbabf4c641b1",
            "d8b3b11f7e2943739b417a3eef9b91a5",
            "c75271feb8504ba0bce5acc099ff0edb",
            "ec5615924c73417187f6f8805cb0e73e",
            "49c7fe66e4b34bb99992e8e230619811",
            "d228c38f1cfe46e48a578fc562adddac",
            "eee039522c8549bd8d69cee0f5b6d978",
            "fa37d88938384f628c63dcd1e9031ce7",
            "e5f825f7208641f1b84b82b56edd148a",
            "a03b9a13d3174e599edf363ee4c2865d",
            "412991e5ea214711b0c891c81ddc0952",
            "a6408100246842ebb8e675f226bd249d",
            "c3d37b1b32b248b3b943b50f6bd13ca1",
            "d16eb67ec7474dae9c5a189a6175ae10",
            "af02995bc42c4af39def482ae8d75724",
            "bd539ce262b747e3aa56bbc6e4256e8f",
            "878273c4c8c3485c8c61c5b06ab3336f",
            "ef3e0c7121c34be4bc91290d90190446",
            "41d8a5ec78bc436f873b517441c65e29",
            "7a1886b3cd664dd98ffda501f206a204",
            "3c06a864a272456a99e52484b5ad2856",
            "7355419000f4445db4681013a53a1f87",
            "45660c6f45c34dc88b095cf515a4367a",
            "a79108fc56634aa08cccf41fe5056106",
            "f0aaae1452824bda916c64f4689ad48d",
            "8b0d273e18c24429bd8c8bbb97512a3e",
            "3e32244a5ab3433a9534cd85c6adee9a",
            "450d229df009413db91770eee8779abc",
            "482fa60b55474e57856685a5af0c20b6",
            "2a43f7871001445996e363a7e15ccba3",
            "3748b52479fd42ed8899e4ca06491c1b",
            "8c160b1d78294d0da2a9722b69485c96",
            "b9b8f60bb48046dc9aca019dfaa51859",
            "b5ee6bf26b414f819454552eaf8ffa89",
            "d3d9788fd8674244937565eb7ea33b47",
            "eebb28021d77452cbd95e8188321b9d9",
            "50b43bf68ac14e199246a2d60572355c",
            "aa93ddf12c7f4bec814a084c37505db0",
            "0e922084b4b14f109041a152faa5f718",
            "5c2f29157ab74694b197c541f7709497",
            "04a8d312e86c4890b88aec6237af0a1b",
            "e241e2d91843478ab5c1446767d0e7ca",
            "d639fdb1d8fc43c8927b84b0b7b885fa",
            "3aba0d18283048ac8fc53eb1def5feba",
            "0eead141c7704caca70a390797d5a311",
            "a2de32f458ab4a89940ac46218976182",
            "554543bdda454b8dab1e9a28c20b29c7",
            "d0d49d4a8c3e416ea007f0a33a80e036",
            "e3130f1c5073415a8ef9ea4020956bc9",
            "6a08d78914834f0db27e20ab2fb187b9",
            "c51d0ab01e074b70a903c5f3ca0dd94e",
            "4c11101e773f43699fcd76e9df680d0d",
            "0cc3d2c393404bd6905eef985391565f",
            "a2a7e92f56ed48d79a671741533de670",
            "b2d0229cbf844c84bd9447180a3c1a7a",
            "5b920de4ec8141c89c864cdeec3ca75f",
            "e1b982ae5c4945f188c7f218ab7a6efe",
            "0776c5a89acf472ea4a094214bbb7129",
            "eae5d57755db45cf9a6fe82bbbbb0bb5",
            "fc52228665ab41b1b41459b93aa31787",
            "561958bba6614dbbbef5533e03c22180",
            "6d78cedfb0854770b9bc784274574c85",
            "2762ba1da3b14d859633e60dd96a7b87",
            "13777e7e6c3e4ad98be1b93184eab438",
            "4262b207e4604ed1a92839f685782c03",
            "46dc6a85969141529a798700db36e41f",
            "a1c28f6aaade47d4a514f1a2beddd63e",
            "300132bbe5a0408d849f33049be55fc3",
            "ffd3e11f4e264e5c8b51f9132f960109",
            "840a7a1d90ff4ff7802c994f8d6123ef",
            "2fc018cc3cb24668b5fbc16d0310d690",
            "337035b80f204f09a7efa1247ee05fdb",
            "c86bdd845eeb46519d6d45eb1587a46a",
            "3edee92fd8734927822b88c864faefc9",
            "a19fb494014e4c0ba893adcd17fc14c8",
            "615b5fe5ee60448c8e98fa7af7510f83",
            "6b4ab6816d834827bc3d92ecd2dd0319",
            "f60b8247c1694467880bd84bc632ef82",
            "2c5ce22bbf8e4d3b8ae7940bd04e0155",
            "c40d306565a74d0280f5955a7ddb8161",
            "cd33b8b9bb9349d8b3a894e04f176ec6",
            "b66ba1a7bb3c4fc6adc32b904ca8d319",
            "816afc9cbbc04eee9d26fac8c8280977",
            "cc61ad6641034d58ad28441d48d9d672",
            "518359d3596d4972b00b40f0c3fefc07",
            "8e94056a5d9748ce8409fb54fd9d0795",
            "a5e87613c80042e1a76d9ac618148a5b",
            "5b2d13214c924298a4755025f4e7f186",
            "fc1556eed9044baf86a4f958ed70fb51",
            "5fd0721b47c04a248ef0c06b5e6c2f99",
            "340993083f0f41a6b142560ad9dd91c5",
            "ec00f5be0ec848f6a2c464651f99ddec",
            "73e1d7d047c54fbcb688b517ec21a330",
            "f4cd3c729184420b9442576571467c39",
            "4ff9fd5b5f5446b68be80672eaed9d3f",
            "072392a1ab2443bd88d8aaf968053aa2",
            "3834f6f974164f0d9446c819adf1762a",
            "60e54e4075374731a595d6a4489fd2f1",
            "eaf0dc35357644af97ddf54521ecc124",
            "4ce405ad9daa43fb857e44034b07ad98",
            "813324efc52e4958a974cac78a484eae",
            "cc63952cb8a2446ca925ced2176ab304",
            "7a4a7fe8db7644e7b1127d1de705cfdd",
            "13ca805898a64b3ebeea35d0573de91b",
            "cca4fbe6e32d4bf59c58b26542ed35c1",
            "17916c14fd354f6ebafe47f7d0233f9a",
            "65ee7fb0d61d44978f9c74db726b9aac",
            "47b86e00427c414cb7daa66997fa7969",
            "47137c002a914f26b6645da333e734db",
            "215cd045400544a99bef60c965a16715",
            "81a3167cf833414883f805c93442491f",
            "a541d41ea0134c36bfebd3083854a5be"
          ]
        },
        "id": "J7StT4Jalyc3",
        "outputId": "d50a253e-52b3-4b7f-a034-b23805280c7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==============================\n",
            "Running Strategy 4 Compare reranking models and no reranking.\n",
            "==============================\n",
            "\n",
            "Running with rerank=True, rerank_model=ms-marco-MiniLM-L-12-v2, saving to ./output/llama3_faiss_rerank_ms-marco-MiniLM-L-12-v2.csv\n",
            "Reranking is set to True.\n",
            "Initializing the Hugging Face model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af2493af17844bb98c354df6ee5cf029",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a4d6a32581c43e5b75fcaa8225e692a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba04e092eda64bcd8d2bc615a620dc2e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59542e630e4145d29d9f20b3f0fa6283",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "754df70986ef401285dbbce5c0dc7809",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "818df5cedb5449a886cfafbe83cd3584",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0d506f15f254152bc9a526246b10fc3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "929622420a3b468e82411d9b37ca47f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ceae01abc0c84b07b4619804ffc06e5e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7d6f4a727d84b158a8fe6d8a50b0b79",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "<ipython-input-6-10305cf1b4b8>:110: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized successfully!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b265f73a65924168a4523ac4ab55f582",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4326b52930642aeb97b99685f540e7e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b808f24e403146de857a07ad7cc4fe70",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c75271feb8504ba0bce5acc099ff0edb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d16eb67ec7474dae9c5a189a6175ae10",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0aaae1452824bda916c64f4689ad48d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eebb28021d77452cbd95e8188321b9d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "554543bdda454b8dab1e9a28c20b29c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0776c5a89acf472ea4a094214bbb7129",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ffd3e11f4e264e5c8b51f9132f960109",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c40d306565a74d0280f5955a7ddb8161",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start loading QA from ./data/test/test_questions.csv\n",
            "574\n",
            "Loaded 574 QAs\n",
            "Embeddings already exist! Loading embeddings with dimensionality: 384\n",
            "End loading\n",
            "Building the vectorstore  FAISS ...\n",
            "Retriever built successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 574/574 [00:00<00:00, 30792.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA evaluation completed! Results saved to ./output/llama3_faiss_rerank_ms-marco-MiniLM-L-12-v2.csv\n",
            "Running with rerank=True, rerank_model=ms-marco-MultiBERT-L-12, saving to ./output/llama3_faiss_rerank_ms-marco-MultiBERT-L-12.csv\n",
            "Reranking is set to True.\n",
            "Initializing the Hugging Face model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "340993083f0f41a6b142560ad9dd91c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized successfully!\n",
            "Start loading QA from ./data/test/test_questions.csv\n",
            "574\n",
            "Loaded 574 QAs\n",
            "Embeddings already exist! Loading embeddings with dimensionality: 384\n",
            "End loading\n",
            "Building the vectorstore  FAISS ...\n",
            "Retriever built successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 574/574 [00:00<00:00, 28822.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA evaluation completed! Results saved to ./output/llama3_faiss_rerank_ms-marco-MultiBERT-L-12.csv\n",
            "Running with rerank=False, saving to ./output/llama3_faiss_no_rerank.csv\n",
            "Initializing the Hugging Face model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc63952cb8a2446ca925ced2176ab304",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized successfully!\n",
            "Start loading QA from ./data/test/test_questions.csv\n",
            "574\n",
            "Loaded 574 QAs\n",
            "Embeddings already exist! Loading embeddings with dimensionality: 384\n",
            "End loading\n",
            "Building the vectorstore  FAISS ...\n",
            "Retriever built successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 10/574 [00:03<02:35,  3.62it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "100%|██████████| 574/574 [03:06<00:00,  3.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA evaluation completed! Results saved to ./output/llama3_faiss_no_rerank.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "strategy_number = 5  # Change this to 0, 1, 2, 3, 4, 5, or 6 to test other strategies\n",
        "run_rag_strategy(strategy_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518,
          "referenced_widgets": [
            "d5ca7aa4ad6a4c68bd09f08a4d65105c",
            "425930bd7f2a41dbbfb291e963e25b2c",
            "1991ea95c3674ac6a58fe43c2a3117e0",
            "98976f68aa8e4c0e8713983e4aa488fd",
            "69c79309641140298d3dd8d81cafb092",
            "7ae29132737c4e7190070380f6736ed0",
            "2d7bd01f603d413fab04ae30e01208cb",
            "1c43f3676e2548589d92772019d18236",
            "9402659fe07644ababc4ea50d4ce4e55",
            "59443f4d776a4bc6874fea2040d182ce",
            "3e872c9a93774b519e01f3d79de277c3",
            "75859dfe897545c98affc99515217a02",
            "1ccb8311034749edb1912589a8d479f1",
            "256229d51e59466695a8c656fdc8f7e1",
            "6a5a82e3a091400d81acc0207a7a03c4",
            "12e22a0e4048492f95c82b46f71a0fcd",
            "7a20d4dfea8d4c39867457527c448ecb",
            "708c1c97dca5433abf42caad442d0e44",
            "f5718b2e0218495386932827859c58f3",
            "55b81595ee3449f1876f4cea7ba2d8a6",
            "e4b882d78a824701a062e71e11708fc6",
            "f123f638aa7e4f68bbd2b383896face3"
          ]
        },
        "id": "rSVEeWVbl1ox",
        "outputId": "1cbcde29-6514-49f8-e524-5a0c67fa42c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing the Hugging Face model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5ca7aa4ad6a4c68bd09f08a4d65105c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized successfully!\n",
            "Start loading QA from ./data/annotated/QA_pairs_1.csv\n",
            "3938\n",
            "Loaded 100 QAs\n",
            "Embeddings already exist! Loading embeddings with dimensionality: 384\n",
            "End loading\n",
            "Building the vectorstore  FAISS ...\n",
            "Retriever built successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA evaluation completed! Results saved to ./output/llama3_faiss_similarity_top3.csv\n",
            "Initializing the Hugging Face model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75859dfe897545c98affc99515217a02",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized successfully!\n",
            "Start loading QA from ./data/annotated/QA_pairs_1.csv\n",
            "3938\n",
            "Loaded 100 QAs\n",
            "Embeddings already exist! Loading embeddings with dimensionality: 384\n",
            "End loading\n",
            "Building the vectorstore  FAISS ...\n",
            "Retriever built successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:24<00:00,  4.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA evaluation completed! Results saved to ./output/llama3_faiss_mmr_top3.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "strategy_number = 6  # Change this to 0, 1, 2, 3, 4, 5, or 6 to test other strategies\n",
        "run_rag_strategy(strategy_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e20c790727d9444c8c62583fe1d3d110",
            "89f1769a1fdc43dba761aa5caca4fc3b",
            "c5ceb10b658f4f7485e98640c350f7af",
            "9e2e9107c3194778afa67b26d3a4a5fa",
            "156ddb32f7e64b27ba826ea179ba2567",
            "3def252693be41df9fc1982af05b85c0",
            "cfd46b137ca141febc4c8a6fddc0ae63",
            "378519249c8e46c9941b9221dea53099",
            "d375b73726934a68abddb935f97c0e03",
            "fb167f7826524765ace3c827bfd29b88",
            "627d07d3708049c5b4c204c0abe92e7d",
            "56606deadbcb438eaac835cd7c0852d2",
            "b0af15a0a854451ca03e26c10971237d",
            "5e7ac2633eb84a709e77f36a4821b323",
            "5f43904f9a6a40009a046ed73e5f9766",
            "419ab4e9221f4e9f9f936ff6b72b53d5",
            "a95db70f9b3d46b19f9c3bac4b2f4f56",
            "452694a102364ec99cd4891afc0b425f",
            "5003e7b087ea428c9a2ba5a3d795b34d",
            "8a44013f63344ffcaaa764d7f459cf1d",
            "5ff60c6045a94a41ac7eadee15a05611",
            "2fb308b5612543c0a9e2190377782230",
            "726c16f009ae491eabf7a606ff0d9891",
            "7ac5fd76d70e43248e1960280af5b58e",
            "c857b05b501a46449ebc3a3812bfb8f7",
            "c7d4b8f83c2843d7aa13c93cae4a0413",
            "a1af744a4c2d4871aaa74e7ee63cb3a9",
            "2443c94f89a84368902c7fdef61b9012",
            "40c5d1bc24c44d8a89989db009b22e0d",
            "17036b10a9474b70afd52905454a80bc",
            "2012327e2eb543ebb379a6e3ea3da5a2",
            "f7e25746cca14e67a78ca676c9ce2ff4",
            "a7517044e1154d15b167ce4c30cff822",
            "5d2ba5ff370b42438ad797d14fddddf5",
            "436a76c4c1ca4fbb859eb1915e36be43",
            "5bd579b451994ebdbd14c077dc5f40c0",
            "6d4d754c41ed4e6dab89257af71d87c0",
            "8b7f5300564142d59df10df5f36c190f",
            "e34cfba77d0a4e2fa44f722f25e20ca6",
            "41e340ad535e487ba384f0e9461c117f",
            "76cc04c4988243299b060c3042b9a47c",
            "e1ba36e906e347f3a960052371cb4e49",
            "1bdba964a14d4620b7c5e86ea3fabc42",
            "cae28086d4694ec9bf9b221b0222f636",
            "01c883bb4c654f96823a2b27c06f4181",
            "9e0d8cdc8ec342a5bd47ee15978db920",
            "04aa71229be543e69df84d452c067c2d",
            "4e09dd04b529440f9ed2a1420ccb467a",
            "9b0264f3bedd4effa090b5a1f0e8e673",
            "f7038af1ea534bbea0ab9ee3e671147a",
            "3a43a9f59c6043bd9cb92250eb2d7160",
            "c210cd56f5414bd1ad50e64bbc3711c9",
            "953c2e44171446139a3b2a3d7242acc0",
            "7b9791e2bab4406f9a2a63536a7cea76",
            "43b60540c0c34610a7e4377b0c2dd261",
            "e5b38d32481e40b3b13cdb7fc4482e20",
            "b3f26f140bb8476ba76e2f371d091c03",
            "dfa8869abd3c46c5a2c15b7f0cf731b6",
            "b54183fb1eb74c168316a3cee994f8b1",
            "5af58b5064334418ac82c0fd43a46822",
            "ae7aa16292d34942b8a9c42bbaa73285",
            "7f1eeaa3de1b452c9f90bbb6c5aa91bb",
            "03e878bb88de429d9fc87c986fa21d5c",
            "a1706a862ebf43d38166ae82c1979c4e",
            "c57066fe21e54a2db7755050ae1d83b1",
            "ea727729d1e9426aa73c106e23a26d2f",
            "bf007cb0c7a840b0abd63486ce6f76a9",
            "47b88c67deed4fa49beecd7dcd07bf22",
            "f9c917df58054fd9965626313bda3ae9",
            "0288b73fec7947c4bd4e1ee22368f35b",
            "304cc2086a1f4fb29316474d3ddb4f0d",
            "fd01f40a1c684f099451d7411e22363c",
            "fceea4b4e11a4b5e98e0814f2ae46810",
            "4a902442a4e945cab1c58e1fa3da4fcf",
            "25a9a62de3104a968a44d5fc9a99550e",
            "9ae3502e23fd45f59aba3da3216200cc",
            "3538c3beb4344cabaa582a6236a6c46e",
            "5625ea9e54c7473ebc758eabf2131f22",
            "b39c0b32454a4b7ab876b5e5f446d1d4",
            "ec6fd09be9cf4a399dc883fb3a190def",
            "6e9b2cd0d3bc403da7200350eb6ab4e0",
            "4f352630465b4abaa39a7bac3c6c704d",
            "1f4dbc04b2ab41d98b3f4722dd30b706",
            "68f9e91eafea4203b264d688fdd6f4c6",
            "c20d54896710420abd37fbdd45d278b4",
            "e4c635932cf140ba84b47ad4493f1370",
            "e79830df3834480db839223a4feff6ca",
            "6ec8019cdac147a58bc3f2e8a7795c32",
            "41b7df46ee9c41a3b7594a113728e33e",
            "a37a3802b17b409587fd8d0dbd4be641",
            "f83534cc518e47899670b684f0e98d58",
            "3a53d1caa26e4a8794e9049cd4d890b9",
            "5f53c003cacd453393a5f70283eec70c",
            "d6f6e516f8ca44b1932857b749c674bc",
            "c0092b19a2084a809eb91c3ba94eca63",
            "041726298fe84d99acea27c3a79eccb6",
            "99d77ed0670f4e19b354a9804fb3fbe8",
            "943d316a2722490a8c29c9f95892133e",
            "ef8530de04244b24a6caf6021440c2bf",
            "77151d112f9046689400f512d20e0ea2",
            "f48614666ccd4144a0b5a3cb0fcedcca",
            "80ccdf100514410b9784c22ac88eda61",
            "760e2beb76cf434fa324ca9f8194a87b",
            "81b69451bcb94bb2ac81c019318dd3de",
            "491a5b5dc2074c4c8207277996e4e945",
            "148867bcb24d477e9d9b05153205c475",
            "ba78adab0bbe471b997dd963ddfe06de",
            "c7398710e85040c5ba6ba42c37b13487",
            "d0ae08d9d98c40699d91318b8a6ce845",
            "ca6281c7883f4fd38a86f9c029ed748e",
            "5c64f37372a54187ad72f2561573e9f3",
            "5990b5cc1cf94994a124ae1bbba2306a",
            "a971cd1bda744715a419e70f27097c8b",
            "683ab9ee9f8440f0bf77dae973d9cd4d",
            "e4535144ecd74b6cbd3edf65d509e740",
            "9763a084086c449a972e36f209805be0",
            "3f42bc0d5f894dc2baa5d940fecee226",
            "20cd9a702b604ec08522e3b434ecf700",
            "2e2cc827ae674ad686b360cf8562110d",
            "a1d89ab08775446b84e762ec2cde2d13",
            "e81f1d1c6e3a4bf2a5771d20f97acbad",
            "e81dd51a847d40158b8ad4144f2a1a73",
            "c55dce639e4f4e6891d268a5e97df9fa",
            "4fec644156e04055bc0a9492edd0d2c7",
            "46aa6d0bd6ef4b928f590ad9ecd12bc2",
            "8da3e64ae2ad4c1c9bb680ae8f0df7a1",
            "b8ced29ac7ae4b53971ad20cf3c18b8b",
            "ec7ed73b49c744c7a59df7cf8e9c45b4",
            "99e8043a4ec9443d966e6c166a4e0456",
            "1acf5d11510449499252b2d1f3078f96",
            "3d438a4eb08d4e88ac795a57475ce149",
            "a589c6909cef4b0bbc4791a68cbb76f1",
            "2e6600f62d0d427ebe516fb2d3cd8915",
            "0645df4150e540d68e65ded3bc8ed9c1",
            "9263dfb6d5e54100b63be4f503465f03",
            "6ca50d7714bc4ce78c7f21908d8bb129",
            "f66e5c9a4e024429964ff77bd40bf6b7",
            "5147bb66a7b044ea8f7a5e39ce602f47",
            "43cfbce110d2492b95eba7cf01644ee6",
            "cb30e31c9ad74e9e8119339749a54a19",
            "1fd013827bcc460eaa37764ed02231de",
            "b31424baa3054344be9803a06240b643",
            "17a97af7f6694b6e9092e1c6ff9a493f",
            "e4e93d1dabd345df9c7e494df44dcc5d",
            "e657b012006c4be293fc516791e0f62f",
            "e5411c9b7bc04a9aa7bdf89f4befa10b",
            "a9272a51562c4eee8270758c13adc020",
            "21f89a2608a64adc859e4d78e605e90b",
            "d7abeb5f947945abaae74eff884d5587",
            "f32e1b45b78e4d69a9b2c13517578193",
            "b7c5310c1607401998f48c1ec7958b2b",
            "eeb83c5e66bc42538e73df384fc7085b",
            "4fdaf3658b4049b6b09f21aa8105a5c7",
            "ae602fc388b44d6890817b8e6c34a91b",
            "1b5afc24052944b5ac61d8accf5eca13",
            "01646b0777d64d2687795aa9acf2b49b",
            "e979feef9fae44cc9d96343b057707ab",
            "41e210c2602e43e0aa3bc4252631586e",
            "2b0809bf14764d9a8d46a4e9c11af5cd",
            "7b22a39205cd443fbade46c699106c53",
            "e24ae6dc73404286bf006b7f46080622",
            "498af30c79e340fcab1ae05ffe9d8305",
            "62fb1c06a549422aa5b8c4a4cd861082",
            "8f4d97aa390842f385550a9e327c55e1",
            "de42d7f3287a453c936dab877f7cb7df",
            "3dff702272854bdb99fa488d475e10f1",
            "4529d8b32cb2495688f8ba95f982f8ee",
            "18ca929b38f84e4f9c448ed3cbf71ab1",
            "75ad1ce4ea884c909bf1f9218c2f9f77",
            "1f46a9ee83304599abb0a72907ce2361",
            "9e35887d60d74540bd1c359670dbaca3",
            "0f8ddf8a1c9b4e98b23c2a2cf549d246",
            "463063c3440d443fad99ee38f62389ef",
            "2b7c39ab36bf4de9a294333029555a03",
            "13eaf4f4930c49738d0446078f1f16c8",
            "5a41aa6490674087922e37da154da226",
            "65d37e36b05d407680300b43ef26b7f6",
            "b2efe127faa94fa4b5ef09945e2299a5",
            "24815ba925e64acf88b278cc865eef47",
            "afdeb0544cf1429cab4c93c3341c286b",
            "48dc9473b880466b8cc518d3386656cf",
            "abc445b7168641bc8d7d50f73348bb2d",
            "61e59bc48ad3456d9c834b3d8a33a3de",
            "53320b93e9e740b28cc2833b364b6119",
            "2a3e7205902e4501beea04f035cf2ae2",
            "d529b127a45d4f26b19bf709c47e4012",
            "b773be5b769e4d0392ba45eebb979242",
            "374cf923cf8241a8a565e5f8074450f1",
            "d363ccc9b60f43639a963d0afc5819c3",
            "d4ce221e01274d5d91544f461f008acb",
            "af1ebc7f73594cc0b7e7f2735a1e3b07",
            "0e93a2f139674032b6cfcc95f396dbb8",
            "a80e3b1cde4e42f0b16d6406426e38d7",
            "a873bed1194a4440a1d1bf4dc14fe58a",
            "d5124ae8af60449b9b39e34975704131",
            "e615f1b2918e4649a54e2801f032e449",
            "559f27b3879e46ecb8eb78e2fecdf95b",
            "89a86df7d82f439d855bd26c06e7fabc",
            "2469d065b625428aa8aecc922922e63b",
            "fb81873f5e3a40268b14cc42ffbd6c08",
            "700c869aaeac4ae5bb08c7382ee50ef0",
            "3865cba4042649bdb7ff6342a14cf274",
            "5a83766578f44b62af8eea8e59f006a2",
            "97cca354ad95482c958532586a6e7950",
            "32ae616708524c208b9afb50ea233ef7",
            "c5c7ce0715d746fb97dbe94ea9cc2176",
            "d24eea38ee3d42e1a4405f3e21cd76a7",
            "9d24f7a5e46a45329bde94950b8f9f29",
            "758c59299b6c4f04929419003c1ba333",
            "c17cbc077b0f4e618e8cae19bfe29610",
            "c005b8b75c05455d83a63e8846097b09",
            "b69716f70e534ca39434c58ae86ea127",
            "f05d179667d54f149a6c913b5121f9f9",
            "7e9bff5d5a21427cb9ba9fde23090e73",
            "571b1b62d8b34f7fae4f7c5f6a928a66",
            "5b2a88c6ac6442bea8b0e0443ce691e5",
            "1b105e1ecc164efa9d797902e62da7cb",
            "01c937c580754663b6667894aeefa913",
            "a90328b94ed0420bae5b7247a0c4fd98",
            "1cd2c01c37214024a71aacb9694624d7",
            "d0e46423274749a3b1dac007eea5bfcb",
            "a93d9f09297b419497fe6e6e87c290fa",
            "b469e727397a49e9b8e4477697f6ed12",
            "469de823d99e479f8f4436b9f73c4fda",
            "53dd2a34d5cc45a5ae069b3741a858c0",
            "faa49674e90a496abcead57c5e721823",
            "e3cda865ff9346cbb001e5b27b956e31",
            "ef1c452904894e0da3f44337370f092f",
            "b863583417484e43ae96ceef6bbfd156",
            "4b07212e4c4e4e7bb6deb86dac093a29",
            "f0c22f93070a4b2888cc08d4212ff66e"
          ]
        },
        "id": "jqbt1EEdl4WG",
        "outputId": "49beb84e-5cff-4d06-96c0-4e0adaf45722"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Strategy 6: Compare with/without hypothesis generation.\n",
            "Initializing the Hugging Face model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e20c790727d9444c8c62583fe1d3d110",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56606deadbcb438eaac835cd7c0852d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "726c16f009ae491eabf7a606ff0d9891",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d2ba5ff370b42438ad797d14fddddf5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01c883bb4c654f96823a2b27c06f4181",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5b38d32481e40b3b13cdb7fc4482e20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf007cb0c7a840b0abd63486ce6f76a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5625ea9e54c7473ebc758eabf2131f22",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41b7df46ee9c41a3b7594a113728e33e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77151d112f9046689400f512d20e0ea2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "<ipython-input-6-51ddeaa7937d>:110: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized successfully!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c64f37372a54187ad72f2561573e9f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e81dd51a847d40158b8ad4144f2a1a73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e6600f62d0d427ebe516fb2d3cd8915",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4e93d1dabd345df9c7e494df44dcc5d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b5afc24052944b5ac61d8accf5eca13",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3dff702272854bdb99fa488d475e10f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65d37e36b05d407680300b43ef26b7f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "374cf923cf8241a8a565e5f8074450f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2469d065b625428aa8aecc922922e63b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c17cbc077b0f4e618e8cae19bfe29610",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0e46423274749a3b1dac007eea5bfcb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start loading QA from ./data/annotated/QA_pairs_1.csv\n",
            "3938\n",
            "Loaded 100 QAs\n",
            "Embeddings already exist! Loading embeddings with dimensionality: 384\n",
            "End loading\n",
            "Building the vectorstore  FAISS ...\n",
            "Retriever built successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 1/100 [00:05<08:29,  5.14s/it]WARNING:root:Hypothesis unavailable for query: Who will moderate the Q&A for the \"Political Advertisement XI\" screening?\n",
            "  6%|▌         | 6/100 [00:18<04:49,  3.08s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  9%|▉         | 9/100 [00:30<05:26,  3.59s/it]WARNING:root:Hypothesis unavailable for query: What is the name of the record shop that also hosts concerts and has a huge selection of vinyl?\n",
            " 10%|█         | 10/100 [00:31<04:13,  2.82s/it]WARNING:root:Hypothesis unavailable for query: What is the name of the artist who created the artwork \"Arisaema ringens\"?\n",
            " 11%|█         | 11/100 [00:35<04:37,  3.12s/it]WARNING:root:Hypothesis unavailable for query: What is the amount of the beginning balance of the Community Development Trust Fund in 2023?\n",
            " 13%|█▎        | 13/100 [00:39<04:11,  2.89s/it]WARNING:root:Hypothesis unavailable for query: What is the amount of funding for the City - Operating Restoration of ELA non-personnel line?\n",
            " 18%|█▊        | 18/100 [00:51<03:27,  2.54s/it]WARNING:root:Hypothesis unavailable for query: What is the title of the book by Hax McCullough and Mary Brignano?\n",
            " 22%|██▏       | 22/100 [01:01<03:41,  2.83s/it]WARNING:root:Hypothesis unavailable for query: What is the location of the event \"Introduction to Neurodiversity and Neuroinclusive Learning\"?\n",
            " 23%|██▎       | 23/100 [01:02<02:56,  2.29s/it]WARNING:root:Hypothesis unavailable for query: What is the capacity of the Music Hall venue at Thunderbird Café and Music Hall?\n",
            " 24%|██▍       | 24/100 [01:03<02:21,  1.86s/it]WARNING:root:Hypothesis unavailable for query: In what year did Carnegie Mellon University receive $10 million for its Future of Science Initiative?\n",
            " 32%|███▏      | 32/100 [01:27<03:48,  3.36s/it]WARNING:root:Hypothesis unavailable for query: What is the title of the position with a 2024 rate of $81,074?\n",
            " 39%|███▉      | 39/100 [01:44<02:25,  2.38s/it]WARNING:root:Hypothesis unavailable for query: What is the name of the Douglas Education Center in Monessen?\n",
            " 40%|████      | 40/100 [01:45<02:01,  2.03s/it]WARNING:root:Hypothesis unavailable for query: What is the total expenditure for the Department of Human Resources & Civil Service in 2024?\n",
            " 46%|████▌     | 46/100 [02:02<02:14,  2.49s/it]WARNING:root:Hypothesis unavailable for query: What is the record of Rochester (N.Y.) (UAA) in women's volleyball?\n",
            " 47%|████▋     | 47/100 [02:06<02:34,  2.92s/it]WARNING:root:Hypothesis unavailable for query: What was the name of the organization founded by Dorothy Mae Richardson in 1968?\n",
            " 48%|████▊     | 48/100 [02:07<02:01,  2.34s/it]WARNING:root:Hypothesis unavailable for query: What was the name of the university center built in the past two decades?\n",
            " 50%|█████     | 50/100 [02:15<02:33,  3.06s/it]WARNING:root:Hypothesis unavailable for query: When is the Home Alone event happening at Heinz Hall?\n",
            " 52%|█████▏    | 52/100 [02:19<02:15,  2.82s/it]WARNING:root:Hypothesis unavailable for query: What is the name of the theater where the musical \"Evil Dead the Musical\" will be performed from September 27 to October 26, 2024?\n",
            " 57%|█████▋    | 57/100 [02:36<02:29,  3.48s/it]WARNING:root:Hypothesis unavailable for query: How many linear miles of streets does the Department of Mobility and Infrastructure maintain?\n",
            " 58%|█████▊    | 58/100 [02:40<02:30,  3.58s/it]WARNING:root:Hypothesis unavailable for query: What is the location of the exhibition celebrating the 40th anniversary of the Architecture Archives?\n",
            " 63%|██████▎   | 63/100 [02:56<02:12,  3.57s/it]WARNING:root:Hypothesis unavailable for query: What is the time of the event \"Gerstein Plays Tchaikovsky\" at Heinz Hall?\n",
            " 67%|██████▋   | 67/100 [03:12<02:07,  3.85s/it]WARNING:root:Hypothesis unavailable for query: What is the name of the local band performing at the Pittsburgh Playhouse Fall Fest?\n",
            " 68%|██████▊   | 68/100 [03:13<01:35,  2.99s/it]WARNING:root:Hypothesis unavailable for query: What is the start time of the Blind Pilot event at Music Hall at Thunderbird Café & Music Hall?\n",
            " 69%|██████▉   | 69/100 [03:17<01:41,  3.26s/it]WARNING:root:Hypothesis unavailable for query: How many years did Paul Gerlach direct the Kiltie Band?\n",
            " 73%|███████▎  | 73/100 [03:24<00:56,  2.08s/it]WARNING:root:Hypothesis unavailable for query: What was the amount of money pledged by the Richard P. Simmons family to the PSO?\n",
            " 76%|███████▌  | 76/100 [03:32<00:59,  2.49s/it]WARNING:root:Hypothesis unavailable for query: What is the name of the college that Scott Kaufman is affiliated with?\n",
            " 79%|███████▉  | 79/100 [03:38<00:49,  2.37s/it]WARNING:root:Hypothesis unavailable for query: When is the PSO Disrupt: Walk on the Wild Side event happening at Heinz Hall?\n",
            " 81%|████████  | 81/100 [03:43<00:48,  2.56s/it]WARNING:root:Hypothesis unavailable for query: What is the name of the competition Emily Richter won?\n",
            " 89%|████████▉ | 89/100 [04:08<00:39,  3.61s/it]WARNING:root:Hypothesis unavailable for query: What is the revenue for Wilkinsburg Fire Services in 2024?\n",
            " 92%|█████████▏| 92/100 [04:15<00:24,  3.01s/it]WARNING:root:Hypothesis unavailable for query: Where is the preview of \"Woman With Eyes Closed\" on WQED-FM 89.3 and wqed.org/fm on Friday, April 25, 2025 held?\n",
            " 94%|█████████▍| 94/100 [04:21<00:18,  3.04s/it]WARNING:root:Hypothesis unavailable for query: Where will the Fashion Show take place?\n",
            " 98%|█████████▊| 98/100 [04:33<00:06,  3.16s/it]WARNING:root:Hypothesis unavailable for query: What is the recommended age range for Fiddlesticks concerts?\n",
            " 99%|█████████▉| 99/100 [04:33<00:02,  2.44s/it]WARNING:root:Hypothesis unavailable for query: What is the name of the musical composition performed by the Pittsburgh Symphony Orchestra on November 15?\n",
            "100%|██████████| 100/100 [04:34<00:00,  2.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA evaluation completed! Results saved to ./output/llama3_faiss_test_hypo.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "strategy_number = 7  # Change this to 0, 1, 2, 3, 4, 5, or 6 to test other strategies\n",
        "run_rag_strategy(strategy_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1eUCPdlA216"
      },
      "outputs": [],
      "source": [
        "total_strategies = 8  # Total number of strategies (0 to 7)\n",
        "\n",
        "for strategy in range(total_strategies):\n",
        "  print(f\"\\n==============================\")\n",
        "  print(f\"Running Strategy {strategy}...\")\n",
        "  print(f\"==============================\\n\")\n",
        "  run_rag_strategy(strategy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
