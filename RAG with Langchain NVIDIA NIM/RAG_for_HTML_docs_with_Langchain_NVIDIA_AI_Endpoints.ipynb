{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7da9c54-5c59-4d9c-a207-147c5e7ad115",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf8ff51",
   "metadata": {},
   "source": [
    "# Build a RAG chain for NVIDIA Triton documentation website\n",
    "\n",
    "In this notebook we demonstrate how to build a RAG using [NVIDIA AI Endpoints for LangChain](https://python.langchain.com/docs/integrations/text_embedding/nvidia_ai_endpoints). We create a vector store by downloading web pages and generating their embeddings using FAISS. We then showcase two different chat chains for querying the vector store. For this example, we use the NVIDIA Triton documentation website, though the code can be easily modified to use any other source.  \n",
    "\n",
    "### First stage is to load NVIDIA Triton documentation from the web, chunkify the data, and generate embeddings using FAISS\n",
    "\n",
    "To run this notebook, you need to complete the [setup](https://python.langchain.com/docs/integrations/text_embedding/nvidia_ai_endpoints#setup) and generate an NVIDIA API key. To obtain an API key, Log into [API Catalog](https://build.nvidia.com/), find the model you want to use, and click “Get API Key.” This key is used to both authenticate with the docker registry to pull the NIM container (whether in the brev environment or your own cloud/local environment) and/or allow you to make API calls to the model endpoint hosted on the API catalog. The NVIDIA API catalog is a trial experience of NVIDIA NIM limited to 5000 free API credits. Upon sign-up, users are granted 1000 API credits. To obtain more, click on your profile from within the [API catalog](https://build.nvidia.com/) → ‘Request More’. If you signed up to use the API catalog with a personal email address, you will be asked to provide a business email to activate a free 90-day NVIDIA AI Enterprise license and unlock additional 4000 credits. See [NVIDIA NIM FAQ](https://forums.developer.nvidia.com/t/nvidia-nim-faq/300317) for more information regarding API credits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "980506c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025de714",
   "metadata": {},
   "source": [
    "Provide the API key by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf9a84ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your NVIDIA API key:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fcd102",
   "metadata": {},
   "source": [
    "Helper functions for loading html files, which we'll use to generate the embeddings. We'll use this later to load the relevant html documents from the Triton documentation website and convert to a vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d84c5ef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Union\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def html_document_loader(url: Union[str, bytes]) -> str:\n",
    "    \"\"\"\n",
    "    Loads the HTML content of a document from a given URL and return it's content.\n",
    "\n",
    "    Args:\n",
    "        url: The URL of the document.\n",
    "\n",
    "    Returns:\n",
    "        The content of the document.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error while making the HTTP request.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        html_content = response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {url} due to exception {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        # Create a Beautiful Soup object to parse html\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Remove script and style tags\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "\n",
    "        # Get the plain text from the HTML document\n",
    "        text = soup.get_text()\n",
    "\n",
    "        # Remove excess whitespace and newlines\n",
    "        text = re.sub(\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Exception {e} while loading document\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3d3f0c",
   "metadata": {},
   "source": [
    "Read html files and split text in preparation for embedding generation\n",
    "Note chunk_size value must match the specific LLM used for embedding genetation\n",
    "\n",
    "Make sure to pay attention to the chunk_size parameter in TextSplitter. Setting the right chunk size is critical for RAG performance, as much of a RAG’s success is based on the retrieval step finding the right context for generation. The entire prompt (retrieved chunks + user query) must fit within the LLM’s context window. Therefore, you should not specify chunk sizes too big, and balance them out with the estimated query size. For example, while OpenAI LLMs have a context window of 8k-32k tokens, Llama3 is limited to 8k tokens. Experiment with different chunk sizes, but typical values should be 100-600, depending on the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f48635f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_embeddings(embedding_path: str = \"./data/nv_embedding\"):\n",
    "\n",
    "    embedding_path = \"./data/nv_embedding\"\n",
    "    print(f\"Storing embeddings to {embedding_path}\")\n",
    "\n",
    "    # List of web pages containing NVIDIA Triton technical documentation\n",
    "    urls = [\n",
    "         \"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html\",\n",
    "         \"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/getting_started/quickstart.html\",\n",
    "         \"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_repository.html\",\n",
    "         \"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_analyzer.html\",\n",
    "         \"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/architecture.html\",\n",
    "    ]\n",
    "\n",
    "    documents = []\n",
    "    for url in urls:\n",
    "        document = html_document_loader(url)\n",
    "        documents.append(document)\n",
    "\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=0,\n",
    "        length_function=len,\n",
    "    )\n",
    "    texts = text_splitter.create_documents(documents)\n",
    "    index_docs(url, text_splitter, texts, embedding_path)\n",
    "    print(\"Generated embedding successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942934e8",
   "metadata": {},
   "source": [
    "Generate embeddings using NVIDIA AI Endpoints for LangChain and save embeddings to offline vector store in the ./data/nv_embedding directory for future re-use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27d1aced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def index_docs(url: Union[str, bytes], splitter, documents: List[str], dest_embed_dir) -> None:\n",
    "    \"\"\"\n",
    "    Split the document into chunks and create embeddings for the document\n",
    "\n",
    "    Args:\n",
    "        url: Source url for the document.\n",
    "        splitter: Splitter used to split the document\n",
    "        documents: list of documents whose embeddings needs to be created\n",
    "        dest_embed_dir: destination directory for embeddings\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    embeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\", truncate=\"END\")\n",
    "\n",
    "    for document in documents:\n",
    "        texts = splitter.split_text(document.page_content)\n",
    "\n",
    "        # metadata to attach to document\n",
    "        metadatas = [document.metadata]\n",
    "\n",
    "        # create embeddings and add to vector store\n",
    "        if os.path.exists(dest_embed_dir):\n",
    "            update = FAISS.load_local(folder_path=dest_embed_dir, embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "            update.add_texts(texts, metadatas=metadatas)\n",
    "            update.save_local(folder_path=dest_embed_dir)\n",
    "        else:\n",
    "            docsearch = FAISS.from_texts(texts, embedding=embeddings, metadatas=metadatas)\n",
    "            docsearch.save_local(folder_path=dest_embed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26802a99-f5f5-4fea-8749-b5f0d06f9312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing embeddings to ./data/nv_embedding\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "[429] Too Many Requests\n{'status': 429, 'title': 'Too Many Requests'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcreate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 27\u001b[0m, in \u001b[0;36mcreate_embeddings\u001b[0;34m(embedding_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(\n\u001b[1;32m     22\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     23\u001b[0m     chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     24\u001b[0m     length_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m texts \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39mcreate_documents(documents)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mindex_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_splitter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated embedding successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m, in \u001b[0;36mindex_docs\u001b[0;34m(url, splitter, documents, dest_embed_dir)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(dest_embed_dir):\n\u001b[1;32m     24\u001b[0m     update \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mload_local(folder_path\u001b[38;5;241m=\u001b[39mdest_embed_dir, embeddings\u001b[38;5;241m=\u001b[39membeddings, allow_dangerous_deserialization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mupdate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     update\u001b[38;5;241m.\u001b[39msave_local(folder_path\u001b[38;5;241m=\u001b[39mdest_embed_dir)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_community/vectorstores/faiss.py:338\u001b[0m, in \u001b[0;36mFAISS.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run more texts through the embeddings and add to the vectorstore.\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m    List of ids from adding the texts into the vectorstore.\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    337\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[0;32m--> 338\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__add(texts, embeddings, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_community/vectorstores/faiss.py:247\u001b[0m, in \u001b[0;36mFAISS._embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_embed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_function, Embeddings):\n\u001b[0;32m--> 247\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_function(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/embeddings.py:163\u001b[0m, in \u001b[0;36mNVIDIAEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(texts), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_batch_size):\n\u001b[1;32m    162\u001b[0m     batch \u001b[38;5;241m=\u001b[39m texts[i : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_batch_size]\n\u001b[0;32m--> 163\u001b[0m     all_embeddings\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_embeddings\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/embeddings.py:137\u001b[0m, in \u001b[0;36mNVIDIAEmbeddings._embed\u001b[0;34m(self, texts, model_type)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncate:\n\u001b[1;32m    135\u001b[0m     payload[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncate\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncate\n\u001b[0;32m--> 137\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_req\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpayload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    141\u001b[0m result \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:453\u001b[0m, in \u001b[0;36m_NVIDIAClient.get_req\u001b[0;34m(self, payload, extra_headers)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_req\u001b[39m(\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    449\u001b[0m     payload: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m    450\u001b[0m     extra_headers: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m {},\n\u001b[1;32m    451\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[1;32m    452\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Post to the API.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 453\u001b[0m     response, session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait(response, session)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:349\u001b[0m, in \u001b[0;36m_NVIDIAClient._post\u001b[0;34m(self, invoke_url, payload, extra_headers)\u001b[0m\n\u001b[1;32m    345\u001b[0m session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_session_fn()\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_response \u001b[38;5;241m=\u001b[39m response \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__add_authorization(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_inputs)\n\u001b[1;32m    348\u001b[0m )\n\u001b[0;32m--> 349\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response, session\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:442\u001b[0m, in \u001b[0;36m_NVIDIAClient._try_raise\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    440\u001b[0m     body \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease check or regenerate your API key.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;66;03m# todo: raise as an HTTPError\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheader\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mbody\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: [429] Too Many Requests\n{'status': 429, 'title': 'Too Many Requests'}"
     ]
    }
   ],
   "source": [
    "create_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9831f7ba",
   "metadata": {},
   "source": [
    "### Second stage is to load the embeddings from the vector store and build a RAG using NVIDIAEmbeddings\n",
    "\n",
    "Create the embeddings model using NVIDIA Retrieval QA Embedding endpoint. This model represents words, phrases, or other entities as vectors of numbers and understands the relation between words and phrases. See here for reference: https://build.nvidia.com/nvidia/embed-qa-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f56cadd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model = NVIDIAEmbeddings(model=\"NV-Embed-QA\", truncate=\"END\", allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e211270",
   "metadata": {},
   "source": [
    "Load documents from vector database using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "648b9d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embed documents\n",
    "embedding_path = \"./data/nv_embedding\"\n",
    "docsearch = FAISS.load_local(folder_path=embedding_path, embeddings=embedding_model, allow_dangerous_deserialization=True)\n",
    "retriever = docsearch.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c34644b7-f094-4790-aa69-e5c2a2d4dc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='NVIDIA Triton Inference Server — NVIDIA Triton Inference Server Skip to main content Back to top Ctrl+K NVIDIA Triton Inference Server GitHub NVIDIA Triton Inference Server GitHub Table of Contents Home Release notes Compatibility matrix Getting Started Quick Deployment Guide by backend TRT-LLM vLLM Python with HuggingFace PyTorch ONNX TensorFlow Openvino LLM With TRT-LLM Multimodal model Stable diffusion Scaling guide Multi-Node (AWS) Multi-Instance LLM Features Constrained Decoding Function Calling Speculative Decoding TRT-LLM vLLM Client API Reference OpenAI API KServe API HTTP/REST and GRPC Protocol Extensions Binary tensor data extension Classification extension Schedule policy extension Sequence extension Shared-memory extension Model configuration extension Model repository extension Statistics extension Trace extension Logging extension Parameters extension In-Process Triton Server API C/C++ Python Kafka I/O Rayserve Java Client Libraries Python tritonclient Package API'),\n",
       " Document(metadata={}, page_content='NVIDIA Triton Inference Server — NVIDIA Triton Inference Server Skip to main content Back to top Ctrl+K NVIDIA Triton Inference Server GitHub NVIDIA Triton Inference Server GitHub Table of Contents Home Release notes Compatibility matrix Getting Started Quick Deployment Guide by backend TRT-LLM vLLM Python with HuggingFace PyTorch ONNX TensorFlow Openvino LLM With TRT-LLM Multimodal model Stable diffusion Scaling guide Multi-Node (AWS) Multi-Instance LLM Features Constrained Decoding Function Calling Speculative Decoding TRT-LLM vLLM Client API Reference OpenAI API KServe API HTTP/REST and GRPC Protocol Extensions Binary tensor data extension Classification extension Schedule policy extension Sequence extension Shared-memory extension Model configuration extension Model repository extension Statistics extension Trace extension Logging extension Parameters extension In-Process Triton Server API C/C++ Python Kafka I/O Rayserve Java Client Libraries Python tritonclient Package API'),\n",
       " Document(metadata={}, page_content='NVIDIA Triton Inference Server — NVIDIA Triton Inference Server Skip to main content Back to top Ctrl+K NVIDIA Triton Inference Server GitHub NVIDIA Triton Inference Server GitHub Table of Contents Home Release notes Compatibility matrix Getting Started Quick Deployment Guide by backend TRT-LLM vLLM Python with HuggingFace PyTorch ONNX TensorFlow Openvino LLM With TRT-LLM Multimodal model Stable diffusion Scaling guide Multi-Node (AWS) Multi-Instance LLM Features Constrained Decoding Function Calling Speculative Decoding TRT-LLM vLLM Client API Reference OpenAI API KServe API HTTP/REST and GRPC Protocol Extensions Binary tensor data extension Classification extension Schedule policy extension Sequence extension Shared-memory extension Model configuration extension Model repository extension Statistics extension Trace extension Logging extension Parameters extension In-Process Triton Server API C/C++ Python Kafka I/O Rayserve Java Client Libraries Python tritonclient Package API'),\n",
       " Document(metadata={}, page_content='Metrics Table of Contents Checkpointing in Model Analyzer Model Analyzer Reports Deploying Model Analyzer on a Kubernetes cluster Model Navigator Debugging Guide NVIDIA... NVIDIA Triton Inference Server# Triton Inference Server is an open source inference serving software that streamlines AI inferencing. Triton Inference Server enables teams to deploy any AI model from multiple deep learning and machine learning frameworks, including TensorRT, TensorFlow, PyTorch, ONNX, OpenVINO, Python, RAPIDS FIL, and more. Triton supports inference across cloud, data center, edge and embedded devices on NVIDIA GPUs, x86 and ARM CPU, or AWS Inferentia. Triton Inference Server delivers optimized performance for many query types, including real time, batched, ensembles and audio/video streaming. Triton inference Server is part of NVIDIA AI Enterprise, a software platform that accelerates the data science pipeline and streamlines the development and deployment of production AI. Triton Architecture# The')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This should return documents related to the test query\n",
    "retriever.invoke(\"Deploy TensorRT-LLM Engine on Triton Inference Server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01153bc4",
   "metadata": {},
   "source": [
    "Create a ConversationalRetrievalChain chain. In this chain we demonstrate the use of 2 LLMs: one for summarization and another for chat. This improves the overall result in more complicated scenarios. We'll use Llama3 70B for the first LLM and Mixtral for the Chat element in the chain. We add a question_generator to generate relevant query prompt. See here for reference: https://python.langchain.com/docs/modules/chains/popular/chat_vector_db#conversationalretrievalchain-with-streaming-to-stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "574bf916-411d-4be7-bc5b-bcaee8d98e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONDENSE_QUESTION_PROMPT = PromptTemplate(input_variables=['chat_history', 'question'], input_types={}, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:')\n",
      "QA_PROMPT = PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\")\n"
     ]
    }
   ],
   "source": [
    "print(f\"{CONDENSE_QUESTION_PROMPT = }\")\n",
    "print(f\"{QA_PROMPT = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e460822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatNVIDIA(model='mistralai/mixtral-8x7b-instruct-v0.1')\n",
    "chat = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\", temperature=0.1, max_tokens=1000, top_p=1.0)\n",
    "\n",
    "retriever = docsearch.as_retriever()\n",
    "\n",
    "## Requires question and chat_history\n",
    "qa_chain = (RunnablePassthrough()\n",
    "    ## {question, chat_history} -> str\n",
    "    | CONDENSE_QUESTION_PROMPT | llm | StrOutputParser()\n",
    "    # | RunnablePassthrough(print)\n",
    "    ## str -> {question, context}\n",
    "    | {\"question\": lambda x: x, \"context\": retriever}\n",
    "    # | RunnablePassthrough(print)\n",
    "    ## {question, context} -> str\n",
    "    | QA_PROMPT | chat | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2749482",
   "metadata": {},
   "source": [
    "Ask any question about Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f5ead62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Based on the provided documents, Triton refers to the Triton Inference Server. It's a server that makes machine learning models available for inferencing. It supports multiple deep learning frameworks and scheduling and batching algorithms. It also provides a backend C API for extending its functionality. The models served by Triton can be queried and controlled by a dedicated model management API, and it supports HTTP/REST, gRPC, and C API for inference protocols.\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"What is Triton?\"\n",
    "chat_history += [qa_chain.invoke({\"question\": query, \"chat_history\": chat_history})]\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1d7dd9",
   "metadata": {},
   "source": [
    "Ask another question about Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5e80a22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided documents, the Triton Inference Server supports the following interfaces for receiving inference requests:\n",
      "\n",
      "1. HTTP/REST\n",
      "2. gRPC\n",
      "3. C API\n",
      "\n",
      "These interfaces allow clients to send inference requests to the server for processing."
     ]
    }
   ],
   "source": [
    "query = \"What interfaces does Triton support?\"\n",
    "chat_history += [\"\"]\n",
    "for token in qa_chain.stream({\"question\": query, \"chat_history\": chat_history[:-1]}):\n",
    "    print(token, end=\"\")\n",
    "    chat_history[-1] += token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd01e957",
   "metadata": {},
   "source": [
    "Finally showcase chat capabilites by asking a question about the previous query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a222b8e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Triton Inference Server supports multiple interfaces for receiving inference requests, including HTTP/REST, gRPC, and C API, to provide flexibility and convenience for different use cases and user preferences. \n",
      "\n",
      "HTTP/REST and gRPC are widely used, well-established protocols for communication between services. HTTP/REST is simple and easy to use, while gRPC offers faster communication and additional features like bidirectional streaming and flow control.\n",
      "\n",
      "The C API allows Triton to be integrated directly into applications for edge and other in-process use cases, providing more control and efficiency when embedding the inference server.\n",
      "\n",
      "By supporting these interfaces, Triton Inference Server can cater to a broader range of use cases and user requirements, making it a versatile and adaptable solution for various projects and systems."
     ]
    }
   ],
   "source": [
    "query = \"But why?\"\n",
    "for token in qa_chain.stream({\"question\": query, \"chat_history\": chat_history}):\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d231b9df",
   "metadata": {},
   "source": [
    "Now we demonstrate a simpler chain using a single LLM only, a chat LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2a2f90c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = ChatNVIDIA(\n",
    "    model='mistralai/mixtral-8x7b-instruct-v0.1', \n",
    "    temperature=0.1, \n",
    "    max_tokens=1000, \n",
    "    top_p=1.0\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \n",
    "        \"Use the following pieces of context to answer the question at the end.\"\n",
    "        \" If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
    "        \"\\n\\nHISTORY: {history}\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"\n",
    "    )\n",
    "])\n",
    "\n",
    "## Requires question and chat_history\n",
    "qa_chain = (\n",
    "    RunnablePassthrough.assign(context = (lambda state: state.get(\"question\")) | retriever)\n",
    "    # | RunnablePassthrough(print)\n",
    "    | qa_prompt | chat | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7253f735",
   "metadata": {},
   "source": [
    "Now try asking a question about Triton with the simpler chain. Compare the answer to the result with previous complex chain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b22dcbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Based on the provided context, Triton refers to the NVIDIA Triton Inference Server. It is an open-source inference serving software that streamlines AI inferencing. Triton enables teams to deploy any AI model from multiple deep learning and machine learning frameworks, including TensorRT, TensorFlow, PyTorch, ONNX, OpenVINO, Python, RAPIDS FIL, and more. It delivers optimized performance for various query types and supports inference across different environments like cloud, data center, edge, and embedded devices on NVIDIA GPUs, x86, and ARM CPU, or AWS Inferentia. Triton Inference Server is part of NVIDIA AI Enterprise, a software platform that accelerates the data science pipeline and streamlines the development and deployment of production AI.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"What is Triton?\"\n",
    "chat_history += [qa_chain.invoke({\"question\": query, \"history\": chat_history})]\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fd45fc",
   "metadata": {},
   "source": [
    "Ask another question about Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81f2d55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Does Triton support ONNX?\"\n",
    "chat_history += [\"\"]\n",
    "for token in qa_chain.stream({\"question\": query, \"history\": chat_history[:-1]}):\n",
    "    print(token, end=\"\")\n",
    "    chat_history[-1] += token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58caaebb",
   "metadata": {},
   "source": [
    "Finally showcase chat capabilites by asking a question about the previous query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea39f61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"How come?\"\n",
    "for token in qa_chain.stream({\"question\": query, \"history\": chat_history}):\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f54df1-a8a7-4403-8f5b-c8ee6235d18e",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" width=400/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
