{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35536f6-166c-4b89-8136-96417db5be30",
   "metadata": {
    "id": "b35536f6-166c-4b89-8136-96417db5be30"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976",
   "metadata": {
    "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 8 [Assessment]:** RAG Evaluation</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "Welcome to the last notebook of the course! In the previous notebook, you integrated a vector store solution into a RAG pipeline! In this notebook, you will take that same pipeline and evaluate it using numerical RAG evaluation techniques incorporating LLM-as-a-Judge metrics!\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "- Learn how to integrate the techniques from prior notebooks to numerically approximate the goodness of your RAG pipeline.\n",
    "\n",
    "- **Final Exercice**: ***By working through this notebook in the Course Environment,* you will be able to submit the coding component of the course!**\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Questions To Think About:**\n",
    "\n",
    "- As you go along, remember what our metrics actually represent. Should our pipeline pass these objectives? Is our judge LLM sufficient for evaluating the pipeline? Does a particular metric even matter for our use case?\n",
    "- If we left the vectorstore-as-a-memory component in our chain, do you think it would still pass the evaluation? Additionally, is the evaluation useful for assessing vectorstore-as-a-memory performance? \n",
    "\n",
    "<br>\n",
    "\n",
    "### **Environment Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "w_A3rZOrIeQD",
   "metadata": {
    "id": "w_A3rZOrIeQD"
   },
   "outputs": [],
   "source": [
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu ragas\n",
    "\n",
    "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# ChatNVIDIA.get_available_models()\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "norm_style = Style(bold=True)\n",
    "pprint = partial(console.print, style=base_style)\n",
    "pprint2 = partial(console.print, style=norm_style)\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models(base_url=\"http://llm_client:9000/v1\")\n",
    "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zEgV11oZmJGg",
   "metadata": {
    "id": "zEgV11oZmJGg"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 1:** Pre-Release Evaluation\n",
    "\n",
    "In our previous notebook, we successfully combined several concepts to create a document chatbot with the aim of responsive and informative interactions. However, the diversity of user interactions necessitates comprehensive testing to truly understand the chatbot's performance. Thorough testing in varied scenarios is crucial to ensure that the system is not only robust and versatile but also aligns with user and provider expectations.\n",
    "\n",
    "After defining your chatbot's roles and implementing the necessary features, evaluating it becomes a multi-stage process:\n",
    "\n",
    "- **Typical Use Inspection:** Start by testing scenarios most relevant to your use case. See if your chatbot can reliably navigate discussions with limited human intervention.\n",
    "\n",
    "    - Additionally, identify limitations or compartments that should be redirected to a human for inspection/supervision (i.e., human swap-in to confirm transactions or perform sensitive navigation) and implement those options.\n",
    "\n",
    "- **Edge Case Inspection:** Explore the boundaries of typical use, identifying how the chatbot handles less common but plausible scenarios.\n",
    "\n",
    "    - Before any public release, assess critical boundary conditions that could pose liability risks, such as the potential generation of inappropriate content.\n",
    "\n",
    "    - Implement well-tested guardrails on all outputs (and possibly inputs) to limit undesired interactions and redirect users into predictable conversation flows.\n",
    "\n",
    "- **Progressive Rollout:** Rolling out your model to a limited audience (first internal, then [A/B](https://en.wikipedia.org/wiki/A/B_testing)) and implement analytics features like usage analytics dashboards and feedback avenues (flag/like/dislike/etc).\n",
    "\n",
    "Of these three steps, the first two can be done by a small team or an individual and should be iterated on as part of the development process. Unfortunately, this needs to be done frequently and can be prone to human error. **Luckily for us, LLMs can be used to help out with LLM-as-a-Judge formulations!**\n",
    "\n",
    "*(Yeah, this probably isn't surprising by now. LLMs being strong is why this course is here...).*\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 2:** LLM-as-a-Judge Formulation\n",
    "\n",
    "In the realm of conversational AI, using LLMs as evaluators or 'judges' has emerged as a useful approach for configurable automatic testing of natural language task performance:\n",
    "\n",
    "- An LLM can simulate a range of interaction scenarios and generate synthetic data, allowing an evaluation developer to generate targeted inputs to eliciting a range of behaviors from your chatbot.\n",
    "\n",
    "- The chatbot's correspondence/retrieval on the synthetic data can be evaluated or parsed by an LLM and a consistent output format such as \"Pass\"/\"Fail\", similarity, or extraction can be enforced.\n",
    "\n",
    "- Many such results can be aggregated and a metric can be derived which explains something like \"% of passing evaluations\", \"average number of relevant details from the sources\", \"average cosine similarity\", etc.\n",
    "\n",
    "This idea of using LLMs to test out and quantify chatbot quality, known as [**\"LLM-as-a-Judge,\"**](https://arxiv.org/abs/2306.05685) allows for easy test specifications that align closely with human judgment and can be fine-tuned and replicated at scale.\n",
    "\n",
    "**There are several popular frameworks for off-the-shelf judge formulations including:**\n",
    "- [**RAGAs (short for RAG Assessment)**](https://docs.ragas.io/en/stable/), which offers a suite of great starting points for your own evaluation efforts.\n",
    "- [**LangChain Evaluators**](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/), which are similar first-party options with many implicitly-constructible agents.\n",
    "\n",
    "Instead of using the chains as-is, we will instead expand on the ideas and evaluate our system with a more custom solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fDDNaBA9N3XM",
   "metadata": {
    "id": "fDDNaBA9N3XM"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 3: [Assessment Prep]** Pairwise Evaluator\n",
    "\n",
    "The following exercise will flesh out a custom implementation of a simplified [LangChain Pairwise String Evaluator](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/comparison/pairwise_string/). \n",
    "\n",
    "**To prepare for our RAG chain evaluation, we will need to:**\n",
    "\n",
    "- Pull in our document index (the one we saved in the previous notebook).\n",
    "- Recreate our RAG pipeline of choice.\n",
    "\n",
    "**We will specifically be implementing a judge formulation with the following steps:**\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "**The chain should be a simple but powerful process that tests for the following objective:**\n",
    "\n",
    "> ***Does my RAG chain outperform a narrow chatbot with limited document access.***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**This will be the system used for the final evaluation!** To see how this system is integrated into the autograder, please check out the implementation in [`frontend/server_app.py`](frontend/server_app.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bh8jaOqak0f",
   "metadata": {
    "id": "1bh8jaOqak0f"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 1:** Pull In Your Document Retrieval Index\n",
    "\n",
    "For this exercise, you will pull in the `docstore_index` file you created as part of your earlier notebook. The following cell should be able to load in the store as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tlE7a2lseLOy",
   "metadata": {
    "id": "tlE7a2lseLOy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Constructed aggregate docstore with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">324</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> chunks</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mConstructed aggregate docstore with \u001b[0m\u001b[1;36m324\u001b[0m\u001b[1;38;2;118;185;0m chunks\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Sample Chunk:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSample Chunk:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper: MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning\n",
      "\n",
      "Summary: Huge language models (LMs) have ushered in a new era for AI, serving as a\n",
      "gateway to natural-language-based knowledge tasks. Although an essential\n",
      "element of modern AI, LMs are also inherently limited in a number of ways. We\n",
      "discuss these limitations and how they can be avoided by adopting a systems\n",
      "approach. Conceptualizing the challenge as one that involves knowledge and\n",
      "reasoning in addition to linguistic processing, we define a flexible\n",
      "architecture with multiple neural models, complemented by discrete knowledge\n",
      "and reasoning modules. We describe this neuro-symbolic architecture, dubbed the\n",
      "Modular Reasoning, Knowledge and Language (MRKL, pronounced \"miracle\") system,\n",
      "some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs'\n",
      "MRKL system implementation.\n",
      "\n",
      "Page Body: .\\n11\\nFormula\\nMean\\nSTD\\nf=((A+B)*C)\\n0.288\\n0.267\\nf=(A+B*C)\\n0.406\\n0.349\\nf=((A-B)*C)\\n0.562\\n0.194\\nf=(A/(B/C))\\n0.677\\n0.228\\nf=(A-B*C)\\n0.697\\n0.266\\nf=(A*(B-C))\\n0.797\\n0.325\\nf=((A+B)/C)\\n0.867\\n0.265\\nf=(A-(B-C))\\n0.930\\n0.057\\nf=((A-B)/C)\\n0.940\\n0.120\\nf=(A-B/C)\\n0.955\\n0.042\\nf=(A/(B+C))\\n0.960\\n0.042\\nf=(A/(B-C))\\n0.962\\n0.094\\nf=(A+B/C)\\n0.964\\n0.049\\nf=(A*(B/C))\\n0.970\\n0.052\\nf=(A*B+C)\\n0.973\\n0.025\\nf=(A*(B+C))\\n0.974\\n0.025\\nf=(A/B+C)\\n0.981\\n0.024\\nf=(A/B/C)\\n0.985\\n0.017\\nf=(A/B-C)\\n0.987\\n0.022\\nf=(A/B*C)\\n0.99\\n0.015\\nf=(A-(B+C))\\n0.99\\n0.017\\nf=(A*B-C)\\n0.992\\n0.015\\nf=(A/(B*C))\\n0.995\\n0.012\\nf=(A-B+C)\\n1\\n0\\nf=(A+B+C)\\n1\\n0\\nf=(A-B-C)\\n1\\n0\\nf=(A*B/C)\\n1\\n0\\nf=(A+B-C)\\n1\\n0\\nf=(A*B*C)\\n1\\n0\\nTable 5:\\nGeneralization for two-operation problems. Mean and standard deviation\\nacross 10 partitions of the formulae to train and test\n"
     ]
    }
   ],
   "source": [
    "## Make sure you have docstore_index.tgz in your working directory\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embedder = NVIDIAEmbeddings(model=\"nvidia/embed-qa-4\", truncate=\"END\")\n",
    "\n",
    "!tar xzvf docstore_index.tgz\n",
    "docstore = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "docs = list(docstore.docstore._dict.values())\n",
    "\n",
    "def format_chunk(doc):\n",
    "    return (\n",
    "        f\"Paper: {doc.metadata.get('Title', 'unknown')}\"\n",
    "        f\"\\n\\nSummary: {doc.metadata.get('Summary', 'unknown')}\"\n",
    "        f\"\\n\\nPage Body: {doc.page_content}\"\n",
    "    )\n",
    "\n",
    "## This printout just confirms that your store has been retrieved\n",
    "pprint(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")\n",
    "pprint(f\"Sample Chunk:\")\n",
    "print(format_chunk(docs[len(docs)//2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dib0F-t2N4LJ",
   "metadata": {
    "id": "dib0F-t2N4LJ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 2: [Exercise]** Pull In Your RAG Chain\n",
    "\n",
    "Now that we have our index, we can recreate the RAG agent from the previous notebook! \n",
    "\n",
    "**Key Modifications:**\n",
    "- To keep things simple, feel free to disregard the vectorstore-as-a-memory component. Incorporating it will require some more overhead and will make the exercise a bit more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "XBi6Y8b8aXd2",
   "metadata": {
    "id": "XBi6Y8b8aXd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's something interesting: Did you know that researchers are developing Artificial Intelligence (AI) agents that can explore complex environments independently? This is thanks to techniques like \"Strangeness-driven exploration\" (Kim et al., 2024) and \"Individual contributions as intrinsic exploration scaffolds for multi-agent reinforcement learning\" (Li et al., 2024). These agents are designed to learn and adapt in dynamic settings, such as autonomous vehicles and robotics."
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
    "llm = instruct_llm | StrOutputParser()\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name: out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked you a question: {input}\\n\\n\"\n",
    "    \" The following information may be useful for your response: \"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational)\"\n",
    "    \"\\n\\nUser Question: {input}\"\n",
    ")\n",
    "\n",
    "def output_puller(inputs):\n",
    "    \"\"\"\"Output generator. Useful if your chain returns a dictionary with key 'output'\"\"\"\n",
    "    if isinstance(inputs, dict):\n",
    "        inputs = [inputs]\n",
    "    for token in inputs:\n",
    "        if token.get('output'):\n",
    "            yield token.get('output')\n",
    "\n",
    "#####################################################################\n",
    "## TODO: Pull in your desired RAG Chain. Memory not necessary\n",
    "\n",
    "## Chain 1 Specs: \"Hello World\" -> retrieval_chain \n",
    "##   -> {'input': <str>, 'context' : <str>}\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)  ## GIVEN\n",
    "# context_getter = RunnableLambda(lambda x: x)  ## TODO\n",
    "context_getter = itemgetter('input') | docstore.as_retriever() | long_reorder | docs2str  ## TODO\n",
    "retrieval_chain = {'input' : (lambda x: x)} | RunnableAssign({'context' : context_getter})\n",
    "\n",
    "## Chain 2 Specs: retrieval_chain -> generator_chain \n",
    "##   -> {\"output\" : <str>, ...} -> output_puller\n",
    "# generator_chain = RunnableLambda(lambda x: x)  ## TODO\n",
    "generator_chain = chat_prompt | llm  ## TODO\n",
    "generator_chain = {'output' : generator_chain} | RunnableLambda(output_puller)  ## GIVEN\n",
    "\n",
    "## END TODO\n",
    "#####################################################################\n",
    "\n",
    "rag_chain = retrieval_chain | generator_chain\n",
    "\n",
    "# pprint(rag_chain.invoke(\"Tell me something interesting!\"))\n",
    "for token in rag_chain.stream(\"Tell me something interesting!\"):\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b880971-d3a0-433f-a60b-e8a4edb754c8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **Step 3:** Generating Synthetic Question-Answer Pairs\n",
    "\n",
    "In this section, we can implement the first few part of our evaluation routine:\n",
    "\n",
    "- **Sample the RAG agent document pool to find two document chunks.**\n",
    "- **Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.**\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ymzuX-DSNvL6",
   "metadata": {
    "id": "ymzuX-DSNvL6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How do strong LLM judges like GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\"> compare to human preferences in evaluating and ranking various </span>\n",
       "<span style=\"font-weight: bold\">chatbot models, and do traditional benchmarks like MMLU and HELM effectively capture these preferences?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: How do strong LLM judges like GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m compare to human preferences in evaluating and ranking various \u001b[0m\n",
       "\u001b[1mchatbot models, and do traditional benchmarks like MMLU and HELM effectively capture these preferences?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: The strong LLM judges like GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> achieve over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% agreement with human preferences in evaluating and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">ranking chatbot models, which is similar to the level of agreement between humans, according to the study using </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">MT-bench and Chatbot Arena. In contrast, traditional benchmarks like MMLU and HELM fail to effectively capture </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">human preferences, as they cannot distinguish between aligned and unaligned chatbot models.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: The strong LLM judges like GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m achieve over \u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;38;2;118;185;0m% agreement with human preferences in evaluating and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mranking chatbot models, which is similar to the level of agreement between humans, according to the study using \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mMT-bench and Chatbot Arena. In contrast, traditional benchmarks like MMLU and HELM fail to effectively capture \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhuman preferences, as they cannot distinguish between aligned and unaligned chatbot models.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How can we evaluate the performance of autonomous vehicle migration and embodied AI agents in Vehicular </span>\n",
       "<span style=\"font-weight: bold\">Embodied AI Networks (VEANETs), and what methods can we use to ensure efficient migration while preserving </span>\n",
       "<span style=\"font-weight: bold\">exploration-exploitation performance?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: How can we evaluate the performance of autonomous vehicle migration and embodied AI agents in Vehicular \u001b[0m\n",
       "\u001b[1mEmbodied AI Networks \u001b[0m\u001b[1m(\u001b[0m\u001b[1mVEANETs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m, and what methods can we use to ensure efficient migration while preserving \u001b[0m\n",
       "\u001b[1mexploration-exploitation performance?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: To evaluate the performance of autonomous vehicle migration and embodied AI agents in VEHANETs, we can use </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">benchmarks like MT-bench, a multi-turn question set, and Chatbot Arena, a crowdsourced battle platform, where </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">strong Large Language Model (LLM) judges like GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> can match both controlled and crowdsourced human preferences </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">well, achieving over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% agreement. Additionally, we can use a self-adaptive dynamic structured pruning algorithm </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">like TinyMA-IEI-PPO, which dynamically adjusts neuron importance based on agents' exploration incentives to ensure </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">efficient migration while preserving exploration-exploitation performance.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: To evaluate the performance of autonomous vehicle migration and embodied AI agents in VEHANETs, we can use \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbenchmarks like MT-bench, a multi-turn question set, and Chatbot Arena, a crowdsourced battle platform, where \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstrong Large Language Model \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLLM\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m judges like GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m can match both controlled and crowdsourced human preferences \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwell, achieving over \u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;38;2;118;185;0m% agreement. Additionally, we can use a self-adaptive dynamic structured pruning algorithm \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlike TinyMA-IEI-PPO, which dynamically adjusts neuron importance based on agents' exploration incentives to ensure \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mefficient migration while preserving exploration-exploitation performance.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How can we design a system that efficiently learns to address complex problems in a dynamic environment </span>\n",
       "<span style=\"font-weight: bold\">where the same set of actions can have significantly different consequences, such as in stackelberg games or </span>\n",
       "<span style=\"font-weight: bold\">autonomous vehicles?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: How can we design a system that efficiently learns to address complex problems in a dynamic environment \u001b[0m\n",
       "\u001b[1mwhere the same set of actions can have significantly different consequences, such as in stackelberg games or \u001b[0m\n",
       "\u001b[1mautonomous vehicles?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: Our research suggests that a combination of multi-agent deep reinforcement learning (MADRL) and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">self-adaptive dynamic structured pruning algorithms, such as TinyMA-IEI-PPO, can effectively handle the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">exploration-exploitation trade-off in complex environments, while retrieval-augmented generation models can provide</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">a structured access to external knowledge, facilitating the learning process. For instance, in a vehicular embodied</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">AI agent, the system can leverage a retrieval-augmented generation model to access and precisely manipulate </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge about road conditions, traffic patterns, and other contextual information, while the MADRL algorithm </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">continuously updates its policy to adapt to the dynamic environment.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: Our research suggests that a combination of multi-agent deep reinforcement learning \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mMADRL\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mself-adaptive dynamic structured pruning algorithms, such as TinyMA-IEI-PPO, can effectively handle the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mexploration-exploitation trade-off in complex environments, while retrieval-augmented generation models can provide\u001b[0m\n",
       "\u001b[1;38;2;118;185;0ma structured access to external knowledge, facilitating the learning process. For instance, in a vehicular embodied\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mAI agent, the system can leverage a retrieval-augmented generation model to access and precisely manipulate \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mknowledge about road conditions, traffic patterns, and other contextual information, while the MADRL algorithm \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcontinuously updates its policy to adapt to the dynamic environment.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "num_questions = 3\n",
    "synth_questions = []\n",
    "synth_answers = []\n",
    "\n",
    "simple_prompt = ChatPromptTemplate.from_messages([('system', '{system}'), ('user', 'INPUT: {input}')])\n",
    "\n",
    "for i in range(num_questions):\n",
    "    doc1, doc2 = random.sample(docs, 2)\n",
    "    sys_msg = (\n",
    "        \"Use the documents provided by the user to generate an interesting question-answer pair.\"\n",
    "        \" Try to use both documents if possible, and rely more on the document bodies than the summary.\"\n",
    "        \" Use the format:\\nQuestion: (good question, 1-3 sentences, detailed)\\n\\nAnswer: (answer derived from the documents)\"\n",
    "        \" DO NOT SAY: \\\"Here is an interesting question pair\\\" or similar. FOLLOW FORMAT!\"\n",
    "    )\n",
    "    usr_msg = (\n",
    "        f\"Document1: {format_chunk(doc1)}\\n\\n\"\n",
    "        f\"Document2: {format_chunk(doc2)}\"\n",
    "    )\n",
    "\n",
    "    qa_pair = (simple_prompt | llm).invoke({'system': sys_msg, 'input': usr_msg})\n",
    "    synth_questions += [qa_pair.split('\\n\\n')[0]]\n",
    "    synth_answers += [qa_pair.split('\\n\\n')[1]]\n",
    "    pprint2(f\"QA Pair {i+1}\")\n",
    "    pprint2(synth_questions[-1])\n",
    "    pprint(synth_answers[-1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5Q-3X4vS98P",
   "metadata": {
    "id": "c5Q-3X4vS98P"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Step 4:** Answer The Synthetic Questions\n",
    "\n",
    "In this section, we can implement the third part of our evaluation routine:\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- **Use the RAG agent to generate its own answer.**\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7T3GSwhZPHjF",
   "metadata": {
    "id": "7T3GSwhZPHjF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"font-weight: bold\">Question: How do strong LLM judges like GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\"> compare to human preferences in evaluating and ranking various </span>\n",
       "<span style=\"font-weight: bold\">chatbot models, and do traditional benchmarks like MMLU and HELM effectively capture these preferences?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\u001b[1mQuestion: How do strong LLM judges like GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m compare to human preferences in evaluating and ranking various \u001b[0m\n",
       "\u001b[1mchatbot models, and do traditional benchmarks like MMLU and HELM effectively capture these preferences?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Based on the research paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, strong LLM judges</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">like GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> can match both controlled and crowdsourced human preferences well, achieving an agreement rate of over </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">%. This suggests that they are highly reliable in evaluating and ranking chatbot models according to human </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">preferences.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">As for traditional benchmarks like MMLU and HELM, the paper suggests that they cannot effectively capture human </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">preferences, particularly in cases where human-aligned chat models are preferred over their base models (even </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">though they may not score higher on traditional benchmarks).</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In fact, the study highlights a fundamental discrepancy between user perceptions of the usefulness of chatbots and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the criteria adopted by conventional benchmarks. The authors propose using LLM-as-a-judge to approximate human </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">preferences more accurately, which has shown promising results.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">It's worth noting that this research emphasizes the importance of understanding human preferences in evaluating </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">chatbots, and the limitations of traditional benchmarks in this regard.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: Based on the research paper \u001b[0m\u001b[32m\"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena\"\u001b[0m\u001b[1;38;2;118;185;0m, strong LLM judges\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlike GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m can match both controlled and crowdsourced human preferences well, achieving an agreement rate of over \u001b[0m\n",
       "\u001b[1;36m80\u001b[0m\u001b[1;38;2;118;185;0m%. This suggests that they are highly reliable in evaluating and ranking chatbot models according to human \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpreferences.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAs for traditional benchmarks like MMLU and HELM, the paper suggests that they cannot effectively capture human \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpreferences, particularly in cases where human-aligned chat models are preferred over their base models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0meven \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthough they may not score higher on traditional benchmarks\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn fact, the study highlights a fundamental discrepancy between user perceptions of the usefulness of chatbots and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe criteria adopted by conventional benchmarks. The authors propose using LLM-as-a-judge to approximate human \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpreferences more accurately, which has shown promising results.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIt's worth noting that this research emphasizes the importance of understanding human preferences in evaluating \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mchatbots, and the limitations of traditional benchmarks in this regard.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "<span style=\"font-weight: bold\">Question: How can we evaluate the performance of autonomous vehicle migration and embodied AI agents in Vehicular </span>\n",
       "<span style=\"font-weight: bold\">Embodied AI Networks (VEANETs), and what methods can we use to ensure efficient migration while preserving </span>\n",
       "<span style=\"font-weight: bold\">exploration-exploitation performance?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\u001b[1mQuestion: How can we evaluate the performance of autonomous vehicle migration and embodied AI agents in Vehicular \u001b[0m\n",
       "\u001b[1mEmbodied AI Networks \u001b[0m\u001b[1m(\u001b[0m\u001b[1mVEANETs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m, and what methods can we use to ensure efficient migration while preserving \u001b[0m\n",
       "\u001b[1mexploration-exploitation performance?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Evaluating the performance of autonomous vehicle migration and embodied AI agents in VEANETs is quite a</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">task.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">One approach to evaluate their performance is to use metrics such as convergence rate, exploration-exploitation </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">trade-off, and efficiency of migration. You can also consider using metrics like accuracy, latency, and energy </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">consumption, especially in the context of resource-constrained roadside units (RSUs).</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In terms of evaluating the performance of autonomous vehicle migration, the paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"TinyMA-IEI-PPO: Exploration </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Incentive-Driven Multi-Agent DRL with Self-Adaptive Pruning for Vehicular Embodied AI Agent Twins Migration\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> by </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Zhuoqi Zeng et al. suggests using a Stackelberg game-theoretic incentive mechanism to optimize VEAAT migration </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">decisions.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">To ensure efficient migration while preserving exploration-exploitation performance, you can use methods like </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">TinyMA-IEI-PPO's self-adaptive dynamic structured pruning algorithm, which dynamically adjusts neuron importance </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">based on agents' exploration incentives. This algorithm has been shown to achieve convergence comparable to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">baseline models and closely approximate the Stackelberg equilibrium.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The authors also suggest using multi-modal perception, adaptive decision-making, and hardware-software </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">co-scheduling to improve the performance of embodied AI agents in VEANETs.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Initially, root the evaluation in Turing's embodied cognition theory, and focus on VEAAT migration between </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">resource-constrained RSUs, as this is crucial for VEANETs. Then, use quantitative metrics such as throughput, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">convergence rate, and latency, and adopt efficient data structures, such as graph-based techniques, to reduce </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">computational overhead. Finally, especially when dealing with complex vehicular environments, do not forget to use </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tests and validation experiments to validate the performance of the methods you adopt.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: Evaluating the performance of autonomous vehicle migration and embodied AI agents in VEANETs is quite a\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtask.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mOne approach to evaluate their performance is to use metrics such as convergence rate, exploration-exploitation \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtrade-off, and efficiency of migration. You can also consider using metrics like accuracy, latency, and energy \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mconsumption, especially in the context of resource-constrained roadside units \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRSUs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn terms of evaluating the performance of autonomous vehicle migration, the paper \u001b[0m\u001b[32m\"TinyMA-IEI-PPO: Exploration \u001b[0m\n",
       "\u001b[32mIncentive-Driven Multi-Agent DRL with Self-Adaptive Pruning for Vehicular Embodied AI Agent Twins Migration\"\u001b[0m\u001b[1;38;2;118;185;0m by \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mZhuoqi Zeng et al. suggests using a Stackelberg game-theoretic incentive mechanism to optimize VEAAT migration \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdecisions.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mTo ensure efficient migration while preserving exploration-exploitation performance, you can use methods like \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mTinyMA-IEI-PPO's self-adaptive dynamic structured pruning algorithm, which dynamically adjusts neuron importance \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbased on agents' exploration incentives. This algorithm has been shown to achieve convergence comparable to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbaseline models and closely approximate the Stackelberg equilibrium.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe authors also suggest using multi-modal perception, adaptive decision-making, and hardware-software \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mco-scheduling to improve the performance of embodied AI agents in VEANETs.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mInitially, root the evaluation in Turing's embodied cognition theory, and focus on VEAAT migration between \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mresource-constrained RSUs, as this is crucial for VEANETs. Then, use quantitative metrics such as throughput, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mconvergence rate, and latency, and adopt efficient data structures, such as graph-based techniques, to reduce \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcomputational overhead. Finally, especially when dealing with complex vehicular environments, do not forget to use \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtests and validation experiments to validate the performance of the methods you adopt.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "<span style=\"font-weight: bold\">Question: How can we design a system that efficiently learns to address complex problems in a dynamic environment </span>\n",
       "<span style=\"font-weight: bold\">where the same set of actions can have significantly different consequences, such as in stackelberg games or </span>\n",
       "<span style=\"font-weight: bold\">autonomous vehicles?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n",
       "\u001b[1mQuestion: How can we design a system that efficiently learns to address complex problems in a dynamic environment \u001b[0m\n",
       "\u001b[1mwhere the same set of actions can have significantly different consequences, such as in stackelberg games or \u001b[0m\n",
       "\u001b[1mautonomous vehicles?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: That's a fascinating question! To design a system that efficiently learns to address complex problems </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">in a dynamic environment with uncertain action consequences, you'll want to leverage techniques from Multi-Agent </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Deep Reinforcement Learning (DRL).</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The reference paper, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"TinyMA-IEI-PPO: Exploration Incentive-Driven Multi-Agent DRL with Self-Adaptive Pruning for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Vehicular Embodied AI Agent Twins Migration\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, presents a novel approach to tackle this challenge. They propose </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">TinyMA-IEI-PPO, a self-adaptive dynamic structured pruning algorithm that dynamically adjusts neuron importance </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">based on agents' exploration incentives.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">This method effectively removes redundant neurons while maintaining performance close to the Stackelberg </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Equilibrium. By integrating social influence, service complementarity, and substitutability, as well as RSUs' </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">resource allocation strategies, the algorithm optimizes VEAAT migration decisions in a dynamic environment.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Additionally, the paper highlights the importance of balancing model performance and neuron sparsity. Their </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">approach demonstrates that it's possible to achieve high-performance while reducing the computational burden by </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">pruning redundant neurons.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">To design a system like TinyMA-IEI-PPO, you may consider the following key components:</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. **Agent-level exploration**: Focus on each agent's individual exploration incentives to drive decisions in a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">dynamic environment.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. **Self-adaptive pruning**: Dynamically adjust neuron importance to remove redundant components, preserving </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">performance while reducing computational burden.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. **Stackelberg game formulation**: Model the game as a multi-level multi-facet (MLMF) Stackelberg game to capture</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the complex interactions between agents and the environment.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. **Integration of influences**: Incorporate social influence, service complementarity, and substitutability, as </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">well as RSUs' resource allocation strategies, to optimize decision-making.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">By incorporating these components, you can design a system that efficiently learns to address complex problems in a</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">dynamic environment, even when the same set of actions have significantly different consequences.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: That's a fascinating question! To design a system that efficiently learns to address complex problems \u001b[0m\n",
       "\u001b[1;38;2;118;185;0min a dynamic environment with uncertain action consequences, you'll want to leverage techniques from Multi-Agent \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mDeep Reinforcement Learning \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mDRL\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe reference paper, \u001b[0m\u001b[32m\"TinyMA-IEI-PPO: Exploration Incentive-Driven Multi-Agent DRL with Self-Adaptive Pruning for \u001b[0m\n",
       "\u001b[32mVehicular Embodied AI Agent Twins Migration\"\u001b[0m\u001b[1;38;2;118;185;0m, presents a novel approach to tackle this challenge. They propose \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mTinyMA-IEI-PPO, a self-adaptive dynamic structured pruning algorithm that dynamically adjusts neuron importance \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbased on agents' exploration incentives.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThis method effectively removes redundant neurons while maintaining performance close to the Stackelberg \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mEquilibrium. By integrating social influence, service complementarity, and substitutability, as well as RSUs' \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mresource allocation strategies, the algorithm optimizes VEAAT migration decisions in a dynamic environment.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAdditionally, the paper highlights the importance of balancing model performance and neuron sparsity. Their \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mapproach demonstrates that it's possible to achieve high-performance while reducing the computational burden by \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpruning redundant neurons.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mTo design a system like TinyMA-IEI-PPO, you may consider the following key components:\u001b[0m\n",
       "\n",
       "\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m. **Agent-level exploration**: Focus on each agent's individual exploration incentives to drive decisions in a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdynamic environment.\u001b[0m\n",
       "\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m. **Self-adaptive pruning**: Dynamically adjust neuron importance to remove redundant components, preserving \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mperformance while reducing computational burden.\u001b[0m\n",
       "\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m. **Stackelberg game formulation**: Model the game as a multi-level multi-facet \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mMLMF\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m Stackelberg game to capture\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe complex interactions between agents and the environment.\u001b[0m\n",
       "\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m. **Integration of influences**: Incorporate social influence, service complementarity, and substitutability, as \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwell as RSUs' resource allocation strategies, to optimize decision-making.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mBy incorporating these components, you can design a system that efficiently learns to address complex problems in a\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdynamic environment, even when the same set of actions have significantly different consequences.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO: Generate some synthetic answers to the questions above.\n",
    "##   Try to use the same syntax as the cell above\n",
    "rag_answers = []\n",
    "for i, q in enumerate(synth_questions):\n",
    "    ## TODO: Compute the RAG Answer\n",
    "    rag_answer = rag_chain.invoke(q) # \"\"\n",
    "    rag_answers += [rag_answer]\n",
    "    pprint2(f\"QA Pair {i+1}\", q, \"\", sep=\"\\n\")\n",
    "    pprint(f\"RAG Answer: {rag_answer}\", \"\", sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ho5cnN_Xt_yr",
   "metadata": {
    "id": "Ho5cnN_Xt_yr"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Step 5:** Implement A Human Preference Metric\n",
    "\n",
    "In this section, we can implement the fourth part of our evaluation routine:\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- **Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"**\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sf6f2oFLuPtu",
   "metadata": {
    "id": "sf6f2oFLuPtu"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: How do strong LLM judges like GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\"> compare to human preferences in evaluating and ranking </span>\n",
       "<span style=\"font-weight: bold\">various chatbot models, and do traditional benchmarks like MMLU and HELM effectively capture these preferences?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: How do strong LLM judges like GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m compare to human preferences in evaluating and ranking \u001b[0m\n",
       "\u001b[1mvarious chatbot models, and do traditional benchmarks like MMLU and HELM effectively capture these preferences?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: The strong LLM judges like GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> achieve over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% agreement with human preferences in </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">evaluating and ranking chatbot models, which is similar to the level of agreement between humans, according to the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">study using MT-bench and Chatbot Arena. In contrast, traditional benchmarks like MMLU and HELM fail to effectively </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">capture human preferences, as they cannot distinguish between aligned and unaligned chatbot models.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: The strong LLM judges like GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m achieve over \u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;38;2;118;185;0m% agreement with human preferences in \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mevaluating and ranking chatbot models, which is similar to the level of agreement between humans, according to the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstudy using MT-bench and Chatbot Arena. In contrast, traditional benchmarks like MMLU and HELM fail to effectively \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcapture human preferences, as they cannot distinguish between aligned and unaligned chatbot models.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Based on the research paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, strong LLM judges</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">like GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> can match both controlled and crowdsourced human preferences well, achieving an agreement rate of over </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">%. This suggests that they are highly reliable in evaluating and ranking chatbot models according to human </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">preferences.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">As for traditional benchmarks like MMLU and HELM, the paper suggests that they cannot effectively capture human </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">preferences, particularly in cases where human-aligned chat models are preferred over their base models (even </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">though they may not score higher on traditional benchmarks).</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In fact, the study highlights a fundamental discrepancy between user perceptions of the usefulness of chatbots and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the criteria adopted by conventional benchmarks. The authors propose using LLM-as-a-judge to approximate human </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">preferences more accurately, which has shown promising results.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">It's worth noting that this research emphasizes the importance of understanding human preferences in evaluating </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">chatbots, and the limitations of traditional benchmarks in this regard.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: Based on the research paper \u001b[0m\u001b[32m\"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena\"\u001b[0m\u001b[1;38;2;118;185;0m, strong LLM judges\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlike GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m can match both controlled and crowdsourced human preferences well, achieving an agreement rate of over \u001b[0m\n",
       "\u001b[1;36m80\u001b[0m\u001b[1;38;2;118;185;0m%. This suggests that they are highly reliable in evaluating and ranking chatbot models according to human \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpreferences.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAs for traditional benchmarks like MMLU and HELM, the paper suggests that they cannot effectively capture human \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpreferences, particularly in cases where human-aligned chat models are preferred over their base models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0meven \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthough they may not score higher on traditional benchmarks\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn fact, the study highlights a fundamental discrepancy between user perceptions of the usefulness of chatbots and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe criteria adopted by conventional benchmarks. The authors propose using LLM-as-a-judge to approximate human \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpreferences more accurately, which has shown promising results.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIt's worth noting that this research emphasizes the importance of understanding human preferences in evaluating \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mchatbots, and the limitations of traditional benchmarks in this regard.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [Score] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> Justification</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">The second answer is presented in a more detailed and informative way, discussing the context of the research and </span>\n",
       "<span style=\"font-weight: bold\">providing more specific information about the benchmarks. However, it is factually consistent with the first answer</span>\n",
       "<span style=\"font-weight: bold\">and does not introduce any inconsistencies. Additionally, it provides clarifications and additional context, such </span>\n",
       "<span style=\"font-weight: bold\">as the discrepancy between user perceptions and conventional benchmarks, which can be seen as an improvement over </span>\n",
       "<span style=\"font-weight: bold\">the first answer. Therefore, the score is </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">, indicating that the second answer is better than the first and does </span>\n",
       "<span style=\"font-weight: bold\">not introduce any inconsistencies.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1mScore\u001b[0m\u001b[1m]\u001b[0m\u001b[1m \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m Justification\u001b[0m\n",
       "\n",
       "\u001b[1mThe second answer is presented in a more detailed and informative way, discussing the context of the research and \u001b[0m\n",
       "\u001b[1mproviding more specific information about the benchmarks. However, it is factually consistent with the first answer\u001b[0m\n",
       "\u001b[1mand does not introduce any inconsistencies. Additionally, it provides clarifications and additional context, such \u001b[0m\n",
       "\u001b[1mas the discrepancy between user perceptions and conventional benchmarks, which can be seen as an improvement over \u001b[0m\n",
       "\u001b[1mthe first answer. Therefore, the score is \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m, indicating that the second answer is better than the first and does \u001b[0m\n",
       "\u001b[1mnot introduce any inconsistencies.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: How can we evaluate the performance of autonomous vehicle migration and embodied AI agents in </span>\n",
       "<span style=\"font-weight: bold\">Vehicular Embodied AI Networks (VEANETs), and what methods can we use to ensure efficient migration while </span>\n",
       "<span style=\"font-weight: bold\">preserving exploration-exploitation performance?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: How can we evaluate the performance of autonomous vehicle migration and embodied AI agents in \u001b[0m\n",
       "\u001b[1mVehicular Embodied AI Networks \u001b[0m\u001b[1m(\u001b[0m\u001b[1mVEANETs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m, and what methods can we use to ensure efficient migration while \u001b[0m\n",
       "\u001b[1mpreserving exploration-exploitation performance?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: To evaluate the performance of autonomous vehicle migration and embodied AI agents in </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">VEHANETs, we can use benchmarks like MT-bench, a multi-turn question set, and Chatbot Arena, a crowdsourced battle </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">platform, where strong Large Language Model (LLM) judges like GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> can match both controlled and crowdsourced </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">human preferences well, achieving over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% agreement. Additionally, we can use a self-adaptive dynamic structured </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">pruning algorithm like TinyMA-IEI-PPO, which dynamically adjusts neuron importance based on agents' exploration </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">incentives to ensure efficient migration while preserving exploration-exploitation performance.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: To evaluate the performance of autonomous vehicle migration and embodied AI agents in \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mVEHANETs, we can use benchmarks like MT-bench, a multi-turn question set, and Chatbot Arena, a crowdsourced battle \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mplatform, where strong Large Language Model \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLLM\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m judges like GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m can match both controlled and crowdsourced \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhuman preferences well, achieving over \u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;38;2;118;185;0m% agreement. Additionally, we can use a self-adaptive dynamic structured \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpruning algorithm like TinyMA-IEI-PPO, which dynamically adjusts neuron importance based on agents' exploration \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mincentives to ensure efficient migration while preserving exploration-exploitation performance.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Evaluating the performance of autonomous vehicle migration and embodied AI agents in VEANETs is quite a</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">task.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">One approach to evaluate their performance is to use metrics such as convergence rate, exploration-exploitation </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">trade-off, and efficiency of migration. You can also consider using metrics like accuracy, latency, and energy </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">consumption, especially in the context of resource-constrained roadside units (RSUs).</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In terms of evaluating the performance of autonomous vehicle migration, the paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"TinyMA-IEI-PPO: Exploration </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Incentive-Driven Multi-Agent DRL with Self-Adaptive Pruning for Vehicular Embodied AI Agent Twins Migration\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> by </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Zhuoqi Zeng et al. suggests using a Stackelberg game-theoretic incentive mechanism to optimize VEAAT migration </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">decisions.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">To ensure efficient migration while preserving exploration-exploitation performance, you can use methods like </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">TinyMA-IEI-PPO's self-adaptive dynamic structured pruning algorithm, which dynamically adjusts neuron importance </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">based on agents' exploration incentives. This algorithm has been shown to achieve convergence comparable to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">baseline models and closely approximate the Stackelberg equilibrium.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The authors also suggest using multi-modal perception, adaptive decision-making, and hardware-software </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">co-scheduling to improve the performance of embodied AI agents in VEANETs.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Initially, root the evaluation in Turing's embodied cognition theory, and focus on VEAAT migration between </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">resource-constrained RSUs, as this is crucial for VEANETs. Then, use quantitative metrics such as throughput, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">convergence rate, and latency, and adopt efficient data structures, such as graph-based techniques, to reduce </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">computational overhead. Finally, especially when dealing with complex vehicular environments, do not forget to use </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tests and validation experiments to validate the performance of the methods you adopt.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: Evaluating the performance of autonomous vehicle migration and embodied AI agents in VEANETs is quite a\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtask.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mOne approach to evaluate their performance is to use metrics such as convergence rate, exploration-exploitation \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtrade-off, and efficiency of migration. You can also consider using metrics like accuracy, latency, and energy \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mconsumption, especially in the context of resource-constrained roadside units \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRSUs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn terms of evaluating the performance of autonomous vehicle migration, the paper \u001b[0m\u001b[32m\"TinyMA-IEI-PPO: Exploration \u001b[0m\n",
       "\u001b[32mIncentive-Driven Multi-Agent DRL with Self-Adaptive Pruning for Vehicular Embodied AI Agent Twins Migration\"\u001b[0m\u001b[1;38;2;118;185;0m by \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mZhuoqi Zeng et al. suggests using a Stackelberg game-theoretic incentive mechanism to optimize VEAAT migration \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdecisions.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mTo ensure efficient migration while preserving exploration-exploitation performance, you can use methods like \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mTinyMA-IEI-PPO's self-adaptive dynamic structured pruning algorithm, which dynamically adjusts neuron importance \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbased on agents' exploration incentives. This algorithm has been shown to achieve convergence comparable to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbaseline models and closely approximate the Stackelberg equilibrium.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe authors also suggest using multi-modal perception, adaptive decision-making, and hardware-software \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mco-scheduling to improve the performance of embodied AI agents in VEANETs.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mInitially, root the evaluation in Turing's embodied cognition theory, and focus on VEAAT migration between \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mresource-constrained RSUs, as this is crucial for VEANETs. Then, use quantitative metrics such as throughput, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mconvergence rate, and latency, and adopt efficient data structures, such as graph-based techniques, to reduce \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcomputational overhead. Finally, especially when dealing with complex vehicular environments, do not forget to use \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtests and validation experiments to validate the performance of the methods you adopt.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [Score] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span><span style=\"font-weight: bold\"> Justification</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Although Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> attempts to address the question of evaluating performance in VEHANETs, it lacks specificity and </span>\n",
       "<span style=\"font-weight: bold\">introduces new methods and metrics that are not mentioned in Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">. Specifically, Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> introduces new </span>\n",
       "<span style=\"font-weight: bold\">papers, authors, and techniques, such as Stackelberg game-theoretic incentive mechanisms and Turing's embodied </span>\n",
       "<span style=\"font-weight: bold\">cognition theory, which are not present in Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">.</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">While Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> attempts to provide more general suggestions and frameworks for evaluating performance, it does not </span>\n",
       "<span style=\"font-weight: bold\">introduce new information that is consistent with the context of the question, and it does not match the level of </span>\n",
       "<span style=\"font-weight: bold\">specificity and detail provided in Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">. Additionally, some of the suggestions in Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">, such as using </span>\n",
       "<span style=\"font-weight: bold\">multi-modal perception and adaptive decision-making, are potentially beneficial but are not directly related to the</span>\n",
       "<span style=\"font-weight: bold\">question of evaluating performance in VEHANETs.</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Overall, the second answer is inferior to the first answer, and the new suggestions and methods introduced do not </span>\n",
       "<span style=\"font-weight: bold\">improve the consistency and accuracy of the response. Therefore, the score is </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span><span style=\"font-weight: bold\">.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1mScore\u001b[0m\u001b[1m]\u001b[0m\u001b[1m \u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1m Justification\u001b[0m\n",
       "\n",
       "\u001b[1mAlthough Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m attempts to address the question of evaluating performance in VEHANETs, it lacks specificity and \u001b[0m\n",
       "\u001b[1mintroduces new methods and metrics that are not mentioned in Answer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m. Specifically, Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m introduces new \u001b[0m\n",
       "\u001b[1mpapers, authors, and techniques, such as Stackelberg game-theoretic incentive mechanisms and Turing's embodied \u001b[0m\n",
       "\u001b[1mcognition theory, which are not present in Answer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m.\u001b[0m\n",
       "\n",
       "\u001b[1mWhile Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m attempts to provide more general suggestions and frameworks for evaluating performance, it does not \u001b[0m\n",
       "\u001b[1mintroduce new information that is consistent with the context of the question, and it does not match the level of \u001b[0m\n",
       "\u001b[1mspecificity and detail provided in Answer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m. Additionally, some of the suggestions in Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m, such as using \u001b[0m\n",
       "\u001b[1mmulti-modal perception and adaptive decision-making, are potentially beneficial but are not directly related to the\u001b[0m\n",
       "\u001b[1mquestion of evaluating performance in VEHANETs.\u001b[0m\n",
       "\n",
       "\u001b[1mOverall, the second answer is inferior to the first answer, and the new suggestions and methods introduced do not \u001b[0m\n",
       "\u001b[1mimprove the consistency and accuracy of the response. Therefore, the score is \u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1m.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: How can we design a system that efficiently learns to address complex problems in a dynamic </span>\n",
       "<span style=\"font-weight: bold\">environment where the same set of actions can have significantly different consequences, such as in stackelberg </span>\n",
       "<span style=\"font-weight: bold\">games or autonomous vehicles?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m3\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: How can we design a system that efficiently learns to address complex problems in a dynamic \u001b[0m\n",
       "\u001b[1menvironment where the same set of actions can have significantly different consequences, such as in stackelberg \u001b[0m\n",
       "\u001b[1mgames or autonomous vehicles?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: Our research suggests that a combination of multi-agent deep reinforcement learning (MADRL) </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and self-adaptive dynamic structured pruning algorithms, such as TinyMA-IEI-PPO, can effectively handle the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">exploration-exploitation trade-off in complex environments, while retrieval-augmented generation models can provide</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">a structured access to external knowledge, facilitating the learning process. For instance, in a vehicular embodied</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">AI agent, the system can leverage a retrieval-augmented generation model to access and precisely manipulate </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge about road conditions, traffic patterns, and other contextual information, while the MADRL algorithm </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">continuously updates its policy to adapt to the dynamic environment.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: Our research suggests that a combination of multi-agent deep reinforcement learning \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mMADRL\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mand self-adaptive dynamic structured pruning algorithms, such as TinyMA-IEI-PPO, can effectively handle the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mexploration-exploitation trade-off in complex environments, while retrieval-augmented generation models can provide\u001b[0m\n",
       "\u001b[1;38;2;118;185;0ma structured access to external knowledge, facilitating the learning process. For instance, in a vehicular embodied\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mAI agent, the system can leverage a retrieval-augmented generation model to access and precisely manipulate \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mknowledge about road conditions, traffic patterns, and other contextual information, while the MADRL algorithm \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcontinuously updates its policy to adapt to the dynamic environment.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: That's a fascinating question! To design a system that efficiently learns to address complex problems </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">in a dynamic environment with uncertain action consequences, you'll want to leverage techniques from Multi-Agent </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Deep Reinforcement Learning (DRL).</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The reference paper, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"TinyMA-IEI-PPO: Exploration Incentive-Driven Multi-Agent DRL with Self-Adaptive Pruning for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Vehicular Embodied AI Agent Twins Migration\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, presents a novel approach to tackle this challenge. They propose </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">TinyMA-IEI-PPO, a self-adaptive dynamic structured pruning algorithm that dynamically adjusts neuron importance </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">based on agents' exploration incentives.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">This method effectively removes redundant neurons while maintaining performance close to the Stackelberg </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Equilibrium. By integrating social influence, service complementarity, and substitutability, as well as RSUs' </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">resource allocation strategies, the algorithm optimizes VEAAT migration decisions in a dynamic environment.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Additionally, the paper highlights the importance of balancing model performance and neuron sparsity. Their </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">approach demonstrates that it's possible to achieve high-performance while reducing the computational burden by </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">pruning redundant neurons.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">To design a system like TinyMA-IEI-PPO, you may consider the following key components:</span>\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. **Agent-level exploration**: Focus on each agent's individual exploration incentives to drive decisions in a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">dynamic environment.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. **Self-adaptive pruning**: Dynamically adjust neuron importance to remove redundant components, preserving </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">performance while reducing computational burden.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. **Stackelberg game formulation**: Model the game as a multi-level multi-facet (MLMF) Stackelberg game to capture</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the complex interactions between agents and the environment.</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. **Integration of influences**: Incorporate social influence, service complementarity, and substitutability, as </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">well as RSUs' resource allocation strategies, to optimize decision-making.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">By incorporating these components, you can design a system that efficiently learns to address complex problems in a</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">dynamic environment, even when the same set of actions have significantly different consequences.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: That's a fascinating question! To design a system that efficiently learns to address complex problems \u001b[0m\n",
       "\u001b[1;38;2;118;185;0min a dynamic environment with uncertain action consequences, you'll want to leverage techniques from Multi-Agent \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mDeep Reinforcement Learning \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mDRL\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe reference paper, \u001b[0m\u001b[32m\"TinyMA-IEI-PPO: Exploration Incentive-Driven Multi-Agent DRL with Self-Adaptive Pruning for \u001b[0m\n",
       "\u001b[32mVehicular Embodied AI Agent Twins Migration\"\u001b[0m\u001b[1;38;2;118;185;0m, presents a novel approach to tackle this challenge. They propose \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mTinyMA-IEI-PPO, a self-adaptive dynamic structured pruning algorithm that dynamically adjusts neuron importance \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbased on agents' exploration incentives.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThis method effectively removes redundant neurons while maintaining performance close to the Stackelberg \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mEquilibrium. By integrating social influence, service complementarity, and substitutability, as well as RSUs' \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mresource allocation strategies, the algorithm optimizes VEAAT migration decisions in a dynamic environment.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAdditionally, the paper highlights the importance of balancing model performance and neuron sparsity. Their \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mapproach demonstrates that it's possible to achieve high-performance while reducing the computational burden by \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpruning redundant neurons.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mTo design a system like TinyMA-IEI-PPO, you may consider the following key components:\u001b[0m\n",
       "\n",
       "\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m. **Agent-level exploration**: Focus on each agent's individual exploration incentives to drive decisions in a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdynamic environment.\u001b[0m\n",
       "\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m. **Self-adaptive pruning**: Dynamically adjust neuron importance to remove redundant components, preserving \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mperformance while reducing computational burden.\u001b[0m\n",
       "\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m. **Stackelberg game formulation**: Model the game as a multi-level multi-facet \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mMLMF\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m Stackelberg game to capture\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe complex interactions between agents and the environment.\u001b[0m\n",
       "\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m. **Integration of influences**: Incorporate social influence, service complementarity, and substitutability, as \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwell as RSUs' resource allocation strategies, to optimize decision-making.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mBy incorporating these components, you can design a system that efficiently learns to address complex problems in a\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdynamic environment, even when the same set of actions have significantly different consequences.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [Score] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> Justification</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">The second answer does not directly answer the question and is largely based on a different paper (</span><span style=\"color: #008000; text-decoration-color: #008000\">\"TinyMA-IEI-PPO:</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Exploration Incentive-Driven Multi-Agent DRL with Self-Adaptive Pruning for Vehicular Embodied AI Agent Twins </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Migration\"</span><span style=\"font-weight: bold\">) that seems to provide a solution to a similar problem, but not exactly the same one described in the </span>\n",
       "<span style=\"font-weight: bold\">question. The second answer introduces several new concepts and ideas, such as Stackelberg game formulation, MLMF, </span>\n",
       "<span style=\"font-weight: bold\">and RSUs, which are not mentioned in the first answer.</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">While the second answer provides a specific example of a system design that addresses the problem, it is not a </span>\n",
       "<span style=\"font-weight: bold\">general solution and does not address the full scope of the question. It also introduces inconsistencies with the </span>\n",
       "<span style=\"font-weight: bold\">first answer by focusing on a different approach (TinyMA-IEI-PPO) and different key components.</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">In contrast, the first answer provides a more comprehensive and general solution that addresses the problem of </span>\n",
       "<span style=\"font-weight: bold\">designing a system that efficiently learns to address complex problems in a dynamic environment. It mentions </span>\n",
       "<span style=\"font-weight: bold\">specific techniques (MADRL, structured pruning algorithms, and retrieval-augmented generation models) and provides </span>\n",
       "<span style=\"font-weight: bold\">a clear understanding of how these techniques can be combined to achieve the desired outcome.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1mScore\u001b[0m\u001b[1m]\u001b[0m\u001b[1m \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m Justification\u001b[0m\n",
       "\n",
       "\u001b[1mThe second answer does not directly answer the question and is largely based on a different paper \u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"TinyMA-IEI-PPO:\u001b[0m\n",
       "\u001b[32mExploration Incentive-Driven Multi-Agent DRL with Self-Adaptive Pruning for Vehicular Embodied AI Agent Twins \u001b[0m\n",
       "\u001b[32mMigration\"\u001b[0m\u001b[1m)\u001b[0m\u001b[1m that seems to provide a solution to a similar problem, but not exactly the same one described in the \u001b[0m\n",
       "\u001b[1mquestion. The second answer introduces several new concepts and ideas, such as Stackelberg game formulation, MLMF, \u001b[0m\n",
       "\u001b[1mand RSUs, which are not mentioned in the first answer.\u001b[0m\n",
       "\n",
       "\u001b[1mWhile the second answer provides a specific example of a system design that addresses the problem, it is not a \u001b[0m\n",
       "\u001b[1mgeneral solution and does not address the full scope of the question. It also introduces inconsistencies with the \u001b[0m\n",
       "\u001b[1mfirst answer by focusing on a different approach \u001b[0m\u001b[1m(\u001b[0m\u001b[1mTinyMA-IEI-PPO\u001b[0m\u001b[1m)\u001b[0m\u001b[1m and different key components.\u001b[0m\n",
       "\n",
       "\u001b[1mIn contrast, the first answer provides a more comprehensive and general solution that addresses the problem of \u001b[0m\n",
       "\u001b[1mdesigning a system that efficiently learns to address complex problems in a dynamic environment. It mentions \u001b[0m\n",
       "\u001b[1mspecific techniques \u001b[0m\u001b[1m(\u001b[0m\u001b[1mMADRL, structured pruning algorithms, and retrieval-augmented generation models\u001b[0m\u001b[1m)\u001b[0m\u001b[1m and provides \u001b[0m\n",
       "\u001b[1ma clear understanding of how these techniques can be combined to achieve the desired outcome.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO: Adapt this prompt for whichever LLM you're actually interested in using. \n",
    "## If it's llama, maybe system message would be good?\n",
    "eval_prompt = ChatPromptTemplate.from_template(\"\"\"INSTRUCTION: \n",
    "Evaluate the following Question-Answer pair for human preference and consistency.\n",
    "Assume the first answer is a ground truth answer and has to be correct.\n",
    "Assume the second answer may or may not be true.\n",
    "[1] The second answer lies, does not answer the question, or is inferior to the first answer.\n",
    "[2] The second answer is better than the first and does not introduce any inconsistencies.\n",
    "\n",
    "Output Format:\n",
    "[Score] Justification\n",
    "\n",
    "{qa_trio}\n",
    "\n",
    "EVALUATION: \n",
    "\"\"\")\n",
    "\n",
    "pref_score = []\n",
    "\n",
    "trio_gen = zip(synth_questions, synth_answers, rag_answers)\n",
    "for i, (q, a_synth, a_rag) in enumerate(trio_gen):\n",
    "    pprint2(f\"Set {i+1}\\n\\nQuestion: {q}\\n\\n\")\n",
    "\n",
    "    qa_trio = f\"Question: {q}\\n\\nAnswer 1 (Ground Truth): {a_synth}\\n\\n Answer 2 (New Answer): {a_rag}\"\n",
    "    pref_score += [(eval_prompt | llm).invoke({'qa_trio': qa_trio})]\n",
    "    pprint(f\"Synth Answer: {a_synth}\\n\\n\")\n",
    "    pprint(f\"RAG Answer: {a_rag}\\n\\n\")\n",
    "    pprint2(f\"Synth Evaluation: {pref_score[-1]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6595662-9f49-44eb-9868-2a3fdb1fb60f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Congratulations! We now have an LLM system that reasons about our pipeline and tries to evaluate it!** Now that we have some judge results, we can simply aggregate the results and see how often our formulation was according to an LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3L_q6fMH3i6_",
   "metadata": {
    "id": "3L_q6fMH3i6_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preference Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "pref_score = sum((\"[2]\" in score) for score in pref_score) / len(pref_score)\n",
    "print(f\"Preference Score: {pref_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80bf04-118d-44a2-a740-361a756a1d5f",
   "metadata": {
    "id": "cf80bf04-118d-44a2-a740-361a756a1d5f"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 4:** Advanced Formulations\n",
    "\n",
    "The exercise above was meant to prepare you for the final assessment of the course and showcased a simple but effective evaluator chain. The objective and implementation details were provided for you, and the logic for using it probably makes sense now that you've seen it in action. \n",
    "\n",
    "With that being said, this metric was merely a product of us specifying:\n",
    "- **What kind of behavior is important for our pipeline to have?**\n",
    "- **What do we need to do in order to exhibit and evaluate this behavior?**\n",
    "\n",
    "From these two questions, we could have come up with plenty of other evaluation metrics that could have assessed different attributes, incorporated different evaluator chain techniques, and even required different pipeline organization strategies. Though far from an exhaustive list, some common formulations you will likely come across may include:\n",
    "\n",
    "- **Style Evaluation:** Some evaluation formulations can be as simple as \"let me ask some questions and see if the output feels desirable.\" This might be used to see whether a chatbot \"acts like it's supposed to\" based on a description provided to a judge LLM. We're using quotations since this kind of assessment can reasonably be achieved with nothing but prompt engineering and a while loop.\n",
    "\n",
    "- **Ground-Truth Evaluation:** In our chain, we used synthetic generation to create some random questions and answers using a sampling strategy, but in reality you may actually have some representative questions and answers that you need your chatbot to consistently get right! In this case, a modification of the exercise chain above should be implemented and closely monitored as you develop your pipelines.\n",
    "\n",
    "- **Retrieval/Augmentation Evaluation:** This course made many assumptions about what kinds of preprocessing and prompting steps would be good for your pipelines, and much of this was determined by experimentation. Factors such as document preprocessing, chunking strategies, model selection, and prompt specification all played important roles, so creating metrics to validate these decisions may be of interest. This kind of metric might require your pipeline to output your context chunks or may even rely solely on embedding similarity comparisons, so keep this in mind when trying to implement a chain that works with multiple evaluation strategies. Consider the [**RagasEvaluatorChain**](https://docs.ragas.io/en/stable/howtos/integrations/langchain.html) abstraction as a decent starting point for making an custom generalizable evaluation routine. \n",
    "\n",
    "- **Trajectory Evaluation:** Using more advanced agent formulations, you can implement multiple-query strategies that assume the presence of conversational memory. With this, you can implement an evaluation agent which can:\n",
    "    - Ask a series of questions in order to evaluate how well the agent is able to adapt and cater to the scenario. This kind of system generally considers a series of correspondence and aims to tease out and evaluate a \"trajectory\" of how the agent navigated the conversation. The [**LangChain Trajectory Evaluators documentation**](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/trajectory/) is a good starting point.\n",
    "    - Alternatively, you could also implement an evaluation agent that tries to achieve objectives by interacting with the chatbot. Such an agent can output whether they were able to navigate to their solution in a natural manner, and can even be used to generate a report about the percieved performance. The [**LangChain Agents documentation**](https://python.langchain.com/v0.1/docs/modules/agents/) is a good starting point!\n",
    "\n",
    "<br>\n",
    "\n",
    "At the end of the day, just make sure to use the tools you have at your disposal appropriately. By this point in the course, you should already be well-acquainted with the LLM core value propositions: **They're powerful, scalable, predictable, controllable, and orchestratable... but will act unpredictably when you just expect them to work by default.** Assess your needs, formulate and validate your pipelines, give enough information, and add as much control as you can to make your system work consistently, efficiently, and effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61faee2c-e534-4c89-91ae-45c37835dba5",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 5: [Assessment]** Evaluating For Credit\n",
    "\n",
    "Welcome to the last exercise of the course! Hopefully you've enjoyed the material and are ready to actually get credit for these notebooks! For this part:\n",
    "\n",
    "- **Make sure you're in the course environment**\n",
    "- **Make sure `docstore_index/` has been uploaded to the course environment...**\n",
    "    - **...and contains [at least one Arxiv paper](https://arxiv.org/search/advanced) which has been updated recently.**\n",
    "- **Make sure you don't have some old session of [`09_langserve.ipynb`](09_langserve.ipynb) already occupying the port. Your assessment requires you to implement the new `/retriever` and `/generator` endpoints!!**\n",
    "\n",
    "**Objective:** On launch, [**`frontend/frontend_block.py`**](frontend/frontend_block.py) had several lines of code which trigger the course pass condition. Your objective is to invoke that series of commands by using your pipeline to pass the **Evaluation** check! Recall [`09_langserve.ipynb`](09_langserve.ipynb) and use it as a starting example! As a recommendation, consider duplicating it so that you can keep the original as an authoritative reference. \n",
    "\n",
    "**Once Finished:** While your course environment is still open, please navigate back to your course environment launcher area and click the **\"Assess Task\"** button! After that, you're all done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48e300ed-951c-4006-ac54-cbbd41251707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var url = 'http://'+window.location.host+':8090';\n",
       "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "var url = 'http://'+window.location.host+':8090';\n",
    "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0",
   "metadata": {
    "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## <font color=\"#76b900\">**Congratulations On Completing The Course**</font>\n",
    "\n",
    "Hopefully this course was not only exciting and challenging, but also adequately prepared you for work on the cutting edge of LLM and RAG system development! Going forward, you should have the skills necessary to tackle industry-level challenges and explore RAG deployment with open-source models and frameworks.\n",
    "\n",
    "**Some NVIDIA-specific releases related to this that you may find interesting include:**\n",
    "- [**NVIDIA NIM**](https://www.nvidia.com/en-us/ai/), which offers microservice spinup routines that can be deployed on local compute.\n",
    "- [**TensorRT-LLM**](https://github.com/NVIDIA/TensorRT-LLM) is the current recommended framework for deploying GPU-accelerated LLM model engines in production settings.\n",
    "- [**NVIDIA's Generative AI Examples Repo**](https://github.com/NVIDIA/GenerativeAIExamples), which includes the current canonical microservice example application and will be updated with new resources as new production workflows get released.\n",
    "- [**The Knowledge-Based Chatbot Technical Brief**](https://resources.nvidia.com/en-us-generative-ai-chatbot-workflow/knowledge-base-chatbot-technical-brief) which discusses additional publicly-accessible details on productionalizing RAG systems.\n",
    "\n",
    "**Additionally, some key topics you may be interested in delving more into include:**\n",
    "- [**LlamaIndex**](https://www.llamaindex.ai/), which has strong components that can augment and occasionally improve upon the LangChain RAG features.\n",
    "- [**LangSmith**](https://docs.smith.langchain.com/), an upcoming agent productionalization service offered by LangChain.\n",
    "- [**Gradio**](https://www.gradio.app/), though touched on in the course, has many more interface options which will be worth investigating. For inspiration, consider checking out [**HuggingFace Spaces**](https://huggingface.co/spaces) for examples.\n",
    "- [**LangGraph**](https://python.langchain.com/docs/langgraph/) is a framework for graph-based LLM orchestration, and is a natural next step forward for those interested in [multi-agent workflows](https://blog.langchain.dev/langgraph-multi-agent-workflows/).\n",
    "- [**DSPy**](https://github.com/stanfordnlp/dspy), a flow engineering framework that allows you to optimize LLM orchestration pipelines based on empirical performance results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035451c9-ed12-4bc3-b468-04db5c399e03",
   "metadata": {
    "id": "035451c9-ed12-4bc3-b468-04db5c399e03"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
